{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"Training BERT Model.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f5860a9c6b024709bfb2615130a35c89":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a112a89f365c4c728f7cdebf39f2df4b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8fbc06cde9f84ee38997a9664ce8f9a6","IPY_MODEL_5f924cdc6db742c98ee297bff9f028fd"]}},"a112a89f365c4c728f7cdebf39f2df4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8fbc06cde9f84ee38997a9664ce8f9a6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_90fabfaf97b141d88545b192bd1daeb5","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ad491d4867946b49a2c5ecd5ab4ebc8"}},"5f924cdc6db742c98ee297bff9f028fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_811585f216f846a0b3c30adb45a8e4ee","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:01&lt;00:00, 166kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_84d7c29312aa444b9ab1df7e3b9a5c86"}},"90fabfaf97b141d88545b192bd1daeb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1ad491d4867946b49a2c5ecd5ab4ebc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"811585f216f846a0b3c30adb45a8e4ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"84d7c29312aa444b9ab1df7e3b9a5c86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a113762bafa42838ca582469ad09479":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_31127d5248a84177b243004fcba0c46d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_953cd6194c6a4819ac0e4f8d104f5221","IPY_MODEL_e428bf7f17d24c8da171a74c96690a46"]}},"31127d5248a84177b243004fcba0c46d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"953cd6194c6a4819ac0e4f8d104f5221":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7083826efd904122b9b823143ebd47d9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9889be8e892b48c4a60d3d96cce37b44"}},"e428bf7f17d24c8da171a74c96690a46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e6fadbdd5c5e46b292f6670edda95e71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:01&lt;00:00, 27.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2665bd6006044d759c336518ba354d64"}},"7083826efd904122b9b823143ebd47d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9889be8e892b48c4a60d3d96cce37b44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6fadbdd5c5e46b292f6670edda95e71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2665bd6006044d759c336518ba354d64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd5bb98a541b4cd79faf00c2f5595958":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b65404d9f2594125b21a7053d30ca875","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7f9886c3581743709668847fd3819c72","IPY_MODEL_876b736975ed4343a8e26e7418b9a28b"]}},"b65404d9f2594125b21a7053d30ca875":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7f9886c3581743709668847fd3819c72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a4916e19bf3b4f65b3324dcb1bae0ff2","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bc848705c60341e4b949b634863f17b0"}},"876b736975ed4343a8e26e7418b9a28b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_93f0a7feaca04c939be2db08139a4265","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 466k/466k [00:00&lt;00:00, 1.06MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10d4bea8e93241acab702fb93dc4f16a"}},"a4916e19bf3b4f65b3324dcb1bae0ff2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bc848705c60341e4b949b634863f17b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93f0a7feaca04c939be2db08139a4265":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"10d4bea8e93241acab702fb93dc4f16a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2279fe26867c4ebc9903b6df302d9c19":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e4196956a91649eab0564abeb426f2a5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_33e35631637347349d6eb257df546db4","IPY_MODEL_9083f83f44e6478aa3f4053fa94a3e4e"]}},"e4196956a91649eab0564abeb426f2a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33e35631637347349d6eb257df546db4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b2abad32d6045b592f3412f1cfdcf7c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e8f0c819f95e43918b0b36662dbe9d19"}},"9083f83f44e6478aa3f4053fa94a3e4e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5405f4504d504f5db26d52ad92b85b0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 1.38kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6340a4457c874a789199c76cc2104870"}},"6b2abad32d6045b592f3412f1cfdcf7c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e8f0c819f95e43918b0b36662dbe9d19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5405f4504d504f5db26d52ad92b85b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6340a4457c874a789199c76cc2104870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"368836b9576242198ce8c5ffd3a29c4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1e178b43a3a946ca962cadb74f7ee0ea","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6ba4e0df3a994f91afcfb8e9b9503e58","IPY_MODEL_608eae4c65104ba199ad11616222d4ac"]}},"1e178b43a3a946ca962cadb74f7ee0ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ba4e0df3a994f91afcfb8e9b9503e58":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_816d89132f274829af28449808d48c28","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e4d4a9b56fd5462fa9beef0a1386d27c"}},"608eae4c65104ba199ad11616222d4ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7cd3821600f54ba0b250e62ae40467d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:49&lt;00:00, 8.86MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef448b38a0e242bdb41400e52919c753"}},"816d89132f274829af28449808d48c28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e4d4a9b56fd5462fa9beef0a1386d27c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7cd3821600f54ba0b250e62ae40467d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef448b38a0e242bdb41400e52919c753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"hfMwS62iPEhh"},"source":["## Importing the Data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2x2nZS_PZ_r","executionInfo":{"status":"ok","timestamp":1620102561103,"user_tz":-540,"elapsed":7525,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"c91c4728-95d0-4722-ba4c-342ac4a8c458"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 18.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 49.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 50.6MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Installing collected packages: sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y3u-CV7aPEhl","executionInfo":{"status":"ok","timestamp":1620102564496,"user_tz":-540,"elapsed":10916,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}}},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","from torch.utils.data import (TensorDataset, DataLoader, SequentialSampler)\n","from transformers import BertModel, BertTokenizer"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mMYXH2rPI3v","executionInfo":{"status":"ok","timestamp":1620102687352,"user_tz":-540,"elapsed":133767,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"b9aa10d4-203b-43dd-f10b-109a9e459c54"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xXehWqVoPEhm"},"source":["# Constants\n","DATASET_DIR = '/content/drive/MyDrive/ColabNotebooks/AES-for-Korean/data'\n","SAVE_DIR = '/content/drive/MyDrive/ColabNotebooks/AES-for-Korean/data'\n","\n","X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel.tsv'), sep='\\t', encoding='ISO-8859-1')\n","X = X.dropna(axis=1)\n","X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"Ig-icJQ-PEhm","executionInfo":{"status":"ok","timestamp":1620031554901,"user_tz":-540,"elapsed":5975,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"15b8efa1-c2bb-43ac-98d1-ed7794fc8700"},"source":["X.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>essay_set</th>\n","      <th>essay</th>\n","      <th>domain1_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Dear local newspaper, I think effects computer...</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>Dear @LOCATION1, I know having computers has a...</td>\n","      <td>8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   essay_id  ...  domain1_score\n","0         1  ...              8\n","1         2  ...              9\n","2         3  ...              7\n","3         4  ...             10\n","4         5  ...              8\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"T3Z3QNBXPEhn"},"source":["Minimum and Maximum Scores for each essay set."]},{"cell_type":"code","metadata":{"id":"RcDsTjiPPEho"},"source":["minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n","maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"o8DyN569PEho","executionInfo":{"status":"ok","timestamp":1620031554903,"user_tz":-540,"elapsed":4874,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"75d31460-cfd5-4195-c7f3-d3a403b31cd3"},"source":["old_min = minimum_scores[X['essay_set']]\n","old_max = maximum_scores[X['essay_set']]\n","old_range = old_max - old_min\n","new_min = 0\n","new_max = 1\n","new_range = (new_max - new_min)  \n","X['score'] = (((X['domain1_score'] - old_min) * new_range) / old_range) + new_min\n","\n","# round score to nearest integer for cohen kappa calculation\n","y = X['score']\n","\n","X.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>essay_id</th>\n","      <th>essay_set</th>\n","      <th>essay</th>\n","      <th>domain1_score</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Dear local newspaper, I think effects computer...</td>\n","      <td>8</td>\n","      <td>0.6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n","      <td>9</td>\n","      <td>0.7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n","      <td>7</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n","      <td>10</td>\n","      <td>0.8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>Dear @LOCATION1, I know having computers has a...</td>\n","      <td>8</td>\n","      <td>0.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   essay_id  essay_set  ... domain1_score  score\n","0         1          1  ...             8    0.6\n","1         2          1  ...             9    0.7\n","2         3          1  ...             7    0.5\n","3         4          1  ...            10    0.8\n","4         5          1  ...             8    0.6\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"8jfo_rgoPEho"},"source":["## Preprocessing the Data"]},{"cell_type":"markdown","metadata":{"id":"8hpjSpntPEho"},"source":["We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n","\n","These are all helper functions used to clean the essays."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eCQe5P3PEhp","executionInfo":{"status":"ok","timestamp":1620031556427,"user_tz":-540,"elapsed":5380,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"23aa188f-38a5-4e6b-a42d-016ecff3a283"},"source":["import nltk\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"4oWLcN5cPEhp"},"source":["import numpy as np\n","import re\n","from nltk.corpus import stopwords\n","\n","def essay_to_wordlist(essay_v, remove_stopwords):\n","    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n","    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n","    words = essay_v.lower().split()\n","    if remove_stopwords:\n","        stops = set(stopwords.words(\"english\"))\n","        words = [w for w in words if not w in stops]\n","    return (words)\n","\n","def essay_to_sentences(essay_v, remove_stopwords):\n","    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n","    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","    raw_sentences = tokenizer.tokenize(essay_v.strip())\n","    sentences = []\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            tokenized_sentences = essay_to_wordlist(raw_sentence, remove_stopwords)\n","            if tokenized_sentences:\n","                sentences.append(tokenized_sentences)\n","    return sentences\n","\n","def makeFeatureVec(words, model, num_features):\n","    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n","    featureVec = np.zeros((num_features,),dtype=\"float32\")\n","    num_words = 0.\n","    for word in words:\n","        if word in model:\n","            num_words += 1\n","            featureVec = np.add(featureVec, model[word])       \n","    featureVec = np.divide(featureVec,num_words)\n","    return featureVec\n","\n","def makeFeatureVec2(words, model, num_features):\n","    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n","    featureVec = np.zeros((num_features,),dtype=\"float32\")\n","    for word in words:\n","        if word in model:\n","            featureVec = np.add(featureVec, model[word])\n","    if len(words) != 0:\n","        featureVec = np.divide(featureVec,float(len(words)))\n","    return featureVec\n","\n","def makeFeatureVec3(words, model, num_features):\n","    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n","    featureVec = []\n","    for word in words:\n","        if word in model:\n","            featureVec.append(np.array(model[word], dtype=\"float32\"))\n","    return featureVec\n","\n","def getAvgFeatureVecs(essays, model, num_features):\n","    \"\"\"Main function to generate the word vectors for glove model.\"\"\"\n","    counter = 0\n","    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n","    for essay in essays:\n","        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n","        counter = counter + 1\n","    return essayFeatureVecs\n","\n","def getAvgFeatureVecs2(essay, model, num_features):\n","    \"\"\"Main function to generate the word vectors for glove model.\"\"\"\n","    essayFeatureVecs = np.zeros((len(essay),num_features),dtype=\"float32\")\n","    for cnt, sentence in enumerate(essay):\n","        essayFeatureVecs[cnt] = makeFeatureVec2(sentence, model, num_features)\n","    return essayFeatureVecs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f5860a9c6b024709bfb2615130a35c89","a112a89f365c4c728f7cdebf39f2df4b","8fbc06cde9f84ee38997a9664ce8f9a6","5f924cdc6db742c98ee297bff9f028fd","90fabfaf97b141d88545b192bd1daeb5","1ad491d4867946b49a2c5ecd5ab4ebc8","811585f216f846a0b3c30adb45a8e4ee","84d7c29312aa444b9ab1df7e3b9a5c86","3a113762bafa42838ca582469ad09479","31127d5248a84177b243004fcba0c46d","953cd6194c6a4819ac0e4f8d104f5221","e428bf7f17d24c8da171a74c96690a46","7083826efd904122b9b823143ebd47d9","9889be8e892b48c4a60d3d96cce37b44","e6fadbdd5c5e46b292f6670edda95e71","2665bd6006044d759c336518ba354d64","fd5bb98a541b4cd79faf00c2f5595958","b65404d9f2594125b21a7053d30ca875","7f9886c3581743709668847fd3819c72","876b736975ed4343a8e26e7418b9a28b","a4916e19bf3b4f65b3324dcb1bae0ff2","bc848705c60341e4b949b634863f17b0","93f0a7feaca04c939be2db08139a4265","10d4bea8e93241acab702fb93dc4f16a","2279fe26867c4ebc9903b6df302d9c19","e4196956a91649eab0564abeb426f2a5","33e35631637347349d6eb257df546db4","9083f83f44e6478aa3f4053fa94a3e4e","6b2abad32d6045b592f3412f1cfdcf7c","e8f0c819f95e43918b0b36662dbe9d19","5405f4504d504f5db26d52ad92b85b0b","6340a4457c874a789199c76cc2104870","368836b9576242198ce8c5ffd3a29c4f","1e178b43a3a946ca962cadb74f7ee0ea","6ba4e0df3a994f91afcfb8e9b9503e58","608eae4c65104ba199ad11616222d4ac","816d89132f274829af28449808d48c28","e4d4a9b56fd5462fa9beef0a1386d27c","7cd3821600f54ba0b250e62ae40467d0","ef448b38a0e242bdb41400e52919c753"]},"id":"E_G9PIMqPEhp","executionInfo":{"status":"ok","timestamp":1620031586600,"user_tz":-540,"elapsed":29280,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"e537118d-3c6d-4357-a296-3f6cd03532cd"},"source":["pretrained_model_name = 'bert-base-uncased'\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n","model = BertModel.from_pretrained(pretrained_model_name).cuda()\n","#model.train()\n","\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5860a9c6b024709bfb2615130a35c89","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a113762bafa42838ca582469ad09479","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd5bb98a541b4cd79faf00c2f5595958","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2279fe26867c4ebc9903b6df302d9c19","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"368836b9576242198ce8c5ffd3a29c4f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"VJDTN87JPEhq","executionInfo":{"status":"ok","timestamp":1620031612023,"user_tz":-540,"elapsed":25418,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"a2493dee-d487-43bb-b00e-9eb2ee713e32"},"source":["from sklearn.model_selection import KFold\n","from keras.preprocessing.sequence import pad_sequences\n","\n","essay_data = X['essay']\n","is_remove_stopwords = False\n","tokenized_essay = []\n","padded_tokenized_essay = []\n","attention_mask_essay = []\n","sent_max_len = 200\n","for ix, essay in enumerate(essay_data):\n","  if ix % 1000 == 0:\n","    print(ix)\n","  sentences = essay_to_sentences(essay, remove_stopwords=is_remove_stopwords)\n","  tokenized_sentences = []\n","  for iy, sentence in enumerate(sentences):\n","      tokenized_sentence = np.array(tokenizer.encode(sentence, add_special_tokens=True))\n","      tokenized_sentences.append(tokenized_sentence)\n","  padded_tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=sent_max_len, padding='post')\n","  attention_mask_sentences = np.where(padded_tokenized_sentences != 0, 1, 0)\n","  #padded_tokenized_sentences = torch.tensor(pad_sequences(tokenized_sentences, maxlen=sent_max_len, padding='post'))\n","  #attention_mask_sentences = torch.tensor(np.where(padded_tokenized_sentences != 0, 1, 0))\n","  padded_tokenized_essay.append(padded_tokenized_sentences)\n","  attention_mask_essay.append(attention_mask_sentences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n","7000\n","8000\n","9000\n","10000\n","11000\n","12000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DfNHlss-8hE_"},"source":["tttt_essays = []\n","tttt = 0\n","max_tttt = 0\n","for essay in essay_data:\n","  tttt_tmp = essay_to_sentences(essay, remove_stopwords=False)\n","  tttt_essays.append(tttt_tmp)\n","  tttt += len(tttt_tmp)\n","  if len(tttt_tmp) > max_tttt:\n","    max_tttt = len(tttt_tmp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRnXo_AJ-W7x","executionInfo":{"status":"ok","timestamp":1620031619439,"user_tz":-540,"elapsed":32827,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"69fb433b-ce75-4487-f795-8e6a2e84d2fc"},"source":["padded_tokenized_essay[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16, 200)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJ8CPs449FE1","executionInfo":{"status":"ok","timestamp":1620031619440,"user_tz":-540,"elapsed":32824,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"d936c4dd-b78b-44e5-a6bb-d8a2f53e3b0c"},"source":["print(max_tttt)\n","print(X['essay'][0])\n","print(tttt_essays[0])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["96\n","Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\n","[['dear', 'local', 'newspaper', 'i', 'think', 'effects', 'computers', 'have', 'on', 'people', 'are', 'great', 'learning', 'skills', 'affects', 'because', 'they', 'give', 'us', 'time', 'to', 'chat', 'with', 'friends', 'new', 'people', 'helps', 'us', 'learn', 'about', 'the', 'globe', 'astronomy', 'and', 'keeps', 'us', 'out', 'of', 'troble'], ['thing', 'about'], ['dont', 'you', 'think', 'so'], ['how', 'would', 'you', 'feel', 'if', 'your', 'teenager', 'is', 'always', 'on', 'the', 'phone', 'with', 'friends'], ['do', 'you', 'ever', 'time', 'to', 'chat', 'with', 'your', 'friends', 'or', 'buisness', 'partner', 'about', 'things'], ['well', 'now', 'there', 's', 'a', 'new', 'way', 'to', 'chat', 'the', 'computer', 'theirs', 'plenty', 'of', 'sites', 'on', 'the', 'internet', 'to', 'do', 'so', 'organization', 'organization', 'caps', 'facebook', 'myspace', 'ect'], ['just', 'think', 'now', 'while', 'your', 'setting', 'up', 'meeting', 'with', 'your', 'boss', 'on', 'the', 'computer', 'your', 'teenager', 'is', 'having', 'fun', 'on', 'the', 'phone', 'not', 'rushing', 'to', 'get', 'off', 'cause', 'you', 'want', 'to', 'use', 'it'], ['how', 'did', 'you', 'learn', 'about', 'other', 'countrys', 'states', 'outside', 'of', 'yours'], ['well', 'i', 'have', 'by', 'computer', 'internet', 'it', 's', 'a', 'new', 'way', 'to', 'learn', 'about', 'what', 'going', 'on', 'in', 'our', 'time'], ['you', 'might', 'think', 'your', 'child', 'spends', 'a', 'lot', 'of', 'time', 'on', 'the', 'computer', 'but', 'ask', 'them', 'so', 'question', 'about', 'the', 'economy', 'sea', 'floor', 'spreading', 'or', 'even', 'about', 'the', 'date', 's', 'you', 'll', 'be', 'surprise', 'at', 'how', 'much', 'he', 'she', 'knows'], ['believe', 'it', 'or', 'not', 'the', 'computer', 'is', 'much', 'interesting', 'then', 'in', 'class', 'all', 'day', 'reading', 'out', 'of', 'books'], ['if', 'your', 'child', 'is', 'home', 'on', 'your', 'computer', 'or', 'at', 'a', 'local', 'library', 'it', 's', 'better', 'than', 'being', 'out', 'with', 'friends', 'being', 'fresh', 'or', 'being', 'perpressured', 'to', 'doing', 'something', 'they', 'know', 'isnt', 'right'], ['you', 'might', 'not', 'know', 'where', 'your', 'child', 'is', 'caps', 'forbidde', 'in', 'a', 'hospital', 'bed', 'because', 'of', 'a', 'drive', 'by'], ['rather', 'than', 'your', 'child', 'on', 'the', 'computer', 'learning', 'chatting', 'or', 'just', 'playing', 'games', 'safe', 'and', 'sound', 'in', 'your', 'home', 'or', 'community', 'place'], ['now', 'i', 'hope', 'you', 'have', 'reached', 'a', 'point', 'to', 'understand', 'and', 'agree', 'with', 'me', 'because', 'computers', 'can', 'have', 'great', 'effects', 'on', 'you', 'or', 'child', 'because', 'it', 'gives', 'us', 'time', 'to', 'chat', 'with', 'friends', 'new', 'people', 'helps', 'us', 'learn', 'about', 'the', 'globe', 'and', 'believe', 'or', 'not', 'keeps', 'us', 'out', 'of', 'troble'], ['thank', 'you', 'for', 'listening']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V3cU9QPnzPDB"},"source":["# BERT Embedding\n","\n","You need to execute this section only once."]},{"cell_type":"code","metadata":{"id":"PY18dAh3-np6"},"source":["import csv\n","ff = open(os.path.join(SAVE_DIR, 'embedded_features_special_tokens.csv'), 'a', newline='')\n","writer_ff = csv.writer(ff)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6V7BbSA-VCpS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620036056257,"user_tz":-540,"elapsed":4456711,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"5f0cf8cc-ff17-4d0b-a6d5-f815ece213f4"},"source":["import psutil\n","\n","with torch.no_grad():\n","  print(\"ix, gpu memory, cpu usage, vm usage, data size \")\n","  for ix in range(len(padded_tokenized_essay)):\n","    if ix % 100 == 0:\n","      print(ix, torch.cuda.memory_allocated(0), psutil.cpu_percent(), psutil.virtual_memory()[2])\n","    sen = torch.tensor(padded_tokenized_essay[ix]).cuda()\n","    mask = torch.tensor(attention_mask_essay[ix]).cuda()\n","    last_hidden_states_train = model(sen, attention_mask=mask)\n","    embedded_features = last_hidden_states_train[0].detach().cpu()[:, 0, :].numpy()\n","    # embedded_essay.append(embedded_features)\n","    for i in embedded_features:\n","      writer_ff.writerow(i)\n","    #np.savetxt(os.path.join(SAVE_DIR, 'embedded_features.csv'), embedded_features, delimiter=\",\")\n","    del sen, mask, last_hidden_states_train, embedded_features\n","    torch.cuda.empty_cache()\n","\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ix, gpu memory, cpu usage, vm usage, data size \n","0 439065600 31.3 17.0\n","100 439065600 53.2 16.9\n","200 439065600 53.1 16.9\n","300 439065600 53.2 16.9\n","400 439065600 53.2 16.9\n","500 439065600 53.3 16.9\n","600 439065600 53.1 16.9\n","700 439065600 53.2 16.9\n","800 439065600 53.3 16.9\n","900 439065600 53.1 16.9\n","1000 439065600 53.2 16.9\n","1100 439065600 53.2 16.9\n","1200 439065600 53.1 16.9\n","1300 439065600 53.2 16.9\n","1400 439065600 53.2 16.9\n","1500 439065600 53.2 16.9\n","1600 439065600 53.3 16.9\n","1700 439065600 53.2 16.9\n","1800 439065600 53.2 16.9\n","1900 439065600 53.2 16.9\n","2000 439065600 53.3 16.9\n","2100 439065600 53.3 16.9\n","2200 439065600 53.2 16.9\n","2300 439065600 53.3 16.9\n","2400 439065600 53.3 16.9\n","2500 439065600 53.4 16.9\n","2600 439065600 53.3 16.9\n","2700 439065600 53.4 16.9\n","2800 439065600 53.4 16.9\n","2900 439065600 53.4 16.9\n","3000 439065600 53.3 16.9\n","3100 439065600 53.1 16.9\n","3200 439065600 53.3 16.9\n","3300 439065600 53.2 16.9\n","3400 439065600 53.3 16.9\n","3500 439065600 53.2 16.9\n","3600 439065600 53.4 16.9\n","3700 439065600 53.4 16.9\n","3800 439065600 53.4 16.9\n","3900 439065600 53.3 16.9\n","4000 439065600 53.4 16.9\n","4100 439065600 53.2 16.9\n","4200 439065600 53.5 16.9\n","4300 439065600 53.2 16.9\n","4400 439065600 53.6 16.9\n","4500 439065600 53.1 16.9\n","4600 439065600 53.3 16.9\n","4700 439065600 53.4 16.9\n","4800 439065600 53.4 16.9\n","4900 439065600 53.2 16.9\n","5000 439065600 53.1 16.9\n","5100 439065600 53.3 16.9\n","5200 439065600 53.4 16.9\n","5300 439065600 53.5 16.9\n","5400 439065600 53.0 16.9\n","5500 439065600 53.4 16.9\n","5600 439065600 53.2 16.9\n","5700 439065600 53.2 16.9\n","5800 439065600 53.2 16.9\n","5900 439065600 53.2 16.9\n","6000 439065600 53.4 16.9\n","6100 439065600 53.3 16.9\n","6200 439065600 53.2 16.9\n","6300 439065600 53.2 16.9\n","6400 439065600 53.5 16.9\n","6500 439065600 53.6 16.9\n","6600 439065600 53.2 16.9\n","6700 439065600 53.0 16.9\n","6800 439065600 53.3 16.9\n","6900 439065600 53.6 16.9\n","7000 439065600 53.3 16.9\n","7100 439065600 53.3 16.9\n","7200 439065600 53.3 16.9\n","7300 439065600 53.2 16.9\n","7400 439065600 53.4 16.9\n","7500 439065600 53.2 16.9\n","7600 439065600 53.2 16.9\n","7700 439065600 53.1 16.9\n","7800 439065600 53.2 16.9\n","7900 439065600 53.2 16.9\n","8000 439065600 53.1 16.9\n","8100 439065600 53.5 16.9\n","8200 439065600 53.3 16.9\n","8300 439065600 53.3 16.9\n","8400 439065600 53.2 16.9\n","8500 439065600 53.4 16.9\n","8600 439065600 53.5 16.9\n","8700 439065600 53.4 16.9\n","8800 439065600 53.3 16.9\n","8900 439065600 53.2 16.9\n","9000 439065600 53.5 16.9\n","9100 439065600 53.3 16.9\n","9200 439065600 53.3 16.9\n","9300 439065600 53.2 16.9\n","9400 439065600 53.5 16.9\n","9500 439065600 53.0 16.9\n","9600 439065600 53.2 16.9\n","9700 439065600 53.2 16.9\n","9800 439065600 53.2 16.9\n","9900 439065600 53.3 16.9\n","10000 439065600 53.3 16.9\n","10100 439065600 53.3 16.9\n","10200 439065600 53.2 16.9\n","10300 439065600 53.3 16.9\n","10400 439065600 53.4 16.9\n","10500 439065600 53.3 16.9\n","10600 439065600 53.3 16.9\n","10700 439065600 53.1 16.9\n","10800 439065600 53.3 16.9\n","10900 439065600 53.4 16.9\n","11000 439065600 53.3 16.9\n","11100 439065600 53.2 16.9\n","11200 439065600 53.5 16.9\n","11300 439065600 53.2 16.9\n","11400 439065600 53.3 16.9\n","11500 439065600 53.2 16.9\n","11600 439065600 53.3 16.9\n","11700 439065600 53.2 16.9\n","11800 439065600 53.3 16.9\n","11900 439065600 53.4 16.9\n","12000 439065600 53.2 16.9\n","12100 439065600 53.2 16.9\n","12200 439065600 53.2 16.9\n","12300 439065600 53.1 16.9\n","12400 439065600 53.0 16.9\n","12500 439065600 53.0 16.9\n","12600 439065600 53.0 16.9\n","12700 439065600 53.1 16.9\n","12800 439065600 53.0 16.9\n","12900 439065600 53.0 16.9\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"so6UBeadAevA"},"source":["ff.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dsBopphtV1UL"},"source":["# Load the embedded data"]},{"cell_type":"code","metadata":{"id":"MMJmZ43CV1ZO"},"source":["embedded_essay_raw = pd.read_csv(os.path.join(DATASET_DIR, 'embedded_features_special_tokens.csv'), sep=',', encoding='ISO-8859-1')\n","#embedded_essay_raw = pd.read_csv(os.path.join(DATASET_DIR, 'embedded_features_raw.csv'), sep=',', encoding='ISO-8859-1')\n","#embedded_essay_raw = pd.read_csv(os.path.join(DATASET_DIR, 'embedded_features.csv'), sep=',', encoding='ISO-8859-1')\n","embedded_essay = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQfslPufqYwu","executionInfo":{"status":"ok","timestamp":1620036096183,"user_tz":-540,"elapsed":38291,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"7e17ae30-f32f-43b5-d07b-27bea7919296"},"source":["embedded_essay_raw.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(164770, 768)"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqLlpS3wqMNJ","executionInfo":{"status":"ok","timestamp":1620036097302,"user_tz":-540,"elapsed":39407,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"9b831549-cbb9-4d02-d8de-c35caaa52743"},"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","tmp_ix = 0\n","for ix, essay_raw in enumerate(padded_tokenized_essay):\n","  if ix % 500 == 0:\n","    print(ix)\n","  tmp_len = len(essay_raw)\n","  essay = embedded_essay_raw[tmp_ix:tmp_ix + tmp_len]\n","  embedded_essay.append(essay)\n","  tmp_ix += tmp_len"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","500\n","1000\n","1500\n","2000\n","2500\n","3000\n","3500\n","4000\n","4500\n","5000\n","5500\n","6000\n","6500\n","7000\n","7500\n","8000\n","8500\n","9000\n","9500\n","10000\n","10500\n","11000\n","11500\n","12000\n","12500\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oQAT0ySuyuns"},"source":["#embedded_essay = pad_sequences(embedded_essay, maxlen=96, padding='pre', dtype='float')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCt8smv_PEhr"},"source":["Now we train the model on the dataset.\n","\n","We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n","We will then calculate Average Kappa for all the folds."]},{"cell_type":"markdown","metadata":{"id":"TFwEM_HJPEhs"},"source":["## Defining the model "]},{"cell_type":"markdown","metadata":{"id":"GWOaEAFvPEhs"},"source":["Here we define a 2-Layer LSTM Model. \n","\n","Note that instead of using sigmoid activation in the output layer we will use\n","Relu since we are not normalising training labels."]},{"cell_type":"code","metadata":{"id":"cx-Ci8V7PEhs"},"source":["from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Lambda, Flatten\n","from tensorflow.keras.models import Sequential, load_model, model_from_config\n","import tensorflow.keras.backend as K\n","\n","def get_model():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 200], return_sequences=True))\n","    model.add(LSTM(64, recurrent_dropout=0.4))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='relu'))\n","\n","    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n","    model.summary()\n","\n","    return model\n","\n","def get_sentence_model():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    model.add(GRU(128, dropout=0.4, input_shape=[128, 768], return_sequences=True))\n","    model.add(GRU(64))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='relu'))\n","\n","    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n","    model.summary()\n","\n","    return model\n","\n","def get_sentence_model2():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    model.add(GRU(96, dropout=0.4, input_shape=[128, 768], return_sequences=True))\n","    model.add(GRU(64))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='relu'))\n","\n","    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n","    model.summary()\n","\n","    return model\n","\n","def get_sentence_model3():\n","    \"\"\"Define the model.\"\"\"\n","    model = Sequential()\n","    model.add(GRU(256, dropout=0.4, input_shape=[128, 768], return_sequences=True))\n","    model.add(GRU(128, dropout=0.4, return_sequences=True))\n","    model.add(GRU(64))\n","    model.add(Dropout(0.5))\n","    model.add(Dense(1, activation='relu'))\n","\n","    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n","    model.summary()\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3UqR98sA2hD"},"source":["## Fine-tuning BERT + GRU 2 Layer with PyTorch"]},{"cell_type":"code","metadata":{"id":"7frwhw_A9CoO"},"source":["import torch.nn as nn\n","from transformers import AutoConfig, AutoModel\n","\n","class \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nOrwObglAzbm"},"source":["## Freezed Bert + GRU 2 Layer with Keras"]},{"cell_type":"code","metadata":{"id":"n6okZqE073fH"},"source":["#embedded_essay = pad_sequences(embedded_essay, maxlen=96, padding='pre', dtype='float')\n","\n","import keras\n","import math\n","\n","class DataGenerator(keras.utils.Sequence):\n","\n","  def __init__(self, ids, batch_size=64, shuffle=True):\n","    self.ids = ids\n","    self.batch_size = batch_size\n","    self.shuffle = shuffle\n","    self.on_epoch_end()\n","\n","  def on_epoch_end(self):\n","    # Updates indexes after each epoch\n","    self.indexes = np.arange(len(self.ids))\n","    if self.shuffle == True:\n","      np.random.shuffle(self.indexes)\n","\n","  def __len__(self):\n","    return math.ceil(len(self.ids) / self.batch_size)\n","\n","  def __getitem__(self, index):\n","    # Generated data containing batch_size samples\n","    batch_ids = self.ids[index * self.batch_size:(index + 1) * self.batch_size]\n","\n","    essays = list()\n","    scores = list()\n","    for ix in batch_ids:\n","      essay = embedded_essay[ix]\n","      score = y[ix]\n","      essays.append(essay)\n","      scores.append(score)\n","    essays = pad_sequences(essays, maxlen=128, padding='pre', dtype='float')\n","\n","    return np.array(essays), np.array(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzYvJkz2FtG0","executionInfo":{"status":"ok","timestamp":1620036097303,"user_tz":-540,"elapsed":39398,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"c02ba809-63bf-4465-aef9-88667c22ae48"},"source":["y\n","len(embedded_essay)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12976"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":594},"id":"InriBZ7kI5DH","executionInfo":{"status":"ok","timestamp":1620036097303,"user_tz":-540,"elapsed":39395,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"16a7bdc7-afdd-4d0c-83d1-89d6db5ea1f5"},"source":["embedded_essay[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0.03897584</th>\n","      <th>0.1705755</th>\n","      <th>0.16306049</th>\n","      <th>-0.32822582</th>\n","      <th>-0.4931742</th>\n","      <th>-0.5777212</th>\n","      <th>0.51011056</th>\n","      <th>0.8981184</th>\n","      <th>0.15726711</th>\n","      <th>-0.63853836</th>\n","      <th>-0.16852458</th>\n","      <th>-0.2539917</th>\n","      <th>0.0052978797</th>\n","      <th>0.3112666</th>\n","      <th>0.30889437</th>\n","      <th>-0.23946148</th>\n","      <th>-0.5916331</th>\n","      <th>0.5375616</th>\n","      <th>0.10453381</th>\n","      <th>0.25923082</th>\n","      <th>0.040120613</th>\n","      <th>-0.35811937</th>\n","      <th>0.084900536</th>\n","      <th>-0.12979448</th>\n","      <th>0.20781113</th>\n","      <th>-0.24564661</th>\n","      <th>-0.21821417</th>\n","      <th>-0.18564206</th>\n","      <th>-0.10883415</th>\n","      <th>0.19866161</th>\n","      <th>-0.2736097</th>\n","      <th>0.4945069</th>\n","      <th>-0.06111347</th>\n","      <th>0.09564236</th>\n","      <th>0.044967655</th>\n","      <th>-0.49912897</th>\n","      <th>0.44235414</th>\n","      <th>0.23340337</th>\n","      <th>0.18972217</th>\n","      <th>0.2369034</th>\n","      <th>...</th>\n","      <th>0.17444798</th>\n","      <th>-0.7208587</th>\n","      <th>0.2390435</th>\n","      <th>0.5639985</th>\n","      <th>-0.02137434</th>\n","      <th>-0.05141661</th>\n","      <th>-0.030394815</th>\n","      <th>-0.104016826</th>\n","      <th>-0.5468714</th>\n","      <th>-0.16815141</th>\n","      <th>-0.19427754</th>\n","      <th>0.38222885</th>\n","      <th>0.12647282</th>\n","      <th>0.095467836</th>\n","      <th>-0.37149695</th>\n","      <th>0.43112832</th>\n","      <th>0.28849825</th>\n","      <th>0.55694956</th>\n","      <th>0.24611443</th>\n","      <th>-0.053098254</th>\n","      <th>-0.10154303</th>\n","      <th>-0.17519955</th>\n","      <th>0.4287814</th>\n","      <th>0.5200037</th>\n","      <th>-5.403975</th>\n","      <th>0.04553558</th>\n","      <th>-0.12436247</th>\n","      <th>0.0050980854</th>\n","      <th>-0.22526489</th>\n","      <th>-0.3898146</th>\n","      <th>-0.115320064</th>\n","      <th>-0.37885273</th>\n","      <th>0.02988772</th>\n","      <th>-0.12692152</th>\n","      <th>0.37245676</th>\n","      <th>0.12877496</th>\n","      <th>-0.19407533</th>\n","      <th>-0.6573706</th>\n","      <th>0.3568359</th>\n","      <th>0.39976805</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.031000</td>\n","      <td>-0.039242</td>\n","      <td>-0.320096</td>\n","      <td>0.006946</td>\n","      <td>-0.196671</td>\n","      <td>-0.396228</td>\n","      <td>0.115823</td>\n","      <td>0.360166</td>\n","      <td>-0.096670</td>\n","      <td>0.039806</td>\n","      <td>0.451213</td>\n","      <td>-0.267765</td>\n","      <td>0.124315</td>\n","      <td>0.249646</td>\n","      <td>-0.367936</td>\n","      <td>-0.043657</td>\n","      <td>-0.022469</td>\n","      <td>0.261491</td>\n","      <td>0.099743</td>\n","      <td>-0.213620</td>\n","      <td>-0.189256</td>\n","      <td>-0.105234</td>\n","      <td>-0.299287</td>\n","      <td>0.109172</td>\n","      <td>-0.152034</td>\n","      <td>-0.175036</td>\n","      <td>0.098903</td>\n","      <td>-0.160676</td>\n","      <td>0.029059</td>\n","      <td>0.270451</td>\n","      <td>0.076392</td>\n","      <td>-0.046378</td>\n","      <td>-0.277001</td>\n","      <td>0.104086</td>\n","      <td>-0.090018</td>\n","      <td>-0.100922</td>\n","      <td>0.131680</td>\n","      <td>0.024087</td>\n","      <td>-0.158221</td>\n","      <td>0.047898</td>\n","      <td>...</td>\n","      <td>-0.010769</td>\n","      <td>0.147479</td>\n","      <td>-0.074286</td>\n","      <td>0.275063</td>\n","      <td>-0.389159</td>\n","      <td>0.159046</td>\n","      <td>-0.019297</td>\n","      <td>-0.175742</td>\n","      <td>0.086575</td>\n","      <td>-0.112550</td>\n","      <td>-0.193133</td>\n","      <td>0.107199</td>\n","      <td>-0.088026</td>\n","      <td>0.029302</td>\n","      <td>0.194172</td>\n","      <td>0.284729</td>\n","      <td>0.118845</td>\n","      <td>0.067868</td>\n","      <td>0.018238</td>\n","      <td>0.029295</td>\n","      <td>-0.110705</td>\n","      <td>-0.286266</td>\n","      <td>-0.202426</td>\n","      <td>-0.104493</td>\n","      <td>-9.110782</td>\n","      <td>-0.043799</td>\n","      <td>-0.111329</td>\n","      <td>0.024805</td>\n","      <td>0.037790</td>\n","      <td>0.062525</td>\n","      <td>0.081043</td>\n","      <td>-0.333182</td>\n","      <td>0.135394</td>\n","      <td>0.122937</td>\n","      <td>-0.028214</td>\n","      <td>-0.039936</td>\n","      <td>-0.211770</td>\n","      <td>-0.091390</td>\n","      <td>0.235144</td>\n","      <td>0.060140</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.270068</td>\n","      <td>0.286835</td>\n","      <td>-0.312244</td>\n","      <td>-0.083724</td>\n","      <td>-0.371326</td>\n","      <td>-0.175925</td>\n","      <td>0.310784</td>\n","      <td>0.115957</td>\n","      <td>-0.070045</td>\n","      <td>-0.255145</td>\n","      <td>-0.010835</td>\n","      <td>0.018113</td>\n","      <td>-0.028804</td>\n","      <td>0.281229</td>\n","      <td>0.261835</td>\n","      <td>-0.047650</td>\n","      <td>-0.139911</td>\n","      <td>0.303671</td>\n","      <td>0.401240</td>\n","      <td>0.039095</td>\n","      <td>0.122667</td>\n","      <td>-0.133192</td>\n","      <td>-0.073310</td>\n","      <td>-0.188598</td>\n","      <td>-0.249804</td>\n","      <td>-0.148067</td>\n","      <td>0.042947</td>\n","      <td>-0.225651</td>\n","      <td>0.076616</td>\n","      <td>-0.057281</td>\n","      <td>0.214792</td>\n","      <td>0.259625</td>\n","      <td>-0.402984</td>\n","      <td>0.111978</td>\n","      <td>0.161541</td>\n","      <td>0.090459</td>\n","      <td>0.094468</td>\n","      <td>0.162238</td>\n","      <td>0.264531</td>\n","      <td>0.189335</td>\n","      <td>...</td>\n","      <td>-0.247232</td>\n","      <td>-0.155336</td>\n","      <td>-0.072469</td>\n","      <td>0.505717</td>\n","      <td>-0.052829</td>\n","      <td>0.117284</td>\n","      <td>-0.146075</td>\n","      <td>-0.117058</td>\n","      <td>-0.195858</td>\n","      <td>0.039858</td>\n","      <td>-0.382877</td>\n","      <td>0.371591</td>\n","      <td>0.139386</td>\n","      <td>-0.021546</td>\n","      <td>0.052934</td>\n","      <td>0.125495</td>\n","      <td>0.533277</td>\n","      <td>0.321418</td>\n","      <td>0.190057</td>\n","      <td>-0.311492</td>\n","      <td>-0.189408</td>\n","      <td>-0.094566</td>\n","      <td>0.422168</td>\n","      <td>0.430083</td>\n","      <td>-7.357372</td>\n","      <td>0.060102</td>\n","      <td>-0.136205</td>\n","      <td>-0.376616</td>\n","      <td>-0.042260</td>\n","      <td>-0.169703</td>\n","      <td>0.170489</td>\n","      <td>-0.201935</td>\n","      <td>0.044900</td>\n","      <td>-0.207599</td>\n","      <td>0.126979</td>\n","      <td>0.269769</td>\n","      <td>-0.073095</td>\n","      <td>-0.206223</td>\n","      <td>0.433744</td>\n","      <td>0.234084</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.044999</td>\n","      <td>0.103974</td>\n","      <td>-0.273873</td>\n","      <td>0.062710</td>\n","      <td>-0.203671</td>\n","      <td>-0.184251</td>\n","      <td>0.252243</td>\n","      <td>0.580784</td>\n","      <td>-0.227311</td>\n","      <td>-0.122951</td>\n","      <td>0.451432</td>\n","      <td>0.020150</td>\n","      <td>0.049043</td>\n","      <td>0.352111</td>\n","      <td>0.256963</td>\n","      <td>0.119588</td>\n","      <td>-0.190546</td>\n","      <td>0.499676</td>\n","      <td>0.427755</td>\n","      <td>0.083106</td>\n","      <td>-0.256669</td>\n","      <td>-0.257225</td>\n","      <td>-0.117288</td>\n","      <td>0.003841</td>\n","      <td>0.305160</td>\n","      <td>-0.194973</td>\n","      <td>-0.085303</td>\n","      <td>-0.172681</td>\n","      <td>0.266695</td>\n","      <td>-0.238845</td>\n","      <td>-0.007221</td>\n","      <td>0.306875</td>\n","      <td>0.022228</td>\n","      <td>-0.027960</td>\n","      <td>0.263643</td>\n","      <td>-0.086819</td>\n","      <td>0.064743</td>\n","      <td>-0.166808</td>\n","      <td>0.038233</td>\n","      <td>0.162414</td>\n","      <td>...</td>\n","      <td>0.323896</td>\n","      <td>-0.070453</td>\n","      <td>-0.169819</td>\n","      <td>0.171146</td>\n","      <td>0.061148</td>\n","      <td>0.177073</td>\n","      <td>-0.132677</td>\n","      <td>-0.064897</td>\n","      <td>-0.192132</td>\n","      <td>-0.256930</td>\n","      <td>-0.395001</td>\n","      <td>0.397926</td>\n","      <td>0.119960</td>\n","      <td>0.015796</td>\n","      <td>-0.011583</td>\n","      <td>0.267547</td>\n","      <td>0.597411</td>\n","      <td>0.016349</td>\n","      <td>0.398271</td>\n","      <td>0.018641</td>\n","      <td>-0.199599</td>\n","      <td>-0.225537</td>\n","      <td>0.404656</td>\n","      <td>0.197267</td>\n","      <td>-8.405067</td>\n","      <td>0.052658</td>\n","      <td>-0.291597</td>\n","      <td>-0.414220</td>\n","      <td>-0.030366</td>\n","      <td>-0.260031</td>\n","      <td>0.149828</td>\n","      <td>-0.356936</td>\n","      <td>-0.014905</td>\n","      <td>-0.132931</td>\n","      <td>0.180756</td>\n","      <td>0.205888</td>\n","      <td>-0.224388</td>\n","      <td>-0.503448</td>\n","      <td>0.048857</td>\n","      <td>0.244911</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.017118</td>\n","      <td>0.121252</td>\n","      <td>-0.644177</td>\n","      <td>-0.120316</td>\n","      <td>-0.136243</td>\n","      <td>-0.494843</td>\n","      <td>0.447419</td>\n","      <td>0.686055</td>\n","      <td>-0.444794</td>\n","      <td>-0.281085</td>\n","      <td>0.090090</td>\n","      <td>-0.233360</td>\n","      <td>-0.042814</td>\n","      <td>0.035806</td>\n","      <td>0.235632</td>\n","      <td>-0.089441</td>\n","      <td>-0.000720</td>\n","      <td>0.561433</td>\n","      <td>0.362729</td>\n","      <td>0.034237</td>\n","      <td>-0.137094</td>\n","      <td>-0.155068</td>\n","      <td>-0.099510</td>\n","      <td>-0.216670</td>\n","      <td>-0.217682</td>\n","      <td>-0.000062</td>\n","      <td>-0.171357</td>\n","      <td>-0.095839</td>\n","      <td>0.192478</td>\n","      <td>-0.204740</td>\n","      <td>-0.212839</td>\n","      <td>0.535659</td>\n","      <td>-0.343187</td>\n","      <td>0.064332</td>\n","      <td>0.146969</td>\n","      <td>0.006006</td>\n","      <td>0.191289</td>\n","      <td>0.089183</td>\n","      <td>0.103925</td>\n","      <td>0.000242</td>\n","      <td>...</td>\n","      <td>0.219435</td>\n","      <td>-0.385291</td>\n","      <td>0.141750</td>\n","      <td>0.083287</td>\n","      <td>-0.065311</td>\n","      <td>0.208528</td>\n","      <td>-0.201536</td>\n","      <td>-0.064400</td>\n","      <td>-0.075216</td>\n","      <td>0.059751</td>\n","      <td>-0.390485</td>\n","      <td>0.333034</td>\n","      <td>0.330601</td>\n","      <td>0.071649</td>\n","      <td>-0.061794</td>\n","      <td>0.281069</td>\n","      <td>0.285943</td>\n","      <td>0.143600</td>\n","      <td>0.656836</td>\n","      <td>-0.196885</td>\n","      <td>-0.124114</td>\n","      <td>-0.092115</td>\n","      <td>0.813195</td>\n","      <td>0.426665</td>\n","      <td>-7.306691</td>\n","      <td>-0.044458</td>\n","      <td>-0.576241</td>\n","      <td>-0.317755</td>\n","      <td>-0.150216</td>\n","      <td>-0.361833</td>\n","      <td>0.193215</td>\n","      <td>-0.388127</td>\n","      <td>0.219215</td>\n","      <td>-0.101050</td>\n","      <td>0.335746</td>\n","      <td>0.036126</td>\n","      <td>-0.130592</td>\n","      <td>-0.519933</td>\n","      <td>0.330751</td>\n","      <td>0.133035</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.120396</td>\n","      <td>-0.007655</td>\n","      <td>0.141095</td>\n","      <td>-0.126055</td>\n","      <td>-0.614516</td>\n","      <td>-0.575045</td>\n","      <td>0.409321</td>\n","      <td>0.694351</td>\n","      <td>-0.102841</td>\n","      <td>-0.357710</td>\n","      <td>-0.110360</td>\n","      <td>-0.194320</td>\n","      <td>0.162100</td>\n","      <td>0.205439</td>\n","      <td>0.172931</td>\n","      <td>-0.155892</td>\n","      <td>-0.119357</td>\n","      <td>0.824795</td>\n","      <td>0.201161</td>\n","      <td>-0.070720</td>\n","      <td>-0.113191</td>\n","      <td>-0.431878</td>\n","      <td>0.312597</td>\n","      <td>-0.259597</td>\n","      <td>-0.187396</td>\n","      <td>0.089399</td>\n","      <td>-0.034187</td>\n","      <td>-0.286472</td>\n","      <td>0.200937</td>\n","      <td>0.183425</td>\n","      <td>0.052091</td>\n","      <td>-0.007813</td>\n","      <td>0.146523</td>\n","      <td>0.149793</td>\n","      <td>0.259749</td>\n","      <td>-0.357512</td>\n","      <td>0.372107</td>\n","      <td>0.203365</td>\n","      <td>0.465850</td>\n","      <td>0.209063</td>\n","      <td>...</td>\n","      <td>0.081589</td>\n","      <td>-0.391451</td>\n","      <td>0.185456</td>\n","      <td>0.415927</td>\n","      <td>-0.321061</td>\n","      <td>-0.070721</td>\n","      <td>-0.231353</td>\n","      <td>-0.135785</td>\n","      <td>-0.247936</td>\n","      <td>-0.287477</td>\n","      <td>-0.349191</td>\n","      <td>0.205080</td>\n","      <td>0.151861</td>\n","      <td>-0.017550</td>\n","      <td>-0.429742</td>\n","      <td>0.257255</td>\n","      <td>0.040498</td>\n","      <td>0.510252</td>\n","      <td>0.492107</td>\n","      <td>-0.220305</td>\n","      <td>-0.234759</td>\n","      <td>-0.011245</td>\n","      <td>0.192274</td>\n","      <td>0.427723</td>\n","      <td>-6.496952</td>\n","      <td>-0.078097</td>\n","      <td>-0.173186</td>\n","      <td>-0.174259</td>\n","      <td>-0.266402</td>\n","      <td>-0.830452</td>\n","      <td>-0.207477</td>\n","      <td>-0.430180</td>\n","      <td>0.164151</td>\n","      <td>-0.251946</td>\n","      <td>0.283212</td>\n","      <td>0.026835</td>\n","      <td>-0.079621</td>\n","      <td>-0.697007</td>\n","      <td>0.365411</td>\n","      <td>0.888040</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.306679</td>\n","      <td>-0.197560</td>\n","      <td>0.229467</td>\n","      <td>-0.058572</td>\n","      <td>-0.187343</td>\n","      <td>-0.733091</td>\n","      <td>0.240553</td>\n","      <td>0.565845</td>\n","      <td>-0.058151</td>\n","      <td>-0.468631</td>\n","      <td>0.368518</td>\n","      <td>-0.013291</td>\n","      <td>-0.050753</td>\n","      <td>0.344040</td>\n","      <td>-0.126581</td>\n","      <td>0.009933</td>\n","      <td>-0.089320</td>\n","      <td>0.075200</td>\n","      <td>0.510305</td>\n","      <td>0.056196</td>\n","      <td>-0.180038</td>\n","      <td>-0.266415</td>\n","      <td>0.195196</td>\n","      <td>0.026772</td>\n","      <td>0.086859</td>\n","      <td>0.084377</td>\n","      <td>-0.099780</td>\n","      <td>0.075754</td>\n","      <td>0.116565</td>\n","      <td>0.040138</td>\n","      <td>-0.176776</td>\n","      <td>-0.008303</td>\n","      <td>-0.035012</td>\n","      <td>-0.190269</td>\n","      <td>0.102449</td>\n","      <td>0.183731</td>\n","      <td>0.003760</td>\n","      <td>0.185053</td>\n","      <td>0.090468</td>\n","      <td>0.258771</td>\n","      <td>...</td>\n","      <td>-0.034923</td>\n","      <td>-0.054713</td>\n","      <td>0.029445</td>\n","      <td>0.137261</td>\n","      <td>0.138442</td>\n","      <td>0.285577</td>\n","      <td>0.108432</td>\n","      <td>-0.076638</td>\n","      <td>-0.372027</td>\n","      <td>0.037054</td>\n","      <td>-0.120139</td>\n","      <td>0.035255</td>\n","      <td>-0.183725</td>\n","      <td>0.101950</td>\n","      <td>-0.261305</td>\n","      <td>0.427368</td>\n","      <td>-0.045248</td>\n","      <td>0.191705</td>\n","      <td>0.011393</td>\n","      <td>-0.193674</td>\n","      <td>0.233372</td>\n","      <td>-0.077690</td>\n","      <td>-0.244265</td>\n","      <td>-0.184768</td>\n","      <td>-6.755476</td>\n","      <td>0.025335</td>\n","      <td>-0.015197</td>\n","      <td>0.288077</td>\n","      <td>0.009024</td>\n","      <td>-0.351086</td>\n","      <td>-0.446926</td>\n","      <td>-0.255326</td>\n","      <td>-0.099235</td>\n","      <td>-0.154914</td>\n","      <td>0.170366</td>\n","      <td>-0.141866</td>\n","      <td>-0.095921</td>\n","      <td>-0.277485</td>\n","      <td>0.227058</td>\n","      <td>0.238263</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.332425</td>\n","      <td>0.003333</td>\n","      <td>-0.251256</td>\n","      <td>0.182140</td>\n","      <td>-0.165938</td>\n","      <td>-0.275331</td>\n","      <td>0.182975</td>\n","      <td>0.160790</td>\n","      <td>-0.379758</td>\n","      <td>-0.244285</td>\n","      <td>0.099403</td>\n","      <td>-0.457855</td>\n","      <td>0.076864</td>\n","      <td>0.203388</td>\n","      <td>-0.064996</td>\n","      <td>0.001619</td>\n","      <td>-0.138622</td>\n","      <td>0.447935</td>\n","      <td>0.076835</td>\n","      <td>-0.027965</td>\n","      <td>-0.167361</td>\n","      <td>-0.270469</td>\n","      <td>0.043183</td>\n","      <td>-0.081642</td>\n","      <td>-0.368688</td>\n","      <td>-0.008497</td>\n","      <td>-0.061087</td>\n","      <td>0.115470</td>\n","      <td>-0.040156</td>\n","      <td>0.069567</td>\n","      <td>-0.112650</td>\n","      <td>0.463714</td>\n","      <td>-0.178319</td>\n","      <td>0.366436</td>\n","      <td>0.104355</td>\n","      <td>-0.093582</td>\n","      <td>-0.124677</td>\n","      <td>0.022660</td>\n","      <td>0.122956</td>\n","      <td>-0.043008</td>\n","      <td>...</td>\n","      <td>-0.196851</td>\n","      <td>-0.372139</td>\n","      <td>-0.298602</td>\n","      <td>0.242114</td>\n","      <td>0.014124</td>\n","      <td>0.187351</td>\n","      <td>-0.268485</td>\n","      <td>-0.143034</td>\n","      <td>-0.191448</td>\n","      <td>-0.061732</td>\n","      <td>-0.296879</td>\n","      <td>0.237495</td>\n","      <td>0.087510</td>\n","      <td>-0.045395</td>\n","      <td>0.046470</td>\n","      <td>0.119084</td>\n","      <td>0.546684</td>\n","      <td>0.387078</td>\n","      <td>0.070079</td>\n","      <td>0.007279</td>\n","      <td>-0.383547</td>\n","      <td>0.025572</td>\n","      <td>0.393464</td>\n","      <td>0.459205</td>\n","      <td>-7.115532</td>\n","      <td>0.155110</td>\n","      <td>-0.115857</td>\n","      <td>-0.218860</td>\n","      <td>-0.282082</td>\n","      <td>-0.162134</td>\n","      <td>-0.151747</td>\n","      <td>-0.435132</td>\n","      <td>-0.042474</td>\n","      <td>-0.070441</td>\n","      <td>0.218808</td>\n","      <td>0.030028</td>\n","      <td>-0.018009</td>\n","      <td>-0.170354</td>\n","      <td>0.200916</td>\n","      <td>0.316067</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.385488</td>\n","      <td>0.147821</td>\n","      <td>0.381968</td>\n","      <td>0.111567</td>\n","      <td>-0.290401</td>\n","      <td>-0.659185</td>\n","      <td>0.516419</td>\n","      <td>0.889231</td>\n","      <td>0.062068</td>\n","      <td>-0.557178</td>\n","      <td>0.305354</td>\n","      <td>-0.089696</td>\n","      <td>0.333135</td>\n","      <td>0.707216</td>\n","      <td>-0.246121</td>\n","      <td>-0.178713</td>\n","      <td>-0.258887</td>\n","      <td>0.283793</td>\n","      <td>0.259925</td>\n","      <td>-0.131811</td>\n","      <td>0.095022</td>\n","      <td>-0.216171</td>\n","      <td>0.236896</td>\n","      <td>0.100517</td>\n","      <td>-0.190133</td>\n","      <td>-0.105484</td>\n","      <td>0.068192</td>\n","      <td>0.205345</td>\n","      <td>0.022568</td>\n","      <td>-0.028361</td>\n","      <td>0.281271</td>\n","      <td>0.171402</td>\n","      <td>-0.441261</td>\n","      <td>0.366951</td>\n","      <td>-0.015714</td>\n","      <td>-0.135328</td>\n","      <td>-0.213696</td>\n","      <td>0.304990</td>\n","      <td>0.100781</td>\n","      <td>0.035153</td>\n","      <td>...</td>\n","      <td>0.011131</td>\n","      <td>-0.073032</td>\n","      <td>-0.199011</td>\n","      <td>-0.197592</td>\n","      <td>0.012442</td>\n","      <td>0.013998</td>\n","      <td>-0.086391</td>\n","      <td>0.063594</td>\n","      <td>-0.419786</td>\n","      <td>-0.096817</td>\n","      <td>-0.302082</td>\n","      <td>0.328988</td>\n","      <td>0.303028</td>\n","      <td>0.026370</td>\n","      <td>-0.323927</td>\n","      <td>0.529230</td>\n","      <td>-0.222225</td>\n","      <td>0.463107</td>\n","      <td>-0.020350</td>\n","      <td>-0.060802</td>\n","      <td>0.194464</td>\n","      <td>-0.229696</td>\n","      <td>-0.074280</td>\n","      <td>0.245858</td>\n","      <td>-5.102902</td>\n","      <td>-0.032969</td>\n","      <td>0.148119</td>\n","      <td>0.046616</td>\n","      <td>-0.488129</td>\n","      <td>-0.677289</td>\n","      <td>-0.417181</td>\n","      <td>-0.123907</td>\n","      <td>-0.034119</td>\n","      <td>0.002334</td>\n","      <td>0.094393</td>\n","      <td>-0.308248</td>\n","      <td>0.295410</td>\n","      <td>-0.562011</td>\n","      <td>0.642930</td>\n","      <td>0.391632</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.082006</td>\n","      <td>-0.050099</td>\n","      <td>0.251327</td>\n","      <td>0.156538</td>\n","      <td>0.017610</td>\n","      <td>-0.185316</td>\n","      <td>0.028851</td>\n","      <td>0.348973</td>\n","      <td>-0.106276</td>\n","      <td>-0.394803</td>\n","      <td>0.258577</td>\n","      <td>-0.054708</td>\n","      <td>0.216633</td>\n","      <td>0.250994</td>\n","      <td>-0.326224</td>\n","      <td>-0.215606</td>\n","      <td>0.003612</td>\n","      <td>0.212238</td>\n","      <td>0.183208</td>\n","      <td>0.040695</td>\n","      <td>0.160620</td>\n","      <td>-0.418663</td>\n","      <td>0.425706</td>\n","      <td>-0.174038</td>\n","      <td>0.089943</td>\n","      <td>-0.082298</td>\n","      <td>0.089739</td>\n","      <td>0.035447</td>\n","      <td>-0.113299</td>\n","      <td>-0.072691</td>\n","      <td>-0.152181</td>\n","      <td>0.208996</td>\n","      <td>0.177419</td>\n","      <td>0.091984</td>\n","      <td>-0.100145</td>\n","      <td>0.052098</td>\n","      <td>-0.045827</td>\n","      <td>0.417051</td>\n","      <td>0.136236</td>\n","      <td>-0.066140</td>\n","      <td>...</td>\n","      <td>0.103624</td>\n","      <td>-0.187602</td>\n","      <td>-0.100075</td>\n","      <td>-0.398236</td>\n","      <td>0.187638</td>\n","      <td>0.138101</td>\n","      <td>-0.039383</td>\n","      <td>-0.050993</td>\n","      <td>-0.278638</td>\n","      <td>-0.121926</td>\n","      <td>-0.021772</td>\n","      <td>0.074883</td>\n","      <td>0.247438</td>\n","      <td>0.312501</td>\n","      <td>-0.069777</td>\n","      <td>0.655760</td>\n","      <td>-0.143605</td>\n","      <td>0.109404</td>\n","      <td>0.192065</td>\n","      <td>-0.032839</td>\n","      <td>0.099515</td>\n","      <td>-0.087534</td>\n","      <td>-0.146372</td>\n","      <td>0.068230</td>\n","      <td>-6.386380</td>\n","      <td>-0.254540</td>\n","      <td>-0.045042</td>\n","      <td>-0.101088</td>\n","      <td>-0.269359</td>\n","      <td>-0.491501</td>\n","      <td>-0.548597</td>\n","      <td>-0.205302</td>\n","      <td>0.059601</td>\n","      <td>0.159329</td>\n","      <td>0.079929</td>\n","      <td>0.084331</td>\n","      <td>-0.014158</td>\n","      <td>-0.301412</td>\n","      <td>0.455928</td>\n","      <td>0.307292</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.234583</td>\n","      <td>0.246118</td>\n","      <td>0.052081</td>\n","      <td>-0.023981</td>\n","      <td>-0.382841</td>\n","      <td>-0.771413</td>\n","      <td>0.261784</td>\n","      <td>0.953608</td>\n","      <td>-0.078587</td>\n","      <td>-0.215270</td>\n","      <td>0.466610</td>\n","      <td>0.265769</td>\n","      <td>0.306016</td>\n","      <td>0.110554</td>\n","      <td>0.116790</td>\n","      <td>-0.074714</td>\n","      <td>-0.122824</td>\n","      <td>0.343166</td>\n","      <td>0.486176</td>\n","      <td>-0.084548</td>\n","      <td>0.016450</td>\n","      <td>-0.212998</td>\n","      <td>-0.099676</td>\n","      <td>0.129987</td>\n","      <td>0.453402</td>\n","      <td>-0.083875</td>\n","      <td>-0.261909</td>\n","      <td>0.131570</td>\n","      <td>0.322284</td>\n","      <td>0.086199</td>\n","      <td>-0.280919</td>\n","      <td>0.055981</td>\n","      <td>0.341003</td>\n","      <td>-0.085578</td>\n","      <td>0.383432</td>\n","      <td>-0.493091</td>\n","      <td>0.313817</td>\n","      <td>0.076146</td>\n","      <td>0.087825</td>\n","      <td>0.188018</td>\n","      <td>...</td>\n","      <td>0.336105</td>\n","      <td>-0.182269</td>\n","      <td>0.106508</td>\n","      <td>0.345822</td>\n","      <td>-0.161164</td>\n","      <td>0.033880</td>\n","      <td>-0.209665</td>\n","      <td>-0.211244</td>\n","      <td>-0.627143</td>\n","      <td>-0.413599</td>\n","      <td>-0.361669</td>\n","      <td>0.198772</td>\n","      <td>0.260061</td>\n","      <td>0.287019</td>\n","      <td>-0.037754</td>\n","      <td>0.497421</td>\n","      <td>0.018599</td>\n","      <td>0.160919</td>\n","      <td>0.242704</td>\n","      <td>0.079516</td>\n","      <td>-0.168577</td>\n","      <td>-0.239024</td>\n","      <td>-0.194722</td>\n","      <td>0.290189</td>\n","      <td>-7.629736</td>\n","      <td>-0.020406</td>\n","      <td>0.020378</td>\n","      <td>0.010418</td>\n","      <td>0.191347</td>\n","      <td>-0.701427</td>\n","      <td>-0.479299</td>\n","      <td>0.019886</td>\n","      <td>0.138583</td>\n","      <td>0.001411</td>\n","      <td>0.362004</td>\n","      <td>-0.098332</td>\n","      <td>0.121972</td>\n","      <td>-0.310039</td>\n","      <td>0.310394</td>\n","      <td>0.231359</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.124086</td>\n","      <td>0.132068</td>\n","      <td>-0.380084</td>\n","      <td>-0.071346</td>\n","      <td>-0.455732</td>\n","      <td>-0.507978</td>\n","      <td>0.337711</td>\n","      <td>0.618985</td>\n","      <td>0.013416</td>\n","      <td>-0.728402</td>\n","      <td>-0.118403</td>\n","      <td>-0.150059</td>\n","      <td>0.307949</td>\n","      <td>-0.144825</td>\n","      <td>-0.474081</td>\n","      <td>-0.093958</td>\n","      <td>0.042096</td>\n","      <td>0.533606</td>\n","      <td>0.278246</td>\n","      <td>-0.176002</td>\n","      <td>-0.346150</td>\n","      <td>-0.347391</td>\n","      <td>-0.086304</td>\n","      <td>-0.184243</td>\n","      <td>0.149494</td>\n","      <td>0.295229</td>\n","      <td>0.415868</td>\n","      <td>0.163500</td>\n","      <td>0.105076</td>\n","      <td>0.045778</td>\n","      <td>0.027181</td>\n","      <td>0.605201</td>\n","      <td>-0.202732</td>\n","      <td>0.230891</td>\n","      <td>-0.044590</td>\n","      <td>-0.315158</td>\n","      <td>0.331863</td>\n","      <td>-0.015150</td>\n","      <td>0.110762</td>\n","      <td>-0.223284</td>\n","      <td>...</td>\n","      <td>-0.125612</td>\n","      <td>-0.366487</td>\n","      <td>0.017255</td>\n","      <td>0.180199</td>\n","      <td>-0.058379</td>\n","      <td>0.121288</td>\n","      <td>-0.023621</td>\n","      <td>0.100229</td>\n","      <td>-0.600227</td>\n","      <td>-0.000308</td>\n","      <td>-0.251114</td>\n","      <td>0.316661</td>\n","      <td>0.174228</td>\n","      <td>0.278341</td>\n","      <td>0.018738</td>\n","      <td>0.247394</td>\n","      <td>-0.016453</td>\n","      <td>0.089121</td>\n","      <td>0.478565</td>\n","      <td>-0.007028</td>\n","      <td>-0.042854</td>\n","      <td>-0.191792</td>\n","      <td>0.172451</td>\n","      <td>0.416589</td>\n","      <td>-5.788245</td>\n","      <td>0.096501</td>\n","      <td>-0.228130</td>\n","      <td>0.060843</td>\n","      <td>-0.291736</td>\n","      <td>-0.355873</td>\n","      <td>-0.231041</td>\n","      <td>-0.338670</td>\n","      <td>0.048154</td>\n","      <td>0.133655</td>\n","      <td>0.102707</td>\n","      <td>0.142751</td>\n","      <td>-0.068805</td>\n","      <td>-0.441668</td>\n","      <td>0.172203</td>\n","      <td>0.700031</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.040226</td>\n","      <td>0.139149</td>\n","      <td>-0.208071</td>\n","      <td>-0.044919</td>\n","      <td>-0.058333</td>\n","      <td>-0.292738</td>\n","      <td>0.255774</td>\n","      <td>0.179188</td>\n","      <td>-0.138437</td>\n","      <td>-0.373358</td>\n","      <td>0.164483</td>\n","      <td>-0.290833</td>\n","      <td>0.274803</td>\n","      <td>0.134846</td>\n","      <td>0.173890</td>\n","      <td>0.012654</td>\n","      <td>-0.017211</td>\n","      <td>0.452611</td>\n","      <td>0.014007</td>\n","      <td>-0.165813</td>\n","      <td>-0.355799</td>\n","      <td>-0.149901</td>\n","      <td>0.111559</td>\n","      <td>-0.307892</td>\n","      <td>-0.314118</td>\n","      <td>-0.210411</td>\n","      <td>-0.155659</td>\n","      <td>-0.245071</td>\n","      <td>-0.035172</td>\n","      <td>-0.076862</td>\n","      <td>-0.153477</td>\n","      <td>0.221057</td>\n","      <td>0.010143</td>\n","      <td>-0.031290</td>\n","      <td>0.158074</td>\n","      <td>-0.278905</td>\n","      <td>0.257251</td>\n","      <td>-0.168476</td>\n","      <td>0.310683</td>\n","      <td>0.163375</td>\n","      <td>...</td>\n","      <td>0.154151</td>\n","      <td>-0.275607</td>\n","      <td>-0.117402</td>\n","      <td>0.496063</td>\n","      <td>-0.092437</td>\n","      <td>-0.150577</td>\n","      <td>-0.174143</td>\n","      <td>0.180393</td>\n","      <td>-0.224889</td>\n","      <td>0.096810</td>\n","      <td>-0.296380</td>\n","      <td>0.530982</td>\n","      <td>0.223446</td>\n","      <td>0.111614</td>\n","      <td>0.224023</td>\n","      <td>-0.002329</td>\n","      <td>0.231890</td>\n","      <td>0.412114</td>\n","      <td>0.324033</td>\n","      <td>-0.102307</td>\n","      <td>-0.252018</td>\n","      <td>-0.089469</td>\n","      <td>0.157217</td>\n","      <td>0.632020</td>\n","      <td>-6.835304</td>\n","      <td>-0.181090</td>\n","      <td>-0.279616</td>\n","      <td>-0.074956</td>\n","      <td>-0.326894</td>\n","      <td>-0.425082</td>\n","      <td>0.019343</td>\n","      <td>-0.240397</td>\n","      <td>0.001383</td>\n","      <td>0.066520</td>\n","      <td>0.151348</td>\n","      <td>0.145024</td>\n","      <td>0.002757</td>\n","      <td>-0.468389</td>\n","      <td>0.183341</td>\n","      <td>0.336686</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.008627</td>\n","      <td>-0.243472</td>\n","      <td>-0.203433</td>\n","      <td>-0.052717</td>\n","      <td>0.014720</td>\n","      <td>-0.402716</td>\n","      <td>0.127374</td>\n","      <td>1.017278</td>\n","      <td>-0.271137</td>\n","      <td>-0.345197</td>\n","      <td>0.291340</td>\n","      <td>-0.079157</td>\n","      <td>0.024348</td>\n","      <td>0.134456</td>\n","      <td>-0.098167</td>\n","      <td>0.230313</td>\n","      <td>-0.364743</td>\n","      <td>0.395652</td>\n","      <td>0.397192</td>\n","      <td>0.037877</td>\n","      <td>-0.321392</td>\n","      <td>-0.374464</td>\n","      <td>-0.366522</td>\n","      <td>0.192992</td>\n","      <td>0.133147</td>\n","      <td>-0.109352</td>\n","      <td>0.388136</td>\n","      <td>0.163615</td>\n","      <td>0.156756</td>\n","      <td>-0.308208</td>\n","      <td>-0.365494</td>\n","      <td>0.275232</td>\n","      <td>-0.078013</td>\n","      <td>-0.397553</td>\n","      <td>0.362473</td>\n","      <td>0.063882</td>\n","      <td>0.120701</td>\n","      <td>-0.144189</td>\n","      <td>-0.502473</td>\n","      <td>0.123995</td>\n","      <td>...</td>\n","      <td>0.692220</td>\n","      <td>-0.424446</td>\n","      <td>0.351386</td>\n","      <td>-0.030348</td>\n","      <td>-0.296540</td>\n","      <td>0.108995</td>\n","      <td>-0.242066</td>\n","      <td>0.239420</td>\n","      <td>-0.277151</td>\n","      <td>0.103918</td>\n","      <td>-0.346816</td>\n","      <td>0.555393</td>\n","      <td>0.212405</td>\n","      <td>0.101648</td>\n","      <td>0.034199</td>\n","      <td>0.237494</td>\n","      <td>0.071448</td>\n","      <td>0.236918</td>\n","      <td>-0.003127</td>\n","      <td>0.187134</td>\n","      <td>0.222514</td>\n","      <td>-0.382257</td>\n","      <td>0.104762</td>\n","      <td>-0.108728</td>\n","      <td>-7.809752</td>\n","      <td>0.155322</td>\n","      <td>-0.011627</td>\n","      <td>0.230037</td>\n","      <td>-0.006857</td>\n","      <td>-0.566705</td>\n","      <td>-0.156460</td>\n","      <td>-0.481143</td>\n","      <td>-0.177091</td>\n","      <td>-0.252120</td>\n","      <td>0.206911</td>\n","      <td>-0.189164</td>\n","      <td>-0.314118</td>\n","      <td>-0.545151</td>\n","      <td>0.057258</td>\n","      <td>0.467176</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.118002</td>\n","      <td>0.107717</td>\n","      <td>0.056065</td>\n","      <td>-0.079658</td>\n","      <td>-0.369217</td>\n","      <td>-0.374907</td>\n","      <td>0.621891</td>\n","      <td>0.537519</td>\n","      <td>0.221437</td>\n","      <td>-0.741684</td>\n","      <td>-0.230858</td>\n","      <td>-0.132275</td>\n","      <td>0.107097</td>\n","      <td>0.527438</td>\n","      <td>0.318544</td>\n","      <td>-0.234124</td>\n","      <td>-0.255790</td>\n","      <td>0.437456</td>\n","      <td>0.247798</td>\n","      <td>-0.012869</td>\n","      <td>0.070592</td>\n","      <td>-0.264749</td>\n","      <td>0.045921</td>\n","      <td>-0.254902</td>\n","      <td>-0.066860</td>\n","      <td>-0.323506</td>\n","      <td>-0.302175</td>\n","      <td>-0.296729</td>\n","      <td>-0.053096</td>\n","      <td>0.048827</td>\n","      <td>-0.204207</td>\n","      <td>0.586557</td>\n","      <td>-0.164738</td>\n","      <td>0.080503</td>\n","      <td>0.012691</td>\n","      <td>-0.227452</td>\n","      <td>0.209855</td>\n","      <td>0.227574</td>\n","      <td>0.395985</td>\n","      <td>-0.008939</td>\n","      <td>...</td>\n","      <td>0.105044</td>\n","      <td>-0.579289</td>\n","      <td>0.211984</td>\n","      <td>0.744722</td>\n","      <td>-0.228711</td>\n","      <td>-0.106889</td>\n","      <td>-0.082149</td>\n","      <td>0.012926</td>\n","      <td>-0.660184</td>\n","      <td>0.198519</td>\n","      <td>-0.342731</td>\n","      <td>0.489673</td>\n","      <td>0.377502</td>\n","      <td>0.125455</td>\n","      <td>0.020465</td>\n","      <td>0.213418</td>\n","      <td>0.435805</td>\n","      <td>0.356637</td>\n","      <td>0.153809</td>\n","      <td>-0.212168</td>\n","      <td>-0.042098</td>\n","      <td>0.094238</td>\n","      <td>0.439670</td>\n","      <td>0.478331</td>\n","      <td>-5.934722</td>\n","      <td>-0.090053</td>\n","      <td>-0.341946</td>\n","      <td>-0.041689</td>\n","      <td>-0.089341</td>\n","      <td>-0.406540</td>\n","      <td>0.062831</td>\n","      <td>-0.445625</td>\n","      <td>0.054723</td>\n","      <td>-0.083415</td>\n","      <td>0.408333</td>\n","      <td>-0.018860</td>\n","      <td>0.086937</td>\n","      <td>-0.433579</td>\n","      <td>0.360580</td>\n","      <td>0.285819</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.144243</td>\n","      <td>0.119319</td>\n","      <td>0.106254</td>\n","      <td>0.027941</td>\n","      <td>-0.234331</td>\n","      <td>-0.296676</td>\n","      <td>0.489440</td>\n","      <td>0.717717</td>\n","      <td>-0.028220</td>\n","      <td>-0.285913</td>\n","      <td>0.115799</td>\n","      <td>-0.183653</td>\n","      <td>0.165603</td>\n","      <td>0.397931</td>\n","      <td>-0.093112</td>\n","      <td>-0.262423</td>\n","      <td>-0.180306</td>\n","      <td>0.471298</td>\n","      <td>0.413356</td>\n","      <td>-0.144182</td>\n","      <td>0.025572</td>\n","      <td>-0.418228</td>\n","      <td>-0.041644</td>\n","      <td>0.305667</td>\n","      <td>-0.038346</td>\n","      <td>-0.266129</td>\n","      <td>-0.242048</td>\n","      <td>-0.189879</td>\n","      <td>0.312479</td>\n","      <td>-0.222481</td>\n","      <td>0.180057</td>\n","      <td>0.266694</td>\n","      <td>-0.532643</td>\n","      <td>0.304341</td>\n","      <td>0.030089</td>\n","      <td>0.005153</td>\n","      <td>0.146696</td>\n","      <td>0.046045</td>\n","      <td>-0.082720</td>\n","      <td>0.275651</td>\n","      <td>...</td>\n","      <td>0.195104</td>\n","      <td>-0.022906</td>\n","      <td>0.264011</td>\n","      <td>0.511581</td>\n","      <td>-0.193433</td>\n","      <td>-0.062649</td>\n","      <td>-0.220464</td>\n","      <td>-0.101553</td>\n","      <td>0.092981</td>\n","      <td>-0.263775</td>\n","      <td>0.044065</td>\n","      <td>-0.332800</td>\n","      <td>0.172677</td>\n","      <td>0.070768</td>\n","      <td>0.039700</td>\n","      <td>0.388337</td>\n","      <td>0.205817</td>\n","      <td>0.110765</td>\n","      <td>0.070555</td>\n","      <td>-0.056660</td>\n","      <td>-0.160221</td>\n","      <td>-0.119676</td>\n","      <td>0.224814</td>\n","      <td>-0.069161</td>\n","      <td>-9.500246</td>\n","      <td>-0.166782</td>\n","      <td>-0.020930</td>\n","      <td>-0.292820</td>\n","      <td>0.144059</td>\n","      <td>-0.163234</td>\n","      <td>-0.007268</td>\n","      <td>-0.296565</td>\n","      <td>0.064282</td>\n","      <td>0.062619</td>\n","      <td>0.358558</td>\n","      <td>-0.035897</td>\n","      <td>-0.183948</td>\n","      <td>-0.038531</td>\n","      <td>-0.090076</td>\n","      <td>0.212957</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.243315</td>\n","      <td>0.244231</td>\n","      <td>0.192897</td>\n","      <td>-0.135821</td>\n","      <td>-0.610626</td>\n","      <td>-0.624679</td>\n","      <td>0.479913</td>\n","      <td>0.811540</td>\n","      <td>0.221024</td>\n","      <td>-0.735473</td>\n","      <td>-0.112052</td>\n","      <td>-0.410398</td>\n","      <td>0.089308</td>\n","      <td>0.536065</td>\n","      <td>0.503360</td>\n","      <td>-0.161101</td>\n","      <td>-0.597644</td>\n","      <td>0.877890</td>\n","      <td>0.360172</td>\n","      <td>0.155568</td>\n","      <td>0.062969</td>\n","      <td>-0.420423</td>\n","      <td>0.289577</td>\n","      <td>-0.188293</td>\n","      <td>0.022706</td>\n","      <td>-0.087902</td>\n","      <td>-0.049643</td>\n","      <td>-0.471225</td>\n","      <td>0.043292</td>\n","      <td>-0.071119</td>\n","      <td>-0.259127</td>\n","      <td>0.504318</td>\n","      <td>-0.111101</td>\n","      <td>0.179166</td>\n","      <td>0.372511</td>\n","      <td>-0.407052</td>\n","      <td>0.196280</td>\n","      <td>0.229137</td>\n","      <td>0.538405</td>\n","      <td>0.289201</td>\n","      <td>...</td>\n","      <td>-0.060043</td>\n","      <td>-0.757229</td>\n","      <td>0.212249</td>\n","      <td>0.647018</td>\n","      <td>-0.037014</td>\n","      <td>-0.059214</td>\n","      <td>-0.192607</td>\n","      <td>-0.253482</td>\n","      <td>-0.374870</td>\n","      <td>0.085478</td>\n","      <td>-0.233350</td>\n","      <td>0.380031</td>\n","      <td>0.640788</td>\n","      <td>0.049614</td>\n","      <td>-0.317109</td>\n","      <td>0.209884</td>\n","      <td>0.399843</td>\n","      <td>0.501757</td>\n","      <td>0.213228</td>\n","      <td>0.003307</td>\n","      <td>-0.028888</td>\n","      <td>0.184393</td>\n","      <td>0.693082</td>\n","      <td>0.454978</td>\n","      <td>-6.349967</td>\n","      <td>0.241672</td>\n","      <td>-0.232609</td>\n","      <td>-0.278350</td>\n","      <td>-0.372315</td>\n","      <td>-0.634160</td>\n","      <td>-0.027688</td>\n","      <td>-0.268128</td>\n","      <td>0.094421</td>\n","      <td>-0.279504</td>\n","      <td>0.256062</td>\n","      <td>-0.039771</td>\n","      <td>-0.080498</td>\n","      <td>-0.285824</td>\n","      <td>0.428468</td>\n","      <td>0.571091</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>16 rows × 768 columns</p>\n","</div>"],"text/plain":["    0.03897584  0.1705755  0.16306049  ...  -0.6573706  0.3568359  0.39976805\n","0    -0.031000  -0.039242   -0.320096  ...   -0.091390   0.235144    0.060140\n","1     0.270068   0.286835   -0.312244  ...   -0.206223   0.433744    0.234084\n","2     0.044999   0.103974   -0.273873  ...   -0.503448   0.048857    0.244911\n","3     0.017118   0.121252   -0.644177  ...   -0.519933   0.330751    0.133035\n","4     0.120396  -0.007655    0.141095  ...   -0.697007   0.365411    0.888040\n","5     0.306679  -0.197560    0.229467  ...   -0.277485   0.227058    0.238263\n","6     0.332425   0.003333   -0.251256  ...   -0.170354   0.200916    0.316067\n","7     0.385488   0.147821    0.381968  ...   -0.562011   0.642930    0.391632\n","8     0.082006  -0.050099    0.251327  ...   -0.301412   0.455928    0.307292\n","9     0.234583   0.246118    0.052081  ...   -0.310039   0.310394    0.231359\n","10    0.124086   0.132068   -0.380084  ...   -0.441668   0.172203    0.700031\n","11    0.040226   0.139149   -0.208071  ...   -0.468389   0.183341    0.336686\n","12    0.008627  -0.243472   -0.203433  ...   -0.545151   0.057258    0.467176\n","13    0.118002   0.107717    0.056065  ...   -0.433579   0.360580    0.285819\n","14    0.144243   0.119319    0.106254  ...   -0.038531  -0.090076    0.212957\n","15    0.243315   0.244231    0.192897  ...   -0.285824   0.428468    0.571091\n","\n","[16 rows x 768 columns]"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gJsxNmly2F_8","executionInfo":{"status":"ok","timestamp":1620040167181,"user_tz":-540,"elapsed":4109268,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"cbd290e9-be01-48b0-c0ec-4ae7be950764"},"source":["from keras.callbacks import EarlyStopping\n","from sklearn.metrics import cohen_kappa_score\n","\n","batch_size = 64\n","\n","cv = KFold(n_splits=5, shuffle=True)\n","cnt = 0\n","for traincv, testcv in cv.split(embedded_essay):\n","  train_gen = DataGenerator(traincv, batch_size=batch_size)\n","  test_gen = DataGenerator(testcv, batch_size=batch_size, shuffle=False)\n","  train_y = y.iloc[traincv]\n","  test_y = y.iloc[testcv]\n","\n","  train_steps = len(traincv) // batch_size\n","  valid_steps = len(testcv) // batch_size\n","\n","  early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n","  sentence_model = get_sentence_model3()\n","  sentence_model.fit(train_gen, steps_per_epoch=train_steps, validation_steps=valid_steps,\n","                    epochs=50, callbacks=[early_stopping])\n","\n","  y_sent_pred = sentence_model.predict(test_gen) *100\n","  y_sent_pred = np.round(y_sent_pred)\n","\n","  sentence_result = cohen_kappa_score(np.array(np.round(test_y * 100)), y_sent_pred, weights='quadratic')\n","  print(\"Kappa Score\", cnt, \": {}\".format(sentence_result))\n","  cnt += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru (GRU)                    (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_1 (GRU)                  (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_2 (GRU)                  (None, 64)                37248     \n","_________________________________________________________________\n","dropout (Dropout)            (None, 64)                0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 49s 98ms/step - loss: 0.3217 - mae: 0.4667\n","Epoch 2/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0632 - mae: 0.2005\n","Epoch 3/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0368 - mae: 0.1509\n","Epoch 4/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0353 - mae: 0.1462\n","Epoch 5/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0329 - mae: 0.1414\n","Epoch 6/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0321 - mae: 0.1391\n","Epoch 7/50\n","162/162 [==============================] - 16s 96ms/step - loss: 0.0306 - mae: 0.1351\n","Epoch 8/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0296 - mae: 0.1337\n","Epoch 9/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0283 - mae: 0.1292\n","Epoch 10/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0281 - mae: 0.1293\n","Epoch 11/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0279 - mae: 0.1293\n","Epoch 12/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0252 - mae: 0.1219\n","Epoch 13/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0264 - mae: 0.1249\n","Epoch 14/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0266 - mae: 0.1246\n","Epoch 15/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0241 - mae: 0.1192\n","Epoch 16/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0234 - mae: 0.1170\n","Epoch 17/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0237 - mae: 0.1182\n","Epoch 18/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0237 - mae: 0.1181\n","Epoch 19/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0218 - mae: 0.1125\n","Epoch 20/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0237 - mae: 0.1184\n","Epoch 21/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0236 - mae: 0.1187\n","Epoch 22/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0217 - mae: 0.1121\n","Epoch 23/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0229 - mae: 0.1160\n","Epoch 24/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0192 - mae: 0.1052\n","Epoch 25/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0205 - mae: 0.1095\n","Epoch 26/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0210 - mae: 0.1110\n","Epoch 27/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0191 - mae: 0.1058\n","Epoch 28/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0201 - mae: 0.1098\n","Epoch 29/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0189 - mae: 0.1056\n","Epoch 30/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0189 - mae: 0.1054\n","Epoch 31/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0173 - mae: 0.1011\n","Epoch 32/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0182 - mae: 0.1034\n","Epoch 33/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0162 - mae: 0.0971\n","Epoch 34/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0196 - mae: 0.1091\n","Epoch 35/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0156 - mae: 0.0963\n","Epoch 36/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0160 - mae: 0.0974\n","Epoch 37/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0153 - mae: 0.0957\n","Epoch 38/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0154 - mae: 0.0961\n","Epoch 39/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0143 - mae: 0.0923\n","Epoch 40/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0173 - mae: 0.1027\n","Epoch 41/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0137 - mae: 0.0901\n","Epoch 42/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0143 - mae: 0.0919\n","Epoch 43/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0141 - mae: 0.0921\n","Epoch 44/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0130 - mae: 0.0881\n","Epoch 45/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0126 - mae: 0.0863\n","Epoch 46/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0125 - mae: 0.0862\n","Epoch 47/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0115 - mae: 0.0833\n","Epoch 48/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0120 - mae: 0.0843\n","Epoch 49/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0113 - mae: 0.0814\n","Epoch 50/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0116 - mae: 0.0832\n","Kappa Score 0 : 0.7749926767489875\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_3 (GRU)                  (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_4 (GRU)                  (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_5 (GRU)                  (None, 64)                37248     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 21s 98ms/step - loss: 0.4372 - mae: 0.5867\n","Epoch 2/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.3915 - mae: 0.5800\n","Epoch 3/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.1383 - mae: 0.2880\n","Epoch 4/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0359 - mae: 0.1493\n","Epoch 5/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0348 - mae: 0.1448\n","Epoch 6/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0317 - mae: 0.1395\n","Epoch 7/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0312 - mae: 0.1369\n","Epoch 8/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0295 - mae: 0.1330\n","Epoch 9/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0288 - mae: 0.1313\n","Epoch 10/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0267 - mae: 0.1269\n","Epoch 11/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0232 - mae: 0.1179\n","Epoch 12/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0274 - mae: 0.1270\n","Epoch 13/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0281 - mae: 0.1306\n","Epoch 14/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0240 - mae: 0.1190\n","Epoch 15/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0248 - mae: 0.1209\n","Epoch 16/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0226 - mae: 0.1149\n","Epoch 17/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0239 - mae: 0.1193\n","Epoch 18/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0234 - mae: 0.1173\n","Epoch 19/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0245 - mae: 0.1197\n","Epoch 20/50\n","162/162 [==============================] - 16s 96ms/step - loss: 0.0226 - mae: 0.1151\n","Epoch 21/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0206 - mae: 0.1095\n","Epoch 22/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0217 - mae: 0.1126\n","Epoch 23/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0220 - mae: 0.1140\n","Epoch 24/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0220 - mae: 0.1140\n","Epoch 25/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0209 - mae: 0.1109\n","Epoch 26/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0189 - mae: 0.1061\n","Epoch 27/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0216 - mae: 0.1128\n","Epoch 28/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0192 - mae: 0.1069\n","Epoch 29/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0176 - mae: 0.1015\n","Epoch 30/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0193 - mae: 0.1072\n","Epoch 31/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0197 - mae: 0.1084\n","Epoch 32/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0172 - mae: 0.1015\n","Epoch 33/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0168 - mae: 0.1004\n","Epoch 34/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0170 - mae: 0.1007\n","Epoch 35/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0158 - mae: 0.0972\n","Epoch 36/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0152 - mae: 0.0964\n","Epoch 37/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0144 - mae: 0.0923\n","Epoch 38/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0161 - mae: 0.0971\n","Epoch 39/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0149 - mae: 0.0941\n","Epoch 40/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0153 - mae: 0.0962\n","Epoch 41/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0127 - mae: 0.0863\n","Epoch 42/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0128 - mae: 0.0876\n","Epoch 43/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0133 - mae: 0.0899\n","Epoch 44/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0124 - mae: 0.0866\n","Epoch 45/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0118 - mae: 0.0842\n","Epoch 46/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0119 - mae: 0.0839\n","Epoch 47/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0112 - mae: 0.0821\n","Epoch 48/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0111 - mae: 0.0811\n","Epoch 49/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0108 - mae: 0.0802\n","Epoch 50/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0104 - mae: 0.0785\n","Kappa Score 1 : 0.7106152363452943\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_6 (GRU)                  (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_7 (GRU)                  (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_8 (GRU)                  (None, 64)                37248     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 21s 99ms/step - loss: 0.3939 - mae: 0.5426\n","Epoch 2/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0576 - mae: 0.1882\n","Epoch 3/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0382 - mae: 0.1536\n","Epoch 4/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0344 - mae: 0.1463\n","Epoch 5/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0354 - mae: 0.1485\n","Epoch 6/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0347 - mae: 0.1456\n","Epoch 7/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0339 - mae: 0.1430\n","Epoch 8/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0299 - mae: 0.1349\n","Epoch 9/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0300 - mae: 0.1345\n","Epoch 10/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0275 - mae: 0.1283\n","Epoch 11/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0267 - mae: 0.1263\n","Epoch 12/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0242 - mae: 0.1199\n","Epoch 13/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0264 - mae: 0.1253\n","Epoch 14/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0252 - mae: 0.1228\n","Epoch 15/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0258 - mae: 0.1245\n","Epoch 16/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0245 - mae: 0.1206\n","Epoch 17/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0238 - mae: 0.1182\n","Epoch 18/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0192 - mae: 0.1072\n","Epoch 19/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0237 - mae: 0.1181\n","Epoch 20/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0211 - mae: 0.1118\n","Epoch 21/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0210 - mae: 0.1121\n","Epoch 22/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0206 - mae: 0.1103\n","Epoch 23/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0255 - mae: 0.1244\n","Epoch 24/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0198 - mae: 0.1087\n","Epoch 25/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0212 - mae: 0.1115\n","Epoch 26/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0195 - mae: 0.1071\n","Epoch 27/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0188 - mae: 0.1064\n","Epoch 28/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0202 - mae: 0.1095\n","Epoch 29/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0203 - mae: 0.1097\n","Epoch 30/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0170 - mae: 0.1003\n","Epoch 31/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0164 - mae: 0.0984\n","Epoch 32/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0177 - mae: 0.1017\n","Epoch 33/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0162 - mae: 0.0976\n","Epoch 34/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0184 - mae: 0.1047\n","Epoch 35/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0156 - mae: 0.0966\n","Epoch 36/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0173 - mae: 0.1028\n","Epoch 37/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0158 - mae: 0.0968\n","Epoch 38/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0158 - mae: 0.0965\n","Epoch 39/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0153 - mae: 0.0951\n","Epoch 40/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0143 - mae: 0.0926\n","Epoch 41/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0131 - mae: 0.0881\n","Epoch 42/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0151 - mae: 0.0951\n","Epoch 43/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0136 - mae: 0.0901\n","Epoch 44/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0126 - mae: 0.0870\n","Epoch 45/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0133 - mae: 0.0899\n","Epoch 46/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0117 - mae: 0.0831\n","Epoch 47/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0113 - mae: 0.0823\n","Epoch 48/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0110 - mae: 0.0812\n","Epoch 49/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0110 - mae: 0.0807\n","Epoch 50/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0111 - mae: 0.0809\n","Kappa Score 2 : 0.759233772628802\n","Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_9 (GRU)                  (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_10 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_11 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 21s 99ms/step - loss: 0.3738 - mae: 0.5472\n","Epoch 2/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0807 - mae: 0.2220\n","Epoch 3/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0408 - mae: 0.1594\n","Epoch 4/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0349 - mae: 0.1458\n","Epoch 5/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0328 - mae: 0.1396\n","Epoch 6/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0308 - mae: 0.1368\n","Epoch 7/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0301 - mae: 0.1344\n","Epoch 8/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0300 - mae: 0.1332\n","Epoch 9/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0258 - mae: 0.1237\n","Epoch 10/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0270 - mae: 0.1272\n","Epoch 11/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0288 - mae: 0.1303\n","Epoch 12/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0284 - mae: 0.1300\n","Epoch 13/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0242 - mae: 0.1201\n","Epoch 14/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0266 - mae: 0.1254\n","Epoch 15/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0242 - mae: 0.1181\n","Epoch 16/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0244 - mae: 0.1200\n","Epoch 17/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0244 - mae: 0.1199\n","Epoch 18/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0233 - mae: 0.1166\n","Epoch 19/50\n","162/162 [==============================] - 16s 97ms/step - loss: 0.0262 - mae: 0.1247\n","Epoch 20/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0215 - mae: 0.1116\n","Epoch 21/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0236 - mae: 0.1165\n","Epoch 22/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0208 - mae: 0.1103\n","Epoch 23/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0183 - mae: 0.1034\n","Epoch 24/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0233 - mae: 0.1175\n","Epoch 25/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0209 - mae: 0.1110\n","Epoch 26/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0189 - mae: 0.1048\n","Epoch 27/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0195 - mae: 0.1061\n","Epoch 28/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0181 - mae: 0.1021\n","Epoch 29/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0175 - mae: 0.1010\n","Epoch 30/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0180 - mae: 0.1024\n","Epoch 31/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0218 - mae: 0.1150\n","Epoch 32/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0185 - mae: 0.1046\n","Epoch 33/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0176 - mae: 0.1020\n","Epoch 34/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0155 - mae: 0.0963\n","Epoch 35/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0160 - mae: 0.0977\n","Epoch 36/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0156 - mae: 0.0961\n","Epoch 37/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0159 - mae: 0.0968\n","Epoch 38/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0146 - mae: 0.0938\n","Epoch 39/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0161 - mae: 0.0975\n","Epoch 40/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0141 - mae: 0.0916\n","Epoch 41/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0134 - mae: 0.0897\n","Epoch 42/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0134 - mae: 0.0897\n","Epoch 43/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0122 - mae: 0.0856\n","Epoch 44/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0120 - mae: 0.0847\n","Epoch 45/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0113 - mae: 0.0828\n","Epoch 46/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0111 - mae: 0.0819\n","Epoch 47/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0121 - mae: 0.0856\n","Epoch 48/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0102 - mae: 0.0789\n","Epoch 49/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0110 - mae: 0.0812\n","Epoch 50/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0106 - mae: 0.0795\n","Kappa Score 3 : 0.7752315869371749\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_12 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_13 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_14 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 21s 100ms/step - loss: 0.2076 - mae: 0.3670\n","Epoch 2/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0525 - mae: 0.1828\n","Epoch 3/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0414 - mae: 0.1598\n","Epoch 4/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0345 - mae: 0.1458\n","Epoch 5/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0330 - mae: 0.1419\n","Epoch 6/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0309 - mae: 0.1376\n","Epoch 7/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0281 - mae: 0.1294\n","Epoch 8/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0295 - mae: 0.1324\n","Epoch 9/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0287 - mae: 0.1310\n","Epoch 10/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0259 - mae: 0.1241\n","Epoch 11/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0277 - mae: 0.1273\n","Epoch 12/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0275 - mae: 0.1273\n","Epoch 13/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0260 - mae: 0.1238\n","Epoch 14/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0258 - mae: 0.1238\n","Epoch 15/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0245 - mae: 0.1215\n","Epoch 16/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0243 - mae: 0.1199\n","Epoch 17/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0254 - mae: 0.1219\n","Epoch 18/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0231 - mae: 0.1169\n","Epoch 19/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0214 - mae: 0.1120\n","Epoch 20/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0227 - mae: 0.1152\n","Epoch 21/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0212 - mae: 0.1106\n","Epoch 22/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0228 - mae: 0.1151\n","Epoch 23/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0212 - mae: 0.1129\n","Epoch 24/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0218 - mae: 0.1136\n","Epoch 25/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0198 - mae: 0.1074\n","Epoch 26/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0214 - mae: 0.1118\n","Epoch 27/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0200 - mae: 0.1086\n","Epoch 28/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0197 - mae: 0.1072\n","Epoch 29/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0205 - mae: 0.1080\n","Epoch 30/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0196 - mae: 0.1063\n","Epoch 31/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0201 - mae: 0.1091\n","Epoch 32/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0185 - mae: 0.1041\n","Epoch 33/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0174 - mae: 0.0998\n","Epoch 34/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0170 - mae: 0.0998\n","Epoch 35/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0165 - mae: 0.0975\n","Epoch 36/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0166 - mae: 0.0995\n","Epoch 37/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0166 - mae: 0.1000\n","Epoch 38/50\n","162/162 [==============================] - 16s 98ms/step - loss: 0.0168 - mae: 0.1003\n","Epoch 39/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0148 - mae: 0.0922\n","Epoch 40/50\n","162/162 [==============================] - 16s 99ms/step - loss: 0.0154 - mae: 0.0957\n","Epoch 41/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0145 - mae: 0.0933\n","Epoch 42/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0125 - mae: 0.0859\n","Epoch 43/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0146 - mae: 0.0937\n","Epoch 44/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0151 - mae: 0.0946\n","Epoch 45/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0140 - mae: 0.0913\n","Epoch 46/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0136 - mae: 0.0898\n","Epoch 47/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0137 - mae: 0.0896\n","Epoch 48/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0121 - mae: 0.0842\n","Epoch 49/50\n","162/162 [==============================] - 16s 101ms/step - loss: 0.0134 - mae: 0.0893\n","Epoch 50/50\n","162/162 [==============================] - 16s 100ms/step - loss: 0.0115 - mae: 0.0823\n","Kappa Score 4 : 0.7695190035286622\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XoY_0aTjCuKc","outputId":"d3a2dd91-806b-4eba-9fe4-117608c592ff"},"source":["from keras.callbacks import EarlyStopping\n","from sklearn.metrics import cohen_kappa_score\n","\n","batch_size = 64\n","\n","cv = KFold(n_splits=5, shuffle=True)\n","cnt = 0\n","for traincv, testcv in cv.split(embedded_essay):\n","  train_gen = DataGenerator(traincv, batch_size=batch_size)\n","  test_gen = DataGenerator(testcv, batch_size=batch_size, shuffle=False)\n","  train_y = y.iloc[traincv]\n","  test_y = y.iloc[testcv]\n","\n","  train_steps = len(traincv) // batch_size\n","  valid_steps = len(testcv) // batch_size\n","\n","  early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n","  sentence_model = get_sentence_model3()\n","  sentence_model.fit(train_gen, steps_per_epoch=train_steps, validation_steps=valid_steps,\n","                    epochs=50, callbacks=[early_stopping])\n","\n","  y_sent_pred = sentence_model.predict(test_gen) *100\n","  y_sent_pred = np.round(y_sent_pred)\n","\n","  sentence_result = cohen_kappa_score(np.array(np.round(test_y * 100)), y_sent_pred, weights='quadratic')\n","  print(\"Kappa Score\", cnt, \": {}\".format(sentence_result))\n","  cnt += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_26\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_52 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_53 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_54 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_26 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 11s 48ms/step - loss: 0.3405 - mae: 0.5058\n","Epoch 2/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0563 - mae: 0.1882\n","Epoch 3/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0437 - mae: 0.1652\n","Epoch 4/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0371 - mae: 0.1502\n","Epoch 5/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0357 - mae: 0.1487\n","Epoch 6/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0333 - mae: 0.1418\n","Epoch 7/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0317 - mae: 0.1379\n","Epoch 8/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0308 - mae: 0.1366\n","Epoch 9/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0293 - mae: 0.1326\n","Epoch 10/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0294 - mae: 0.1328\n","Epoch 11/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0268 - mae: 0.1273\n","Epoch 12/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0275 - mae: 0.1273\n","Epoch 13/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0265 - mae: 0.1272\n","Epoch 14/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0282 - mae: 0.1295\n","Epoch 15/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0278 - mae: 0.1291\n","Epoch 16/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0253 - mae: 0.1235\n","Epoch 17/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0260 - mae: 0.1245\n","Epoch 18/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0247 - mae: 0.1206\n","Epoch 19/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0229 - mae: 0.1160\n","Epoch 20/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0235 - mae: 0.1183\n","Epoch 21/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0216 - mae: 0.1135\n","Epoch 22/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0217 - mae: 0.1134\n","Epoch 23/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0207 - mae: 0.1108\n","Epoch 24/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0189 - mae: 0.1060\n","Epoch 25/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0201 - mae: 0.1092\n","Epoch 26/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0187 - mae: 0.1056\n","Epoch 27/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0195 - mae: 0.1089\n","Epoch 28/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0183 - mae: 0.1044\n","Epoch 29/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0174 - mae: 0.1019\n","Epoch 30/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0163 - mae: 0.0987\n","Epoch 31/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0173 - mae: 0.1017\n","Epoch 32/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0150 - mae: 0.0945\n","Epoch 33/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0138 - mae: 0.0908\n","Epoch 34/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0134 - mae: 0.0890\n","Epoch 35/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0133 - mae: 0.0900\n","Epoch 36/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0133 - mae: 0.0891\n","Epoch 37/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0121 - mae: 0.0850\n","Epoch 38/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0118 - mae: 0.0846\n","Epoch 39/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0109 - mae: 0.0801\n","Epoch 40/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0108 - mae: 0.0807\n","Epoch 41/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0120 - mae: 0.0848\n","Epoch 42/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0100 - mae: 0.0774\n","Epoch 43/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0094 - mae: 0.0756\n","Epoch 44/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0449 - mae: 0.1521\n","Epoch 45/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0247 - mae: 0.1215\n","Epoch 46/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0144 - mae: 0.0931\n","Epoch 47/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0128 - mae: 0.0889\n","Epoch 48/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0117 - mae: 0.0834\n","Kappa Score 0 : 0.7239701883533608\n","Model: \"sequential_27\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_55 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_56 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_57 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_27 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 12s 47ms/step - loss: 0.2946 - mae: 0.4206\n","Epoch 2/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0586 - mae: 0.1915\n","Epoch 3/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0417 - mae: 0.1608\n","Epoch 4/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0418 - mae: 0.1604\n","Epoch 5/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0369 - mae: 0.1505\n","Epoch 6/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0344 - mae: 0.1441\n","Epoch 7/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0311 - mae: 0.1373\n","Epoch 8/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0296 - mae: 0.1337\n","Epoch 9/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0306 - mae: 0.1364\n","Epoch 10/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0296 - mae: 0.1329\n","Epoch 11/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0282 - mae: 0.1299\n","Epoch 12/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0270 - mae: 0.1273\n","Epoch 13/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0287 - mae: 0.1304\n","Epoch 14/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0255 - mae: 0.1228\n","Epoch 15/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0249 - mae: 0.1208\n","Epoch 16/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0284 - mae: 0.1300\n","Epoch 17/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0263 - mae: 0.1250\n","Epoch 18/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0258 - mae: 0.1228\n","Epoch 19/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0258 - mae: 0.1233\n","Epoch 20/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0235 - mae: 0.1179\n","Epoch 21/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0217 - mae: 0.1128\n","Epoch 22/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0209 - mae: 0.1117\n","Epoch 23/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0221 - mae: 0.1147\n","Epoch 24/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0204 - mae: 0.1101\n","Epoch 25/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0206 - mae: 0.1115\n","Epoch 26/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0210 - mae: 0.1111\n","Epoch 27/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0183 - mae: 0.1030\n","Epoch 28/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0186 - mae: 0.1047\n","Epoch 29/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0182 - mae: 0.1034\n","Epoch 30/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0180 - mae: 0.1035\n","Epoch 31/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0167 - mae: 0.0995\n","Epoch 32/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0158 - mae: 0.0963\n","Epoch 33/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0150 - mae: 0.0942\n","Epoch 34/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0161 - mae: 0.0979\n","Epoch 35/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0158 - mae: 0.0967\n","Epoch 36/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0142 - mae: 0.0917\n","Epoch 37/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0126 - mae: 0.0866\n","Epoch 38/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0135 - mae: 0.0891\n","Epoch 39/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0112 - mae: 0.0810\n","Epoch 40/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0104 - mae: 0.0784\n","Epoch 41/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0118 - mae: 0.0837\n","Epoch 42/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0114 - mae: 0.0815\n","Epoch 43/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0106 - mae: 0.0786\n","Epoch 44/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0100 - mae: 0.0769\n","Epoch 45/50\n","162/162 [==============================] - 8s 46ms/step - loss: 0.0105 - mae: 0.0783\n","Epoch 46/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0097 - mae: 0.0757\n","Epoch 47/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0103 - mae: 0.0769\n","Epoch 48/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0092 - mae: 0.0726\n","Epoch 49/50\n","162/162 [==============================] - 9s 53ms/step - loss: 0.0097 - mae: 0.0746\n","Epoch 50/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0080 - mae: 0.0677\n","Kappa Score 1 : 0.7315380871926351\n","Model: \"sequential_28\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_58 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_59 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_60 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_28 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 11s 48ms/step - loss: 0.2249 - mae: 0.3822\n","Epoch 2/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0573 - mae: 0.1897\n","Epoch 3/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0417 - mae: 0.1613\n","Epoch 4/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0366 - mae: 0.1503\n","Epoch 5/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0327 - mae: 0.1406\n","Epoch 6/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0341 - mae: 0.1440\n","Epoch 7/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0311 - mae: 0.1371\n","Epoch 8/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0312 - mae: 0.1370\n","Epoch 9/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0308 - mae: 0.1358\n","Epoch 10/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0303 - mae: 0.1338\n","Epoch 11/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0280 - mae: 0.1290\n","Epoch 12/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0283 - mae: 0.1313\n","Epoch 13/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0244 - mae: 0.1206\n","Epoch 14/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0275 - mae: 0.1281\n","Epoch 15/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0263 - mae: 0.1243\n","Epoch 16/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0246 - mae: 0.1196\n","Epoch 17/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0245 - mae: 0.1195\n","Epoch 18/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0231 - mae: 0.1173\n","Epoch 19/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0236 - mae: 0.1172\n","Epoch 20/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0207 - mae: 0.1108\n","Epoch 21/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0218 - mae: 0.1137\n","Epoch 22/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0205 - mae: 0.1105\n","Epoch 23/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0197 - mae: 0.1078\n","Epoch 24/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0186 - mae: 0.1036\n","Epoch 25/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0189 - mae: 0.1050\n","Epoch 26/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0194 - mae: 0.1076\n","Epoch 27/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0172 - mae: 0.1012\n","Epoch 28/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0175 - mae: 0.1020\n","Epoch 29/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0164 - mae: 0.0982\n","Epoch 30/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0157 - mae: 0.0962\n","Epoch 31/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0153 - mae: 0.0951\n","Epoch 32/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0137 - mae: 0.0899\n","Epoch 33/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0130 - mae: 0.0872\n","Epoch 34/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0133 - mae: 0.0893\n","Epoch 35/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0154 - mae: 0.0966\n","Epoch 36/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0119 - mae: 0.0835\n","Epoch 37/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0118 - mae: 0.0832\n","Epoch 38/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0109 - mae: 0.0796\n","Epoch 39/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0106 - mae: 0.0780\n","Epoch 40/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0102 - mae: 0.0773\n","Epoch 41/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0111 - mae: 0.0805\n","Epoch 42/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0109 - mae: 0.0785\n","Epoch 43/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0098 - mae: 0.0750\n","Epoch 44/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0096 - mae: 0.0742\n","Epoch 45/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0088 - mae: 0.0718\n","Epoch 46/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0093 - mae: 0.0719\n","Epoch 47/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0082 - mae: 0.0679\n","Epoch 48/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0086 - mae: 0.0713\n","Epoch 49/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0078 - mae: 0.0682\n","Epoch 50/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0074 - mae: 0.0654\n","Kappa Score 2 : 0.7247522525148209\n","Model: \"sequential_29\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_61 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_62 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_63 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_29 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_29 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 11s 47ms/step - loss: 0.2000 - mae: 0.3514\n","Epoch 2/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0633 - mae: 0.2000\n","Epoch 3/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0423 - mae: 0.1617\n","Epoch 4/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0391 - mae: 0.1561\n","Epoch 5/50\n","162/162 [==============================] - 8s 46ms/step - loss: 0.0335 - mae: 0.1438\n","Epoch 6/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0356 - mae: 0.1485\n","Epoch 7/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0337 - mae: 0.1434\n","Epoch 8/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0314 - mae: 0.1374\n","Epoch 9/50\n","162/162 [==============================] - 8s 46ms/step - loss: 0.0299 - mae: 0.1339\n","Epoch 10/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0297 - mae: 0.1353\n","Epoch 11/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0293 - mae: 0.1328\n","Epoch 12/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0260 - mae: 0.1243\n","Epoch 13/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0293 - mae: 0.1322\n","Epoch 14/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0261 - mae: 0.1253\n","Epoch 15/50\n","162/162 [==============================] - 8s 46ms/step - loss: 0.0261 - mae: 0.1254\n","Epoch 16/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0244 - mae: 0.1209\n","Epoch 17/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0254 - mae: 0.1221\n","Epoch 18/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0224 - mae: 0.1148\n","Epoch 19/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0232 - mae: 0.1165\n","Epoch 20/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0238 - mae: 0.1175\n","Epoch 21/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0224 - mae: 0.1141\n","Epoch 22/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0229 - mae: 0.1174\n","Epoch 23/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0232 - mae: 0.1174\n","Epoch 24/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0200 - mae: 0.1078\n","Epoch 25/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0197 - mae: 0.1070\n","Epoch 26/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0200 - mae: 0.1075\n","Epoch 27/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0199 - mae: 0.1090\n","Epoch 28/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0174 - mae: 0.1009\n","Epoch 29/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0185 - mae: 0.1038\n","Epoch 30/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0167 - mae: 0.0985\n","Epoch 31/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0157 - mae: 0.0955\n","Epoch 32/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0150 - mae: 0.0932\n","Epoch 33/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0159 - mae: 0.0963\n","Epoch 34/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0167 - mae: 0.0995\n","Epoch 35/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0139 - mae: 0.0901\n","Epoch 36/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0141 - mae: 0.0911\n","Epoch 37/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0133 - mae: 0.0873\n","Epoch 38/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0135 - mae: 0.0888\n","Epoch 39/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0150 - mae: 0.0938\n","Epoch 40/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0123 - mae: 0.0844\n","Epoch 41/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0126 - mae: 0.0862\n","Epoch 42/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0115 - mae: 0.0820\n","Epoch 43/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0122 - mae: 0.0832\n","Epoch 44/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0107 - mae: 0.0792\n","Epoch 45/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0105 - mae: 0.0774\n","Epoch 46/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0109 - mae: 0.0796\n","Epoch 47/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0106 - mae: 0.0775\n","Epoch 48/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0108 - mae: 0.0790\n","Epoch 49/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0109 - mae: 0.0794\n","Epoch 50/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0101 - mae: 0.0769\n","Kappa Score 3 : 0.7399161212040734\n","Model: \"sequential_30\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_64 (GRU)                 (None, 128, 256)          787968    \n","_________________________________________________________________\n","gru_65 (GRU)                 (None, 128, 128)          148224    \n","_________________________________________________________________\n","gru_66 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_30 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_30 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 973,505\n","Trainable params: 973,505\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 12s 48ms/step - loss: 0.3349 - mae: 0.4971\n","Epoch 2/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0600 - mae: 0.1937\n","Epoch 3/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0458 - mae: 0.1690\n","Epoch 4/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0413 - mae: 0.1593\n","Epoch 5/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0357 - mae: 0.1483\n","Epoch 6/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0353 - mae: 0.1474\n","Epoch 7/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0336 - mae: 0.1424\n","Epoch 8/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0317 - mae: 0.1389\n","Epoch 9/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0329 - mae: 0.1414\n","Epoch 10/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0310 - mae: 0.1369\n","Epoch 11/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0305 - mae: 0.1350\n","Epoch 12/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0269 - mae: 0.1273\n","Epoch 13/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0274 - mae: 0.1287\n","Epoch 14/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0278 - mae: 0.1286\n","Epoch 15/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0261 - mae: 0.1243\n","Epoch 16/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0242 - mae: 0.1200\n","Epoch 17/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0261 - mae: 0.1243\n","Epoch 18/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0252 - mae: 0.1227\n","Epoch 19/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0228 - mae: 0.1158\n","Epoch 20/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0235 - mae: 0.1189\n","Epoch 21/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0242 - mae: 0.1203\n","Epoch 22/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0217 - mae: 0.1134\n","Epoch 23/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0218 - mae: 0.1144\n","Epoch 24/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0227 - mae: 0.1164\n","Epoch 25/50\n","162/162 [==============================] - 8s 49ms/step - loss: 0.0194 - mae: 0.1076\n","Epoch 26/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0207 - mae: 0.1113\n","Epoch 27/50\n","162/162 [==============================] - 8s 47ms/step - loss: 0.0181 - mae: 0.1039\n","Epoch 28/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0197 - mae: 0.1087\n","Epoch 29/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0183 - mae: 0.1045\n","Epoch 30/50\n","162/162 [==============================] - 8s 48ms/step - loss: 0.0174 - mae: 0.1019\n","Epoch 31/50\n"," 17/162 [==>...........................] - ETA: 6s - loss: 0.0173 - mae: 0.1004"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bj1M0mDG1wL0","executionInfo":{"status":"ok","timestamp":1619755215332,"user_tz":-540,"elapsed":1579366,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"951fa584-2a8a-4927-aa10-5eff4c573934"},"source":["from keras.callbacks import EarlyStopping\n","from sklearn.metrics import cohen_kappa_score\n","\n","batch_size = 64\n","\n","cv = KFold(n_splits=5, shuffle=True)\n","cnt = 0\n","for traincv, testcv in cv.split(embedded_essay):\n","  train_gen = DataGenerator(traincv, batch_size=batch_size)\n","  test_gen = DataGenerator(testcv, batch_size=batch_size, shuffle=False)\n","  train_y = y.iloc[traincv]\n","  test_y = y.iloc[testcv]\n","\n","  train_steps = len(traincv) // batch_size\n","  valid_steps = len(testcv) // batch_size\n","\n","  early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n","  sentence_model = get_sentence_model()\n","  sentence_model.fit(train_gen, steps_per_epoch=train_steps, validation_steps=valid_steps,\n","                    epochs=50, callbacks=[early_stopping])\n","\n","  y_sent_pred = sentence_model.predict(test_gen) *100\n","  y_sent_pred = np.round(y_sent_pred)\n","\n","  sentence_result = cohen_kappa_score(np.array(np.round(test_y * 100)), y_sent_pred, weights='quadratic')\n","  print(\"Kappa Score\", cnt, \": {}\".format(sentence_result))\n","  cnt += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_20 (GRU)                 (None, 96, 128)           344832    \n","_________________________________________________________________\n","gru_21 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 382,145\n","Trainable params: 382,145\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 9s 37ms/step - loss: 0.3627 - mae: 0.4369\n","Epoch 2/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0645 - mae: 0.2000\n","Epoch 3/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0453 - mae: 0.1681\n","Epoch 4/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0388 - mae: 0.1553\n","Epoch 5/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0327 - mae: 0.1404\n","Epoch 6/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0327 - mae: 0.1415\n","Epoch 7/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0302 - mae: 0.1359\n","Epoch 8/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0310 - mae: 0.1361\n","Epoch 9/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0307 - mae: 0.1349\n","Epoch 10/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0280 - mae: 0.1288\n","Epoch 11/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0274 - mae: 0.1284\n","Epoch 12/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0260 - mae: 0.1240\n","Epoch 13/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0260 - mae: 0.1241\n","Epoch 14/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0262 - mae: 0.1245\n","Epoch 15/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0261 - mae: 0.1242\n","Epoch 16/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0260 - mae: 0.1232\n","Epoch 17/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0247 - mae: 0.1203\n","Epoch 18/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0251 - mae: 0.1211\n","Epoch 19/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0246 - mae: 0.1199\n","Epoch 20/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0260 - mae: 0.1217\n","Epoch 21/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0243 - mae: 0.1190\n","Epoch 22/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0222 - mae: 0.1127\n","Epoch 23/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0227 - mae: 0.1158\n","Epoch 24/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0217 - mae: 0.1122\n","Epoch 25/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0217 - mae: 0.1121\n","Epoch 26/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0209 - mae: 0.1107\n","Epoch 27/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0230 - mae: 0.1156\n","Epoch 28/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0194 - mae: 0.1074\n","Epoch 29/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0213 - mae: 0.1110\n","Epoch 30/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0197 - mae: 0.1057\n","Epoch 31/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0192 - mae: 0.1046\n","Epoch 32/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0201 - mae: 0.1083\n","Epoch 33/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0176 - mae: 0.1009\n","Epoch 34/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0178 - mae: 0.1025\n","Epoch 35/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0174 - mae: 0.1000\n","Epoch 36/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0183 - mae: 0.1034\n","Epoch 37/50\n","162/162 [==============================] - 6s 36ms/step - loss: 0.0165 - mae: 0.0989\n","Epoch 38/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0160 - mae: 0.0970\n","Epoch 39/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0159 - mae: 0.0967\n","Epoch 40/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0155 - mae: 0.0957\n","Epoch 41/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0161 - mae: 0.0971\n","Epoch 42/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0158 - mae: 0.0960\n","Epoch 43/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0152 - mae: 0.0948\n","Epoch 44/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0164 - mae: 0.0978\n","Epoch 45/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0148 - mae: 0.0935\n","Epoch 46/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0163 - mae: 0.0974\n","Epoch 47/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0155 - mae: 0.0953\n","Epoch 48/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0143 - mae: 0.0915\n","Epoch 49/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0138 - mae: 0.0898\n","Epoch 50/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0146 - mae: 0.0932\n","Kappa Score 0 : 0.7534399540110519\n","Model: \"sequential_11\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_22 (GRU)                 (None, 96, 128)           344832    \n","_________________________________________________________________\n","gru_23 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_11 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 382,145\n","Trainable params: 382,145\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 9s 38ms/step - loss: 0.3622 - mae: 0.4646\n","Epoch 2/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0662 - mae: 0.2033\n","Epoch 3/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0446 - mae: 0.1666\n","Epoch 4/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0396 - mae: 0.1553\n","Epoch 5/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0333 - mae: 0.1423\n","Epoch 6/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0323 - mae: 0.1396\n","Epoch 7/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0305 - mae: 0.1366\n","Epoch 8/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0311 - mae: 0.1372\n","Epoch 9/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0304 - mae: 0.1349\n","Epoch 10/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0298 - mae: 0.1340\n","Epoch 11/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0301 - mae: 0.1340\n","Epoch 12/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0291 - mae: 0.1323\n","Epoch 13/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0279 - mae: 0.1282\n","Epoch 14/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0292 - mae: 0.1310\n","Epoch 15/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0273 - mae: 0.1267\n","Epoch 16/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0250 - mae: 0.1213\n","Epoch 17/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0265 - mae: 0.1255\n","Epoch 18/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0257 - mae: 0.1226\n","Epoch 19/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0261 - mae: 0.1238\n","Epoch 20/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0243 - mae: 0.1199\n","Epoch 21/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0253 - mae: 0.1210\n","Epoch 22/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0219 - mae: 0.1138\n","Epoch 23/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0253 - mae: 0.1233\n","Epoch 24/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0238 - mae: 0.1185\n","Epoch 25/50\n","162/162 [==============================] - 7s 41ms/step - loss: 0.0222 - mae: 0.1140\n","Epoch 26/50\n","162/162 [==============================] - 7s 41ms/step - loss: 0.0244 - mae: 0.1193\n","Epoch 27/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0225 - mae: 0.1141\n","Epoch 28/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0232 - mae: 0.1160\n","Epoch 29/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0213 - mae: 0.1118\n","Epoch 30/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0225 - mae: 0.1142\n","Epoch 31/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0222 - mae: 0.1141\n","Epoch 32/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0213 - mae: 0.1117\n","Epoch 33/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0189 - mae: 0.1048\n","Epoch 34/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0197 - mae: 0.1075\n","Epoch 35/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0195 - mae: 0.1065\n","Epoch 36/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0174 - mae: 0.1006\n","Epoch 37/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0177 - mae: 0.1016\n","Epoch 38/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0197 - mae: 0.1065\n","Epoch 39/50\n","162/162 [==============================] - 6s 37ms/step - loss: 0.0185 - mae: 0.1049\n","Epoch 40/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0177 - mae: 0.1015\n","Epoch 41/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0171 - mae: 0.1003\n","Epoch 42/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0183 - mae: 0.1025\n","Epoch 43/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0156 - mae: 0.0957\n","Epoch 44/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0158 - mae: 0.0959\n","Epoch 45/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0171 - mae: 0.0993\n","Epoch 46/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0159 - mae: 0.0966\n","Epoch 47/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0160 - mae: 0.0972\n","Epoch 48/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0155 - mae: 0.0947\n","Epoch 49/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0149 - mae: 0.0938\n","Epoch 50/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0143 - mae: 0.0919\n","Kappa Score 1 : 0.738495116427988\n","Model: \"sequential_12\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_24 (GRU)                 (None, 96, 128)           344832    \n","_________________________________________________________________\n","gru_25 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 382,145\n","Trainable params: 382,145\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 9s 39ms/step - loss: 0.4874 - mae: 0.4304\n","Epoch 2/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0682 - mae: 0.2063\n","Epoch 3/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0434 - mae: 0.1649\n","Epoch 4/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0375 - mae: 0.1519\n","Epoch 5/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0366 - mae: 0.1506\n","Epoch 6/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0352 - mae: 0.1465\n","Epoch 7/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0314 - mae: 0.1378\n","Epoch 8/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0300 - mae: 0.1338\n","Epoch 9/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0289 - mae: 0.1322\n","Epoch 10/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0295 - mae: 0.1322\n","Epoch 11/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0261 - mae: 0.1244\n","Epoch 12/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0289 - mae: 0.1311\n","Epoch 13/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0270 - mae: 0.1278\n","Epoch 14/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0255 - mae: 0.1233\n","Epoch 15/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0250 - mae: 0.1220\n","Epoch 16/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0247 - mae: 0.1204\n","Epoch 17/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0259 - mae: 0.1230\n","Epoch 18/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0250 - mae: 0.1201\n","Epoch 19/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0250 - mae: 0.1207\n","Epoch 20/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0243 - mae: 0.1200\n","Epoch 21/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0245 - mae: 0.1207\n","Epoch 22/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0227 - mae: 0.1162\n","Epoch 23/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0221 - mae: 0.1138\n","Epoch 24/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0207 - mae: 0.1101\n","Epoch 25/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0226 - mae: 0.1154\n","Epoch 26/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0201 - mae: 0.1082\n","Epoch 27/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0214 - mae: 0.1118\n","Epoch 28/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0210 - mae: 0.1100\n","Epoch 29/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0188 - mae: 0.1052\n","Epoch 30/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0203 - mae: 0.1085\n","Epoch 31/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0192 - mae: 0.1063\n","Epoch 32/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0182 - mae: 0.1035\n","Epoch 33/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0186 - mae: 0.1039\n","Epoch 34/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0184 - mae: 0.1032\n","Epoch 35/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0190 - mae: 0.1053\n","Epoch 36/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0181 - mae: 0.1028\n","Epoch 37/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0194 - mae: 0.1065\n","Epoch 38/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0184 - mae: 0.1040\n","Epoch 39/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0175 - mae: 0.1017\n","Epoch 40/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0153 - mae: 0.0952\n","Epoch 41/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0160 - mae: 0.0969\n","Epoch 42/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0159 - mae: 0.0960\n","Epoch 43/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0158 - mae: 0.0968\n","Epoch 44/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0164 - mae: 0.0984\n","Epoch 45/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0147 - mae: 0.0933\n","Epoch 46/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0153 - mae: 0.0949\n","Epoch 47/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0146 - mae: 0.0936\n","Epoch 48/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0137 - mae: 0.0900\n","Epoch 49/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0134 - mae: 0.0884\n","Epoch 50/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0147 - mae: 0.0931\n","Kappa Score 2 : 0.7279396137144782\n","Model: \"sequential_13\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_26 (GRU)                 (None, 96, 128)           344832    \n","_________________________________________________________________\n","gru_27 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 382,145\n","Trainable params: 382,145\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 9s 39ms/step - loss: 0.3600 - mae: 0.4287\n","Epoch 2/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0709 - mae: 0.2116\n","Epoch 3/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0463 - mae: 0.1700\n","Epoch 4/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0377 - mae: 0.1529\n","Epoch 5/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0338 - mae: 0.1444\n","Epoch 6/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0329 - mae: 0.1416\n","Epoch 7/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0308 - mae: 0.1371\n","Epoch 8/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0322 - mae: 0.1407\n","Epoch 9/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0307 - mae: 0.1354\n","Epoch 10/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0281 - mae: 0.1296\n","Epoch 11/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0284 - mae: 0.1310\n","Epoch 12/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0265 - mae: 0.1252\n","Epoch 13/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0269 - mae: 0.1263\n","Epoch 14/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0284 - mae: 0.1285\n","Epoch 15/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0272 - mae: 0.1279\n","Epoch 16/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0251 - mae: 0.1217\n","Epoch 17/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0259 - mae: 0.1241\n","Epoch 18/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0256 - mae: 0.1222\n","Epoch 19/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0230 - mae: 0.1163\n","Epoch 20/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0237 - mae: 0.1181\n","Epoch 21/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0210 - mae: 0.1110\n","Epoch 22/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0222 - mae: 0.1138\n","Epoch 23/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0230 - mae: 0.1154\n","Epoch 24/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0223 - mae: 0.1149\n","Epoch 25/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0208 - mae: 0.1112\n","Epoch 26/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0215 - mae: 0.1117\n","Epoch 27/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0198 - mae: 0.1077\n","Epoch 28/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0216 - mae: 0.1124\n","Epoch 29/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0194 - mae: 0.1063\n","Epoch 30/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0183 - mae: 0.1029\n","Epoch 31/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0208 - mae: 0.1100\n","Epoch 32/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0195 - mae: 0.1072\n","Epoch 33/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0173 - mae: 0.1016\n","Epoch 34/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0184 - mae: 0.1044\n","Epoch 35/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0193 - mae: 0.1054\n","Epoch 36/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0180 - mae: 0.1031\n","Epoch 37/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0181 - mae: 0.1031\n","Epoch 38/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0170 - mae: 0.0996\n","Epoch 39/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0167 - mae: 0.0987\n","Epoch 40/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0164 - mae: 0.0978\n","Epoch 41/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0160 - mae: 0.0970\n","Epoch 42/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0157 - mae: 0.0965\n","Epoch 43/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0153 - mae: 0.0954\n","Epoch 44/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0130 - mae: 0.0865\n","Epoch 45/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0150 - mae: 0.0945\n","Epoch 46/50\n","162/162 [==============================] - 6s 40ms/step - loss: 0.0145 - mae: 0.0918\n","Epoch 47/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0144 - mae: 0.0921\n","Epoch 48/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0147 - mae: 0.0936\n","Epoch 49/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0132 - mae: 0.0884\n","Epoch 50/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0133 - mae: 0.0886\n","Kappa Score 3 : 0.7299611323502261\n","Model: \"sequential_14\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_28 (GRU)                 (None, 96, 128)           344832    \n","_________________________________________________________________\n","gru_29 (GRU)                 (None, 64)                37248     \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, 64)                0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 1)                 65        \n","=================================================================\n","Total params: 382,145\n","Trainable params: 382,145\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n","162/162 [==============================] - 9s 39ms/step - loss: 0.3975 - mae: 0.4217\n","Epoch 2/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0664 - mae: 0.2029\n","Epoch 3/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0444 - mae: 0.1661\n","Epoch 4/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0362 - mae: 0.1482\n","Epoch 5/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0331 - mae: 0.1418\n","Epoch 6/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0346 - mae: 0.1458\n","Epoch 7/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0305 - mae: 0.1356\n","Epoch 8/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0313 - mae: 0.1373\n","Epoch 9/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0296 - mae: 0.1342\n","Epoch 10/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0280 - mae: 0.1298\n","Epoch 11/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0257 - mae: 0.1238\n","Epoch 12/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0285 - mae: 0.1304\n","Epoch 13/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0252 - mae: 0.1223\n","Epoch 14/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0279 - mae: 0.1283\n","Epoch 15/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0253 - mae: 0.1233\n","Epoch 16/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0240 - mae: 0.1192\n","Epoch 17/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0274 - mae: 0.1263\n","Epoch 18/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0239 - mae: 0.1188\n","Epoch 19/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0244 - mae: 0.1203\n","Epoch 20/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0249 - mae: 0.1203\n","Epoch 21/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0253 - mae: 0.1220\n","Epoch 22/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0243 - mae: 0.1196\n","Epoch 23/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0247 - mae: 0.1195\n","Epoch 24/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0213 - mae: 0.1114\n","Epoch 25/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0232 - mae: 0.1168\n","Epoch 26/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0209 - mae: 0.1108\n","Epoch 27/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0206 - mae: 0.1112\n","Epoch 28/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0214 - mae: 0.1118\n","Epoch 29/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0199 - mae: 0.1077\n","Epoch 30/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0202 - mae: 0.1081\n","Epoch 31/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0214 - mae: 0.1113\n","Epoch 32/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0203 - mae: 0.1092\n","Epoch 33/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0188 - mae: 0.1050\n","Epoch 34/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0193 - mae: 0.1067\n","Epoch 35/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0195 - mae: 0.1077\n","Epoch 36/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0189 - mae: 0.1068\n","Epoch 37/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0181 - mae: 0.1028\n","Epoch 38/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0191 - mae: 0.1058\n","Epoch 39/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0180 - mae: 0.1031\n","Epoch 40/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0179 - mae: 0.1023\n","Epoch 41/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0176 - mae: 0.1008\n","Epoch 42/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0159 - mae: 0.0970\n","Epoch 43/50\n","162/162 [==============================] - 6s 40ms/step - loss: 0.0177 - mae: 0.1027\n","Epoch 44/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0172 - mae: 0.1010\n","Epoch 45/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0164 - mae: 0.0984\n","Epoch 46/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0159 - mae: 0.0962\n","Epoch 47/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0167 - mae: 0.0988\n","Epoch 48/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0155 - mae: 0.0963\n","Epoch 49/50\n","162/162 [==============================] - 6s 39ms/step - loss: 0.0155 - mae: 0.0950\n","Epoch 50/50\n","162/162 [==============================] - 6s 38ms/step - loss: 0.0146 - mae: 0.0928\n","Kappa Score 4 : 0.71835948528386\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_Wd44yEhpyg","executionInfo":{"status":"ok","timestamp":1619752532081,"user_tz":-540,"elapsed":574,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"d5042c6d-473b-4e04-dc6f-8ccf461278b1"},"source":["test_y = y.iloc[testcv]\n","sentence_result = cohen_kappa_score(np.array(np.round(test_y * 100)), y_sent_pred, weights='quadratic')\n","print(\"Kappa Score 0\", \": {}\".format(sentence_result))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Kappa Score 0 : 0.7346137426085717\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZLTQOzbWxp5q","executionInfo":{"status":"ok","timestamp":1619752688402,"user_tz":-540,"elapsed":659,"user":{"displayName":"강재윤","photoUrl":"","userId":"16097446264405090619"}},"outputId":"cd10635b-4c1f-4d0f-9bec-1de0ced15138"},"source":["print(np.array(np.round(test_y * 100))[0:100])\n","print(y_sent_pred[0:100])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 70.  60.  60.  70.  70.  70.  60.  80.  70.  20.  70.  60.  60.  80.\n","  60.  60.  70.  60.  60. 100.  60.  50.  50.  50.  70.  60.  60. 100.\n","  80.  60.  70.  80.  70.  80.  60.  60.  80.  60.  30.  70.  80.  60.\n","  80.  80.  60.  40.  40.  60.  70.  70.  70.  60.  60.  70.  50.  90.\n","  60.  60.  80.  80.  80.  70.  70.  80.  60.  60.  60.  60.  70.  90.\n","  60.  50.  40.  80.  60.  40.  60.  40.  80.  60.  60.  80.  70.  70.\n","  60.  60.  60. 100.  80.  60.  70.  60.  50. 100.  70.  60.  60.  70.\n","  60.  40.]\n","[[64.]\n"," [69.]\n"," [51.]\n"," [68.]\n"," [68.]\n"," [56.]\n"," [56.]\n"," [87.]\n"," [77.]\n"," [31.]\n"," [69.]\n"," [72.]\n"," [78.]\n"," [64.]\n"," [60.]\n"," [66.]\n"," [73.]\n"," [55.]\n"," [59.]\n"," [83.]\n"," [59.]\n"," [55.]\n"," [67.]\n"," [52.]\n"," [72.]\n"," [49.]\n"," [51.]\n"," [70.]\n"," [66.]\n"," [47.]\n"," [76.]\n"," [89.]\n"," [83.]\n"," [75.]\n"," [73.]\n"," [63.]\n"," [85.]\n"," [58.]\n"," [10.]\n"," [68.]\n"," [83.]\n"," [62.]\n"," [63.]\n"," [69.]\n"," [62.]\n"," [53.]\n"," [47.]\n"," [70.]\n"," [66.]\n"," [79.]\n"," [66.]\n"," [60.]\n"," [56.]\n"," [58.]\n"," [57.]\n"," [72.]\n"," [66.]\n"," [58.]\n"," [83.]\n"," [64.]\n"," [84.]\n"," [78.]\n"," [71.]\n"," [59.]\n"," [52.]\n"," [67.]\n"," [54.]\n"," [66.]\n"," [65.]\n"," [74.]\n"," [77.]\n"," [35.]\n"," [52.]\n"," [70.]\n"," [73.]\n"," [54.]\n"," [54.]\n"," [60.]\n"," [59.]\n"," [68.]\n"," [77.]\n"," [61.]\n"," [68.]\n"," [62.]\n"," [61.]\n"," [73.]\n"," [45.]\n"," [73.]\n"," [74.]\n"," [68.]\n"," [67.]\n"," [64.]\n"," [62.]\n"," [81.]\n"," [80.]\n"," [67.]\n"," [64.]\n"," [70.]\n"," [42.]\n"," [54.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NBKfgw4JPEht"},"source":["## Training Phase - Sentence"]},{"cell_type":"markdown","metadata":{"id":"VJvP2q1EPEhu"},"source":["문장 단위, 모델 돌리는 부분만 (전처리는 위에서)"]}]}