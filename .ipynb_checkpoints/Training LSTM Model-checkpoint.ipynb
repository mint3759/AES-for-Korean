{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n",
    "maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  score  \n",
       "0              8    0.6  \n",
       "1              9    0.7  \n",
       "2              7    0.5  \n",
       "3             10    0.8  \n",
       "4              8    0.6  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_min = minimum_scores[X['essay_set']]\n",
    "old_max = maximum_scores[X['essay_set']]\n",
    "old_range = old_max - old_min\n",
    "new_min = 0\n",
    "new_max = 1\n",
    "new_range = (new_max - new_min)  \n",
    "X['score'] = (((X['domain1_score'] - old_min) * new_range) / old_range) + new_min\n",
    "\n",
    "# round score to nearest integer for cohen kappa calculation\n",
    "y = np.round(X['score'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12976, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set = []\n",
    "for cnt in range(8):\n",
    "    is_cnt = X['essay_set'] == (cnt+1)\n",
    "    set_cnt = X[is_cnt]\n",
    "    X_set.append(set_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1783, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_set[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tmax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tmax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])       \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def makeFeatureVec2(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    if len(words) != 0:\n",
    "        featureVec = np.divide(featureVec,float(len(words)))\n",
    "    return featureVec\n",
    "\n",
    "def makeFeatureVec3(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = []\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            featureVec.append(np.array(model[word], dtype=\"float32\"))\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for glove model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs\n",
    "\n",
    "def getAvgFeatureVecs2(essay, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for glove model.\"\"\"\n",
    "    essayFeatureVecs = np.zeros((len(essay),num_features),dtype=\"float32\")\n",
    "    for cnt, sentence in enumerate(essay):\n",
    "        essayFeatureVecs[cnt] = makeFeatureVec2(sentence, model, num_features)\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove embeddings\n",
    "embedding_dict={}\n",
    "with open(os.path.join(DATASET_DIR, 'glove/glove.6B.200d.txt'),'r', encoding='UTF8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word] = vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fold Started\n",
      "10380\n",
      "2596\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "num_features = 200\n",
    "\n",
    "trainData_sent = []\n",
    "testData_sent = []\n",
    "y_trainData_sent = []\n",
    "y_testData_sent = []\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print('##Fold Started')\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "\n",
    "    y_trainData_sent.append(y_train)\n",
    "    y_testData_sent.append(y_test)\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "\n",
    "    trainDataVecs = []\n",
    "    testDataVecs = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "        trainDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "        trainDataVecs.append(np.array(trainDataVec, dtype=\"float32\"))\n",
    "\n",
    "    for essay in test_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "        testDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "        testDataVecs.append(np.array(testDataVec, dtype=\"float32\"))\n",
    "        \n",
    "    trainDataVecs = pad_sequences(trainDataVecs, maxlen=128, padding='pre', dtype='float')\n",
    "    testDataVecs = pad_sequences(testDataVecs, maxlen=128, padding='pre', dtype='float')\n",
    "    trainData_sent.append(np.array(trainDataVecs, dtype=\"float32\"))\n",
    "    testData_sent.append(np.array(testDataVecs, dtype=\"float32\"))\n",
    "    print(len(trainDataVecs))\n",
    "    print(len(testDataVecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 200], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_sentence_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, dropout=0.4, recurrent_dropout=0.4, input_shape=[128, 200], return_sequences=True))\n",
    "    model.add(GRU(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_word_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(GRU(128, dropout=0.4, recurrent_dropout=0.4, input_shape=[512, 200], return_sequences=True))\n",
    "    model.add(GRU(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase - Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU가 생기면 돌릴 코드...data 단어 단위로 쪼개기 + 모델 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 0--------\n",
      "\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.09 GiB for an array with shape (1426, 512, 200) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-229-db7c798b7216>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mtestDataVecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataVec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtrainDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainDataVecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtestDataVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataVecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AES\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    154\u001b[0m           \u001b[1;32mor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m   \"\"\"\n\u001b[1;32m--> 156\u001b[1;33m   return sequence.pad_sequences(\n\u001b[0m\u001b[0;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AES\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     83\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AES\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.09 GiB for an array with shape (1426, 512, 200) and data type float64"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv3 = KFold(n_splits=5, shuffle=True)\n",
    "num_features = 200\n",
    "\n",
    "word_model_cnt = 0\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "word_models = []\n",
    "word_results = []\n",
    "for traincv, testcv in cv3.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(word_model_cnt))\n",
    "    \n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "\n",
    "    trainDataVecs = []\n",
    "    testDataVecs = []\n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        words = essay_to_wordlist(essay, remove_stopwords=True)\n",
    "        trainDataVec = makeFeatureVec3(words, embedding_dict, num_features)\n",
    "        trainDataVecs.append(np.array(trainDataVec, dtype=\"float32\"))\n",
    "\n",
    "    for essay in test_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        words = essay_to_wordlist(essay, remove_stopwords=True)\n",
    "        testDataVec = makeFeatureVec3(words, embedding_dict, num_features)\n",
    "        testDataVecs.append(np.array(testDataVec, dtype=\"float32\"))\n",
    "        \n",
    "    trainDataVecs = pad_sequences(trainDataVecs, maxlen=512, padding='pre', dtype='float')\n",
    "    testDataVecs = pad_sequences(testDataVecs, maxlen=512, padding='pre', dtype='float')\n",
    "    \n",
    "    word_model = get_word_model()\n",
    "    word_model.fit(trainDataVecs, testDataVecs, batch_size=64, epochs=50, callbacks=[early_stopping])\n",
    "    \n",
    "    y_pred_word = word_model.predict(testDataVecs)\n",
    "\n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred_word = np.round(y_pred_word)\n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values, y_pred_word, weights='quadratic')\n",
    "    print(\"Kappa Score\", cnt, \": {}\".format(result))\n",
    "    word_models.append(word_model)\n",
    "    word_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase - Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 단위, 모델 돌리는 부분만 (전처리는 위에서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.6\n",
       "1    0.7\n",
       "3    0.8\n",
       "4    0.6\n",
       "6    0.8\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trainData_sent[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 0--------\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_2 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 36s 202ms/step - loss: 0.0805 - mae: 0.2222\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 39s 237ms/step - loss: 0.0429 - mae: 0.1642\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 38s 232ms/step - loss: 0.0381 - mae: 0.1536\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 39s 236ms/step - loss: 0.0347 - mae: 0.1453\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 38s 233ms/step - loss: 0.0349 - mae: 0.1461\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 39s 239ms/step - loss: 0.0320 - mae: 0.1385\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 39s 240ms/step - loss: 0.0311 - mae: 0.1377\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 41s 249ms/step - loss: 0.0305 - mae: 0.1353\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 40s 247ms/step - loss: 0.0311 - mae: 0.1365\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 41s 250ms/step - loss: 0.0297 - mae: 0.1336\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 41s 249ms/step - loss: 0.0288 - mae: 0.1320\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 41s 254ms/step - loss: 0.0279 - mae: 0.1292\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 43s 261ms/step - loss: 0.0268 - mae: 0.1272\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 43s 265ms/step - loss: 0.0269 - mae: 0.1269\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 44s 271ms/step - loss: 0.0264 - mae: 0.1245\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 43s 263ms/step - loss: 0.0253 - mae: 0.1234\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 42s 260ms/step - loss: 0.0258 - mae: 0.1235\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 44s 269ms/step - loss: 0.0254 - mae: 0.1225\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 43s 263ms/step - loss: 0.0241 - mae: 0.1198\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 42s 260ms/step - loss: 0.0231 - mae: 0.1176\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 42s 257ms/step - loss: 0.0231 - mae: 0.1173\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 42s 257ms/step - loss: 0.0227 - mae: 0.1156\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 42s 256ms/step - loss: 0.0231 - mae: 0.1167\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 42s 256ms/step - loss: 0.0220 - mae: 0.1131\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 41s 254ms/step - loss: 0.0223 - mae: 0.1144\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 41s 254ms/step - loss: 0.0221 - mae: 0.1136\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 41s 254ms/step - loss: 0.0214 - mae: 0.1123\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 41s 250ms/step - loss: 0.0208 - mae: 0.1109\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 42s 256ms/step - loss: 0.0216 - mae: 0.1121\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 42s 256ms/step - loss: 0.0202 - mae: 0.1097\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 42s 256ms/step - loss: 0.0212 - mae: 0.1116\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 42s 260ms/step - loss: 0.0202 - mae: 0.1086\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0198 - mae: 0.1086\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 43s 265ms/step - loss: 0.0203 - mae: 0.1085\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 44s 267ms/step - loss: 0.0193 - mae: 0.1067\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 43s 267ms/step - loss: 0.0196 - mae: 0.1073\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 43s 264ms/step - loss: 0.0195 - mae: 0.1083\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 43s 266ms/step - loss: 0.0188 - mae: 0.1056\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 43s 262ms/step - loss: 0.0186 - mae: 0.1046\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 42s 258ms/step - loss: 0.0185 - mae: 0.1043\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 43s 262ms/step - loss: 0.0185 - mae: 0.1045\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 43s 261ms/step - loss: 0.0184 - mae: 0.1046\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 43s 263ms/step - loss: 0.0182 - mae: 0.1046\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 43s 261ms/step - loss: 0.0176 - mae: 0.1022\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 43s 265ms/step - loss: 0.0182 - mae: 0.1044\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 43s 265ms/step - loss: 0.0175 - mae: 0.1018\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 43s 262ms/step - loss: 0.0174 - mae: 0.1008\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 44s 267ms/step - loss: 0.0174 - mae: 0.1006\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 44s 268ms/step - loss: 0.0165 - mae: 0.0986\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 44s 268ms/step - loss: 0.0161 - mae: 0.0974\n",
      "Kappa Score 0 : 0.7693377600188759\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_4 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 45s 259ms/step - loss: 0.0777 - mae: 0.2179\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 44s 271ms/step - loss: 0.0454 - mae: 0.1672\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 45s 276ms/step - loss: 0.0387 - mae: 0.1544\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 45s 277ms/step - loss: 0.0375 - mae: 0.1517\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 46s 280ms/step - loss: 0.0331 - mae: 0.1408\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 45s 278ms/step - loss: 0.0328 - mae: 0.1412\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 45s 277ms/step - loss: 0.0321 - mae: 0.1386\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 45s 273ms/step - loss: 0.0310 - mae: 0.1363\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 45s 275ms/step - loss: 0.0301 - mae: 0.1341\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 45s 277ms/step - loss: 0.0292 - mae: 0.1320\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 45s 277ms/step - loss: 0.0280 - mae: 0.1294\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 45s 279ms/step - loss: 0.0281 - mae: 0.1291\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 46s 279ms/step - loss: 0.0266 - mae: 0.1250\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 46s 280ms/step - loss: 0.0275 - mae: 0.1267\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 45s 279ms/step - loss: 0.0257 - mae: 0.1232\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 45s 278ms/step - loss: 0.0256 - mae: 0.1228\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 45s 277ms/step - loss: 0.0250 - mae: 0.1215\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0246 - mae: 0.1205\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0225 - mae: 0.1157\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 46s 285ms/step - loss: 0.0239 - mae: 0.1187\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0237 - mae: 0.1180\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0228 - mae: 0.1156\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 46s 281ms/step - loss: 0.0223 - mae: 0.1148\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 46s 285ms/step - loss: 0.0224 - mae: 0.1141\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0217 - mae: 0.1134\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0219 - mae: 0.1133\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0217 - mae: 0.1115\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0209 - mae: 0.1104\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0220 - mae: 0.1136\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0202 - mae: 0.1098\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0202 - mae: 0.1096\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0200 - mae: 0.1083\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 46s 281ms/step - loss: 0.0206 - mae: 0.1094\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0190 - mae: 0.1059\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 46s 281ms/step - loss: 0.0194 - mae: 0.1075\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0191 - mae: 0.1064\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0192 - mae: 0.1061\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0186 - mae: 0.1041\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0195 - mae: 0.1070\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0186 - mae: 0.1036\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0179 - mae: 0.1035\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0175 - mae: 0.1024\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0178 - mae: 0.1022\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 46s 283ms/step - loss: 0.0181 - mae: 0.1028\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0176 - mae: 0.1026\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 46s 281ms/step - loss: 0.0177 - mae: 0.1020\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 46s 281ms/step - loss: 0.0175 - mae: 0.1011\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 46s 284ms/step - loss: 0.0162 - mae: 0.0970\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 46s 285ms/step - loss: 0.0168 - mae: 0.0994\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 46s 282ms/step - loss: 0.0159 - mae: 0.0978\n",
      "Kappa Score 1 : 0.7541579693260282\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 51s 295ms/step - loss: 0.0849 - mae: 0.2268\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 51s 311ms/step - loss: 0.0425 - mae: 0.1614\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0369 - mae: 0.1500\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0349 - mae: 0.1452\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0328 - mae: 0.1402\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0318 - mae: 0.1387\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 51s 316ms/step - loss: 0.0309 - mae: 0.1369\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 51s 312ms/step - loss: 0.0305 - mae: 0.1340\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0291 - mae: 0.1319\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 51s 312ms/step - loss: 0.0293 - mae: 0.1312\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0286 - mae: 0.1305\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0288 - mae: 0.1291\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 51s 312ms/step - loss: 0.0274 - mae: 0.1279\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0273 - mae: 0.1270\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0256 - mae: 0.1233\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0257 - mae: 0.1230\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0246 - mae: 0.1203\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 51s 312ms/step - loss: 0.0241 - mae: 0.1190\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0241 - mae: 0.1183\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0235 - mae: 0.1172\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0233 - mae: 0.1165\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 51s 314ms/step - loss: 0.0228 - mae: 0.1160\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 51s 313ms/step - loss: 0.0228 - mae: 0.1153\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 51s 314ms/step - loss: 0.0230 - mae: 0.1159\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0229 - mae: 0.1159\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 52s 318ms/step - loss: 0.0219 - mae: 0.1129\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 52s 316ms/step - loss: 0.0214 - mae: 0.1119\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 51s 316ms/step - loss: 0.0217 - mae: 0.1127\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 52s 316ms/step - loss: 0.0205 - mae: 0.1097\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 51s 316ms/step - loss: 0.0204 - mae: 0.1099\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0211 - mae: 0.1109\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 51s 316ms/step - loss: 0.0211 - mae: 0.1111\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0207 - mae: 0.1090\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 51s 314ms/step - loss: 0.0202 - mae: 0.1087\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0193 - mae: 0.1070\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0196 - mae: 0.1072\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 52s 316ms/step - loss: 0.0193 - mae: 0.1061\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0186 - mae: 0.1037\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 52s 318ms/step - loss: 0.0183 - mae: 0.1036\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 52s 319ms/step - loss: 0.0188 - mae: 0.1054\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 52s 318ms/step - loss: 0.0181 - mae: 0.1031\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0182 - mae: 0.1037\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0183 - mae: 0.1032\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 52s 318ms/step - loss: 0.0179 - mae: 0.1020\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0179 - mae: 0.1026\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 52s 318ms/step - loss: 0.0177 - mae: 0.1017\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 51s 315ms/step - loss: 0.0170 - mae: 0.1004\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0170 - mae: 0.1003\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 52s 316ms/step - loss: 0.0173 - mae: 0.1002\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 52s 317ms/step - loss: 0.0164 - mae: 0.0986\n",
      "Kappa Score 2 : 0.7299364600903404\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 59s 346ms/step - loss: 0.0919 - mae: 0.2341\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 0.0446 - mae: 0.1661\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0409 - mae: 0.1580\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 0.0364 - mae: 0.1492\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0339 - mae: 0.1439\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 59s 362ms/step - loss: 0.0332 - mae: 0.1406\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0318 - mae: 0.1389\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 0.0320 - mae: 0.1384\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.0301 - mae: 0.1342\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.0307 - mae: 0.1358\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.0281 - mae: 0.1296\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0290 - mae: 0.1306\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0287 - mae: 0.1308\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.0274 - mae: 0.1280\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 0.0266 - mae: 0.1252\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 0.0270 - mae: 0.1266\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0257 - mae: 0.1235\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0254 - mae: 0.1222\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0243 - mae: 0.1200\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0240 - mae: 0.1194\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0247 - mae: 0.1197\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0230 - mae: 0.1165\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0234 - mae: 0.1169\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0225 - mae: 0.1144\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 60s 368ms/step - loss: 0.0234 - mae: 0.1174\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 60s 368ms/step - loss: 0.0227 - mae: 0.1152\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 0.0217 - mae: 0.1132\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 60s 368ms/step - loss: 0.0213 - mae: 0.1121\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0216 - mae: 0.1121\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0210 - mae: 0.1109\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0205 - mae: 0.1101\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 0.0203 - mae: 0.1096\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0206 - mae: 0.1096\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0199 - mae: 0.1080\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0202 - mae: 0.1087\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0193 - mae: 0.1071\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0194 - mae: 0.1066\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 0.0181 - mae: 0.1045\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0188 - mae: 0.1058\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0184 - mae: 0.1037\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0187 - mae: 0.1056\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 0.0181 - mae: 0.1036\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0184 - mae: 0.1045\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0179 - mae: 0.1030\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0179 - mae: 0.1027\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0173 - mae: 0.1010\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0172 - mae: 0.1010\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0175 - mae: 0.1019\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 0.0164 - mae: 0.0984\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0165 - mae: 0.0986\n",
      "Kappa Score 3 : 0.7726595499638282\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 58s 338ms/step - loss: 0.0984 - mae: 0.2402\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 59s 360ms/step - loss: 0.0436 - mae: 0.1643\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 59s 362ms/step - loss: 0.0396 - mae: 0.1565\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 59s 362ms/step - loss: 0.0370 - mae: 0.1488\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 59s 363ms/step - loss: 0.0350 - mae: 0.1470\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 0.0324 - mae: 0.1400\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 60s 366ms/step - loss: 0.0311 - mae: 0.1372\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0305 - mae: 0.1364\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 0.0326 - mae: 0.1390\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.0302 - mae: 0.1335\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 60s 371ms/step - loss: 0.0290 - mae: 0.1314\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0284 - mae: 0.1307\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0280 - mae: 0.1290\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0263 - mae: 0.1257\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0273 - mae: 0.1270\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0262 - mae: 0.1245\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 60s 371ms/step - loss: 0.0258 - mae: 0.1238\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.0250 - mae: 0.1222\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.0248 - mae: 0.1212\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 60s 371ms/step - loss: 0.0251 - mae: 0.1214\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0234 - mae: 0.1177\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.0233 - mae: 0.1172\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 60s 369ms/step - loss: 0.0232 - mae: 0.1161\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 60s 368ms/step - loss: 0.0225 - mae: 0.1147\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 60s 370ms/step - loss: 0.0226 - mae: 0.1153\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0215 - mae: 0.1129\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0227 - mae: 0.1144\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0212 - mae: 0.1113\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0211 - mae: 0.1105\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0220 - mae: 0.1138\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 61s 371ms/step - loss: 0.0203 - mae: 0.1093\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 61s 375ms/step - loss: 0.0204 - mae: 0.1095\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 61s 374ms/step - loss: 0.0205 - mae: 0.1104\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 61s 375ms/step - loss: 0.0203 - mae: 0.1085\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 61s 374ms/step - loss: 0.0202 - mae: 0.1089\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 61s 376ms/step - loss: 0.0196 - mae: 0.1073\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0205 - mae: 0.1101\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0190 - mae: 0.1069\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0190 - mae: 0.1048\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0185 - mae: 0.1041\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 60s 371ms/step - loss: 0.0187 - mae: 0.1048\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0188 - mae: 0.1046\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0181 - mae: 0.1036\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0182 - mae: 0.1035\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0182 - mae: 0.1031\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 61s 375ms/step - loss: 0.0177 - mae: 0.1021\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 61s 374ms/step - loss: 0.0170 - mae: 0.1009\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0171 - mae: 0.0997\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 61s 372ms/step - loss: 0.0169 - mae: 0.0992\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 61s 373ms/step - loss: 0.0170 - mae: 0.1005\n",
      "Kappa Score 4 : 0.7710267388564125\n",
      "Average Kappa score after a 5-fold cross validation:  0.7594\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "sentence_models = []\n",
    "sentence_results = []\n",
    "for cnt in range(5):\n",
    "    \n",
    "    print(\"\\n--------Fold {}--------\\n\".format(cnt))\n",
    "    sentence_model = get_sentence_model()\n",
    "    sentence_model.fit(trainData_sent[cnt], y_trainData_sent[cnt], batch_size=64, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "    y_sent_pred = sentence_model.predict(testData_sent[cnt]) * 100\n",
    "\n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_sent_pred2 = np.round(y_sent_pred)\n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    sentence_result = cohen_kappa_score(np.round(y_testData_sent[cnt].values * 100), y_sent_pred2, weights='quadratic')\n",
    "    print(\"Kappa Score\", cnt, \": {}\".format(sentence_result))\n",
    "    sentence_results.append(sentence_result)\n",
    "    sentence_models.append(sentence_model)\n",
    "\n",
    "print(\"Average Kappa score after a 5-fold cross validation: \", np.round(np.array(sentence_results).mean(),decimals=4))\n",
    "\n",
    "if np.round(np.array(sentence_results).mean(),decimals=4) > 0.75:\n",
    "    sentence_models[sentence_results.index(max(sentence_results))].save('./final_gru_sent.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에세이 프롬프트 별로 문장단위 전처리 + 모델 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Preprocessing Started\n",
      "##Preprocessing Started\n",
      "##Preprocessing Started\n",
      "##Preprocessing Started\n",
      "##Preprocessing Started\n",
      "\n",
      "--------Fold 0--------\n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 13s 423ms/step - loss: 0.1877 - mae: 0.3529\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 10s 419ms/step - loss: 0.0608 - mae: 0.1961\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0474 - mae: 0.1708\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0383 - mae: 0.1557\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0413 - mae: 0.1591\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.0352 - mae: 0.1482\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0311 - mae: 0.1402\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0270 - mae: 0.1292\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.0265 - mae: 0.1287\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 9s 404ms/step - loss: 0.0251 - mae: 0.1250\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0261 - mae: 0.1289\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0238 - mae: 0.1207\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0233 - mae: 0.1216\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0214 - mae: 0.1154\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 10s 413ms/step - loss: 0.0213 - mae: 0.1162\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0192 - mae: 0.1095\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.0205 - mae: 0.1133\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0199 - mae: 0.1112\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 9s 408ms/step - loss: 0.0210 - mae: 0.1153\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 9s 401ms/step - loss: 0.0187 - mae: 0.1094\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 9s 400ms/step - loss: 0.0173 - mae: 0.1039\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0207 - mae: 0.1149\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0170 - mae: 0.1020\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0183 - mae: 0.1078\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.0172 - mae: 0.1060\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0170 - mae: 0.1039\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 9s 399ms/step - loss: 0.0180 - mae: 0.1069\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0164 - mae: 0.1008\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0166 - mae: 0.1032\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 9s 399ms/step - loss: 0.0151 - mae: 0.0979\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.0165 - mae: 0.1022\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 9s 398ms/step - loss: 0.0151 - mae: 0.0977\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 9s 399ms/step - loss: 0.0152 - mae: 0.0985\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0158 - mae: 0.0987\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0150 - mae: 0.0978\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0150 - mae: 0.0971\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 9s 400ms/step - loss: 0.0155 - mae: 0.1005\n",
      "Kappa Score 0 : 0.017840096347693013\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_14 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 13s 423ms/step - loss: 0.1754 - mae: 0.3404\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 10s 419ms/step - loss: 0.0576 - mae: 0.1887\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 10s 420ms/step - loss: 0.0405 - mae: 0.1622\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.0408 - mae: 0.1628\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0336 - mae: 0.1442\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0320 - mae: 0.1434\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 10s 418ms/step - loss: 0.0268 - mae: 0.1328\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 10s 418ms/step - loss: 0.0260 - mae: 0.1298\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0222 - mae: 0.1187\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.0223 - mae: 0.1198\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0229 - mae: 0.1225\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0216 - mae: 0.1187\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0196 - mae: 0.1107\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.0213 - mae: 0.1188\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 9s 412ms/step - loss: 0.0202 - mae: 0.1132\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0196 - mae: 0.1107\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0195 - mae: 0.1119\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0176 - mae: 0.1029\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 10s 418ms/step - loss: 0.0180 - mae: 0.1081\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 10s 417ms/step - loss: 0.0165 - mae: 0.1030\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 10s 415ms/step - loss: 0.0179 - mae: 0.1053\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.0184 - mae: 0.1091\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 10s 416ms/step - loss: 0.0172 - mae: 0.1035\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 10s 419ms/step - loss: 0.0160 - mae: 0.0989\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 10s 419ms/step - loss: 0.0162 - mae: 0.1004\n",
      "Kappa Score 1 : 0.011674401603845697\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_16 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_17 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 13s 413ms/step - loss: 0.1337 - mae: 0.2890\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.0527 - mae: 0.1805\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0469 - mae: 0.1725\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0430 - mae: 0.1641\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0336 - mae: 0.1463\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0303 - mae: 0.1355\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0283 - mae: 0.1341\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 9s 408ms/step - loss: 0.0254 - mae: 0.1270\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0231 - mae: 0.1186\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0244 - mae: 0.1253\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0250 - mae: 0.1241\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0205 - mae: 0.1145\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0213 - mae: 0.1130\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0203 - mae: 0.1118\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0211 - mae: 0.1122\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0209 - mae: 0.1155\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0194 - mae: 0.1101\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0192 - mae: 0.1076\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0174 - mae: 0.1044\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0187 - mae: 0.1083\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0185 - mae: 0.1096\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0187 - mae: 0.1082\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0171 - mae: 0.1031\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0164 - mae: 0.1009\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0158 - mae: 0.1005\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0166 - mae: 0.1011\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 9s 408ms/step - loss: 0.0173 - mae: 0.1033\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0156 - mae: 0.0987\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0161 - mae: 0.0999\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0138 - mae: 0.0924\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0137 - mae: 0.0935\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 9s 400ms/step - loss: 0.0155 - mae: 0.0985\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0158 - mae: 0.0994\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 9s 402ms/step - loss: 0.0154 - mae: 0.0980\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 9s 403ms/step - loss: 0.0163 - mae: 0.1017\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 9s 400ms/step - loss: 0.0144 - mae: 0.0940\n",
      "Kappa Score 2 : 0.012547042835563071\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_18 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_19 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 12s 402ms/step - loss: 0.1384 - mae: 0.2960\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0533 - mae: 0.1817\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0427 - mae: 0.1630\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 0.0371 - mae: 0.1568\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 9s 390ms/step - loss: 0.0319 - mae: 0.1424\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 9s 390ms/step - loss: 0.0348 - mae: 0.1454\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0291 - mae: 0.1351\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0259 - mae: 0.1286\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 9s 390ms/step - loss: 0.0229 - mae: 0.1194\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0237 - mae: 0.1219\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0213 - mae: 0.1159\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 9s 391ms/step - loss: 0.0204 - mae: 0.1128\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0204 - mae: 0.1113\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0182 - mae: 0.1083\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0192 - mae: 0.1108\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0188 - mae: 0.1098\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0188 - mae: 0.1078\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0180 - mae: 0.1065\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0193 - mae: 0.1089\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0171 - mae: 0.1046\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0174 - mae: 0.1042\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0171 - mae: 0.1047\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0150 - mae: 0.0965\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0163 - mae: 0.1024\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0152 - mae: 0.0971\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0153 - mae: 0.1001\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0160 - mae: 0.1009\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 9s 397ms/step - loss: 0.0159 - mae: 0.1007\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0141 - mae: 0.0944\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0155 - mae: 0.0996\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0137 - mae: 0.0928\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0133 - mae: 0.0916\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0145 - mae: 0.0938\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 9s 393ms/step - loss: 0.0159 - mae: 0.1010\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0148 - mae: 0.0949\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0139 - mae: 0.0939\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 9s 396ms/step - loss: 0.0147 - mae: 0.0947\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0137 - mae: 0.0936\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0132 - mae: 0.0922\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0139 - mae: 0.0928\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0138 - mae: 0.0923\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 0.0145 - mae: 0.0956\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0136 - mae: 0.0928\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0137 - mae: 0.0923\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0126 - mae: 0.0891\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0130 - mae: 0.0917\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 9s 395ms/step - loss: 0.0130 - mae: 0.0899\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0126 - mae: 0.0899\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 9s 394ms/step - loss: 0.0122 - mae: 0.0878\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 9s 392ms/step - loss: 0.0128 - mae: 0.0902\n",
      "Kappa Score 3 : 0.01529849067638378\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_20 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_21 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 12s 403ms/step - loss: 0.1541 - mae: 0.3119\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.0556 - mae: 0.1847\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0485 - mae: 0.1750\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0454 - mae: 0.1719\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 10s 413ms/step - loss: 0.0381 - mae: 0.1544\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 10s 414ms/step - loss: 0.0415 - mae: 0.1599\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0319 - mae: 0.1409\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0302 - mae: 0.1390\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 9s 411ms/step - loss: 0.0275 - mae: 0.1336\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 10s 413ms/step - loss: 0.0256 - mae: 0.1271\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 9s 410ms/step - loss: 0.0246 - mae: 0.1235\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0223 - mae: 0.1193\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0210 - mae: 0.1166\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0200 - mae: 0.1130\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 9s 409ms/step - loss: 0.0201 - mae: 0.1099\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0209 - mae: 0.1146\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0240 - mae: 0.1250\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0202 - mae: 0.1138\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0186 - mae: 0.1085\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0184 - mae: 0.1054\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0188 - mae: 0.1092\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0184 - mae: 0.1075\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0175 - mae: 0.1050\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 9s 408ms/step - loss: 0.0172 - mae: 0.1049\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0152 - mae: 0.0982\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0167 - mae: 0.1021\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 9s 405ms/step - loss: 0.0166 - mae: 0.1014\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 9s 408ms/step - loss: 0.0166 - mae: 0.1024\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 9s 406ms/step - loss: 0.0154 - mae: 0.0975\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 9s 407ms/step - loss: 0.0153 - mae: 0.0992\n",
      "Kappa Score 4 : 0.012129543636380413\n",
      "Average Kappa score of prompt  0  after a 5-fold cross validation:  0.0139\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-8b2df9375be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0msentence_results_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0msentence_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentence_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./gru_sentence_set_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0messay_set\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Essay set {} model completed\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0messay_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "sentence_models_set = []\n",
    "sentence_results_set = []\n",
    "for essay_set in range(8):\n",
    "    cv2 = KFold(n_splits=5, shuffle=True)\n",
    "    num_features = 200\n",
    "\n",
    "    trainData_set = []\n",
    "    testData_set = []\n",
    "    Y_trainData_set = []\n",
    "    Y_testData_set = []\n",
    "    sentence_models = []\n",
    "    sentence_results = []\n",
    "    \n",
    "    for traincv, testcv in cv2.split(X_set[essay_set]):\n",
    "        print('##Preprocessing Started')\n",
    "        \n",
    "        X_train_set, X_test_set = X_set[essay_set]['essay'].iloc[traincv], X_set[essay_set]['essay'].iloc[testcv]\n",
    "        Y_train_set, Y_test_set = X_set[essay_set]['score'].iloc[traincv], X_set[essay_set]['score'].iloc[testcv]\n",
    "\n",
    "        Y_trainData_set.append(Y_train_set)\n",
    "        Y_testData_set.append(Y_test_set)\n",
    "\n",
    "        trainDataVecs = []\n",
    "        testDataVecs = []\n",
    "\n",
    "        for essay in X_train_set:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "            trainDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "            trainDataVecs.append(np.array(trainDataVec, dtype=\"float32\"))\n",
    "\n",
    "        for essay in X_test_set:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "            testDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "            testDataVecs.append(np.array(testDataVec, dtype=\"float32\"))\n",
    "\n",
    "        trainDataVecs = pad_sequences(trainDataVecs, maxlen=128, padding='pre', dtype='float')\n",
    "        testDataVecs = pad_sequences(testDataVecs, maxlen=128, padding='pre', dtype='float')\n",
    "        trainData_set.append(np.array(trainDataVecs, dtype=\"float32\"))\n",
    "        testData_set.append(np.array(testDataVecs, dtype=\"float32\"))\n",
    "    for cnt in range(5):\n",
    "        print(\"\\n--------Fold {}--------\\n\".format(cnt))\n",
    "        sentence_model2 = get_sentence_model()\n",
    "        sentence_model2.fit(trainData_set[cnt], Y_trainData_set[cnt], batch_size=64, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "        sen_y_pred = sentence_model2.predict(testData_set[cnt])\n",
    "        \n",
    "        # Round y_pred to the nearest integer.\n",
    "        sen_y_pred2 = np.round(sen_y_pred)\n",
    "        # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "        result = cohen_kappa_score(Y_testData_set[cnt].values * 100, sen_y_pred2, weights='quadratic')\n",
    "        print(\"Kappa Score\", cnt, \": {}\".format(result))\n",
    "        sentence_models.append(sentence_model2)\n",
    "        sentence_results.append(result)\n",
    "        \n",
    "    print(\"Average Kappa score of prompt \", essay_set, \" after a 5-fold cross validation: \", np.round(np.array(sentence_results).mean(),decimals=4))\n",
    "    sentence_models_set.append(sentence_models)\n",
    "    sentence_results_set.append(sentence_results)\n",
    "    sentence_models[sentence_results.index(max(sentence_results))].save('./gru_sentence_set_' + str(essay_set) +'.h5')\n",
    "    print(\"Essay set {} model completed\".format(essay_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 0--------\n",
      "\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_6 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 58s 333ms/step - loss: 2783.0079 - mae: 47.8111\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 60s 368ms/step - loss: 1826.9862 - mae: 37.1765\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 1246.1308 - mae: 29.5371\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 814.0512 - mae: 23.3068\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 638.3116 - mae: 20.2533\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 598.3554 - mae: 19.5029\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 620.9554 - mae: 19.7572\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 600.9648 - mae: 19.3546\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 59s 364ms/step - loss: 598.2193 - mae: 19.5184\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 597.8500 - mae: 19.4060\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 59s 365ms/step - loss: 609.9983 - mae: 19.6333\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 60s 367ms/step - loss: 592.8093 - mae: 19.2753\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 60s 365ms/step - loss: 608.1195 - mae: 19.6052\n",
      "Kappa Score 0 : 0.0\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_8 (GRU)                  (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 68s 397ms/step - loss: 2664.0943 - mae: 46.6602\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 1777.7738 - mae: 36.6370\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 1168.8157 - mae: 28.5339\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 795.3988 - mae: 22.9637\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 618.0948 - mae: 20.0658\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 610.9498 - mae: 19.6014\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 600.1251 - mae: 19.5096\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 62s 381ms/step - loss: 599.3602 - mae: 19.5397\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 62s 382ms/step - loss: 591.8100 - mae: 19.4707\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 570.1912 - mae: 18.9088\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 551.3456 - mae: 18.5944s - loss: 551.6664 - mae: 18.60\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 62s 382ms/step - loss: 407.1199 - mae: 15.7470\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 62s 382ms/step - loss: 360.9820 - mae: 14.7433\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 342.3791 - mae: 14.3512\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 62s 382ms/step - loss: 319.1824 - mae: 13.8376\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 62s 383ms/step - loss: 327.8088 - mae: 13.9613\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 309.5927 - mae: 13.6337\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 310.4043 - mae: 13.5431\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 299.9230 - mae: 13.3841\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 303.6196 - mae: 13.4994\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 292.5194 - mae: 13.2705\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 287.7681 - mae: 13.1815\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 281.3647 - mae: 13.0479\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 284.8620 - mae: 13.0923\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 278.3053 - mae: 12.9223\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 262.6474 - mae: 12.5738\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 270.6191 - mae: 12.6564\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 268.0745 - mae: 12.6796\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 277.3081 - mae: 12.9358\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 253.6043 - mae: 12.2291\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 261.9064 - mae: 12.5434\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 261.8854 - mae: 12.4451\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 251.9033 - mae: 12.3551\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 250.9712 - mae: 12.3104\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 252.3357 - mae: 12.3905\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 249.2677 - mae: 12.2555\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 243.8447 - mae: 12.0923\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 247.1837 - mae: 12.0967\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 248.6681 - mae: 12.1913\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 260.0704 - mae: 12.3853\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 243.9692 - mae: 12.0423\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 242.2227 - mae: 12.0456\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 233.7824 - mae: 11.9246\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 230.9053 - mae: 11.7757\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 231.9524 - mae: 11.7690\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 236.9856 - mae: 11.9220\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 228.8446 - mae: 11.7056\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 225.6040 - mae: 11.5820\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 230.5424 - mae: 11.7784\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 63s 386ms/step - loss: 222.0669 - mae: 11.6293\n",
      "Kappa Score 1 : 0.7615001829652579\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_10 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 70s 407ms/step - loss: 2698.9862 - mae: 46.9472\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 64s 394ms/step - loss: 1782.7976 - mae: 36.6597\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 64s 394ms/step - loss: 1166.6186 - mae: 28.3266\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 796.0179 - mae: 22.9172\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 644.3869 - mae: 20.4783\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 588.0925 - mae: 19.2655\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 605.2076 - mae: 19.5776\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 583.1593 - mae: 19.1898\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 584.9104 - mae: 19.0861\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 474.9450 - mae: 17.1139\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 390.9898 - mae: 15.3979\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 357.0110 - mae: 14.7236\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 338.9831 - mae: 14.3504\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 325.8621 - mae: 14.0626\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 320.7015 - mae: 13.8728\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 313.1473 - mae: 13.8001\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 302.8251 - mae: 13.5089\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 296.0007 - mae: 13.3470\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 302.5915 - mae: 13.5119\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 291.3194 - mae: 13.2203\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 288.1180 - mae: 13.1559\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 275.3700 - mae: 12.9387\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 279.0655 - mae: 12.9318\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 269.5756 - mae: 12.7967\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 276.4449 - mae: 12.9335\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 263.9300 - mae: 12.5636\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 270.6768 - mae: 12.7799\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 267.4804 - mae: 12.6564\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 271.4495 - mae: 12.7878\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 260.4069 - mae: 12.4547\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 252.9485 - mae: 12.3846\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 256.5190 - mae: 12.3577\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 245.5736 - mae: 12.2092\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 254.2232 - mae: 12.3127\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 252.2941 - mae: 12.3476\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 64s 395ms/step - loss: 252.1920 - mae: 12.2629\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 246.6839 - mae: 12.1747\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 241.5557 - mae: 12.0872\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 252.2612 - mae: 12.2864\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 238.3836 - mae: 11.9359\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 230.7551 - mae: 11.7234\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 232.9567 - mae: 11.7356\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 235.6827 - mae: 11.8694\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 223.9561 - mae: 11.6329\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 226.2563 - mae: 11.6392\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 65s 397ms/step - loss: 226.2552 - mae: 11.5986\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 64s 395ms/step - loss: 236.6425 - mae: 11.9451\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 65s 397ms/step - loss: 224.9735 - mae: 11.6751\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 65s 396ms/step - loss: 223.3502 - mae: 11.4946\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 64s 396ms/step - loss: 231.8108 - mae: 11.7190\n",
      "Kappa Score 2 : 0.7492028560230674\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_12 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_13 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 69s 402ms/step - loss: 2700.5180 - mae: 47.0218\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 1776.0940 - mae: 36.6303\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 1172.0170 - mae: 28.5673\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 819.2843 - mae: 23.4317\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 623.2681 - mae: 20.0615\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 593.3279 - mae: 19.4272\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 600.0476 - mae: 19.4241\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 64s 393ms/step - loss: 603.8545 - mae: 19.5194\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 64s 394ms/step - loss: 576.8445 - mae: 19.0755\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 64s 392ms/step - loss: 430.9188 - mae: 16.2612\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 64s 391ms/step - loss: 375.5451 - mae: 15.0067\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 63s 389ms/step - loss: 346.8909 - mae: 14.5006\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 331.5986 - mae: 14.1800\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 329.6855 - mae: 14.1788\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 314.6169 - mae: 13.6816\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 313.3837 - mae: 13.6865\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 302.6758 - mae: 13.4418\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 298.4007 - mae: 13.3707\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 64s 390ms/step - loss: 287.1404 - mae: 13.2369\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 285.1756 - mae: 13.1377\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 276.1406 - mae: 12.8497\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 284.0871 - mae: 13.0664\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 276.5415 - mae: 12.8916\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 63s 389ms/step - loss: 276.0707 - mae: 12.8830\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 276.0999 - mae: 12.8443\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 271.3954 - mae: 12.6268\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 266.2224 - mae: 12.6142\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 264.3370 - mae: 12.6497\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 263.9020 - mae: 12.6576\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 251.9931 - mae: 12.2713\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 253.5399 - mae: 12.2712\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 253.0937 - mae: 12.4349\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 246.8149 - mae: 12.2082\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 248.1676 - mae: 12.2356\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 247.2718 - mae: 12.2169\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 246.9150 - mae: 12.1368\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 240.2288 - mae: 12.0766\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 244.9096 - mae: 12.0249\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 230.7343 - mae: 11.8780\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 228.0091 - mae: 11.7971\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 63s 384ms/step - loss: 230.9700 - mae: 11.8384\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 230.4503 - mae: 11.7965\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 63s 385ms/step - loss: 225.2252 - mae: 11.5713\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 222.5059 - mae: 11.6808\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 224.7003 - mae: 11.5957\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 225.6880 - mae: 11.7077\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 221.5724 - mae: 11.5652\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 63s 387ms/step - loss: 220.4552 - mae: 11.5691\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 63s 388ms/step - loss: 212.3782 - mae: 11.2798\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 63s 386ms/step - loss: 221.5996 - mae: 11.5720\n",
      "Kappa Score 3 : 0.7672508889309195\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru_14 (GRU)                 (None, 128, 128)          126720    \n",
      "_________________________________________________________________\n",
      "gru_15 (GRU)                 (None, 64)                37248     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 164,033\n",
      "Trainable params: 164,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 61s 357ms/step - loss: 2863.1302 - mae: 48.5934\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 1880.7627 - mae: 37.9432\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 1189.7796 - mae: 28.8296\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 832.3051 - mae: 23.5809\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 653.3829 - mae: 20.5817\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 592.6184 - mae: 19.3086\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 603.5487 - mae: 19.4319\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 419.0467 - mae: 15.9682\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 358.5733 - mae: 14.6377\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 339.6839 - mae: 14.3253\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 332.3861 - mae: 14.2218\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 326.3747 - mae: 14.0194\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 324.7304 - mae: 13.8673\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 304.5798 - mae: 13.5973\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 310.2593 - mae: 13.6239\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 296.8377 - mae: 13.3528\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 299.7145 - mae: 13.4443\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 286.2148 - mae: 13.1288\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 280.8995 - mae: 13.0576\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 291.5451 - mae: 13.1876\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 277.1181 - mae: 12.9750\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 288.1568 - mae: 13.1498\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 279.4674 - mae: 12.9755\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 267.3393 - mae: 12.6742\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 266.2340 - mae: 12.5907\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 58s 357ms/step - loss: 265.1688 - mae: 12.5445\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 58s 357ms/step - loss: 265.9642 - mae: 12.6803\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 265.1348 - mae: 12.6591\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 256.0086 - mae: 12.3822\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 255.9173 - mae: 12.4533\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 259.0353 - mae: 12.4982\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 58s 354ms/step - loss: 255.1970 - mae: 12.3667\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 260.5723 - mae: 12.3768\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 58s 357ms/step - loss: 250.4644 - mae: 12.3012\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 251.5186 - mae: 12.3102\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 248.8978 - mae: 12.3002\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 58s 355ms/step - loss: 237.6963 - mae: 11.9000\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 240.2095 - mae: 12.0811\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 233.6235 - mae: 11.8692\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 58s 356ms/step - loss: 235.4961 - mae: 11.8741s - loss: 235.4723 - mae: 11.87\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 58s 357ms/step - loss: 240.4034 - mae: 12.0162\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 238.6226 - mae: 11.8858\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 229.0429 - mae: 11.7751\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 59s 360ms/step - loss: 235.9846 - mae: 11.9685\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 59s 360ms/step - loss: 227.9260 - mae: 11.7007\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 222.7273 - mae: 11.6167\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 58s 358ms/step - loss: 228.2633 - mae: 11.6415\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 59s 359ms/step - loss: 224.3774 - mae: 11.6270\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 59s 359ms/step - loss: 218.2592 - mae: 11.3974\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 59s 360ms/step - loss: 216.7521 - mae: 11.4558\n",
      "Kappa Score 4 : 0.7593355934631674\n",
      "Average Kappa score after a 5-fold cross validation:  0.6075\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-170-612cdda36e76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Average Kappa score after a 5-fold cross validation: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0msentence_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./final_gru.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "sentence_models = []\n",
    "results = []\n",
    "for cnt in range(5):\n",
    "    \n",
    "    print(\"\\n--------Fold {}--------\\n\".format(cnt))\n",
    "    sentence_model = get_sentence_model()\n",
    "    sentence_model.fit(trainData[cnt], y_trainData[cnt], batch_size=64, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "    y_pred = sentence_model.predict(testData[cnt])\n",
    "\n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.round(y_pred)\n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_testData[cnt].values, y_pred, weights='quadratic')\n",
    "    print(\"Kappa Score\", cnt, \": {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "print(\"Average Kappa score after a 5-fold cross validation: \", np.round(np.array(results).mean(),decimals=4))\n",
    "\n",
    "sentence_models[results.index(max(results))].save('./final_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model.save('./final_gru.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Traning Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 5s 11ms/step - loss: 3160.1289 - mae: 51.3129\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 2153.6464 - mae: 41.2872\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 1587.3351 - mae: 34.2647\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 1161.0362 - mae: 28.2971\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 857.8043 - mae: 23.9808\n",
      "Kappa Score: 0.0012404740275122617\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 6s 11ms/step - loss: 3046.5057 - mae: 50.1823\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 2079.1340 - mae: 40.4082\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 1554.0633 - mae: 33.7826\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 1138.2284 - mae: 28.0496\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 829.8637 - mae: 23.5177\n",
      "Kappa Score: 0.0\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 6s 12ms/step - loss: 3066.5268 - mae: 50.5256\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 2106.8816 - mae: 40.5909\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1554.5862 - mae: 33.7560\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1119.8924 - mae: 27.7801\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 829.5748 - mae: 23.5355\n",
      "Kappa Score: 0.00010179909020457956\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 5s 12ms/step - loss: 3112.4767 - mae: 50.9231\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 2139.0122 - mae: 40.9197\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1581.2568 - mae: 34.1714\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1126.1061 - mae: 27.8803\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 850.0529 - mae: 23.8111\n",
      "Kappa Score: 0.0\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "163/163 [==============================] - 5s 12ms/step - loss: 3165.5266 - mae: 51.5108\n",
      "Epoch 2/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 2227.9326 - mae: 42.1804\n",
      "Epoch 3/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1644.6038 - mae: 35.0024\n",
      "Epoch 4/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 1213.7288 - mae: 29.0126\n",
      "Epoch 5/5\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 874.4031 - mae: 24.2532\n",
      "Kappa Score: 0.0003894149414790382\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    \n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "\n",
    "    num_features = 200 \n",
    "    \n",
    "    model = embedding_dict\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    \n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./final_lstm.h5')\n",
    "            \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.6427\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \", np.round(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the SAT 1 score essay scored [[63.]]\n",
      "the SAT 4 score essay scored [[78.]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "contentBad = \"\"\"\n",
    "    In “Let there be dark,” Paul Bogard talks about the importance of darkness.\n",
    "\n",
    "Darkness is essential to humans. Bogard states, “Our bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of “short sleep” is “long light.” Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isn’t a place for this much artificial light in our lives.” (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n",
    "\n",
    "Animals also need darkness. Bogard states, “The rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well known—the 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggs—and some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the world’s flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earth’s ecology would collapse...” (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n",
    "\"\"\" \n",
    "\n",
    "contentGood = \"\"\"\n",
    "    In response to our world’s growing reliance on artificial light, writer Paul Bogard argues that natural darkness should be preserved in his article “Let There be dark”. He effectively builds his argument by using a personal anecdote, allusions to art and history, and rhetorical questions.\n",
    "\n",
    "Bogard starts his article off by recounting a personal story – a summer spent on a Minnesota lake where there was “woods so dark that [his] hands disappeared before [his] eyes.” In telling this brief anecdote, Bogard challenges the audience to remember a time where they could fully amass themselves in natural darkness void of artificial light. By drawing in his readers with a personal encounter about night darkness, the author means to establish the potential for beauty, glamour, and awe-inspiring mystery that genuine darkness can possess. He builds his argument for the preservation of natural darkness by reminiscing for his readers a first-hand encounter that proves the “irreplaceable value of darkness.” This anecdote provides a baseline of sorts for readers to find credence with the author’s claims.\n",
    "\n",
    "Bogard’s argument is also furthered by his use of allusion to art – Van Gogh’s “Starry Night” – and modern history – Paris’ reputation as “The City of Light”. By first referencing “Starry Night”, a painting generally considered to be undoubtedly beautiful, Bogard establishes that the natural magnificence of stars in a dark sky is definite. A world absent of excess artificial light could potentially hold the key to a grand, glorious night sky like Van Gogh’s according to the writer. This urges the readers to weigh the disadvantages of our world consumed by unnatural, vapid lighting. Furthermore, Bogard’s alludes to Paris as “the famed ‘city of light’”. He then goes on to state how Paris has taken steps to exercise more sustainable lighting practices. By doing this, Bogard creates a dichotomy between Paris’ traditionally alluded-to name and the reality of what Paris is becoming – no longer “the city of light”, but moreso “the city of light…before 2 AM”. This furthers his line of argumentation because it shows how steps can be and are being taken to preserve natural darkness. It shows that even a city that is literally famous for being constantly lit can practically address light pollution in a manner that preserves the beauty of both the city itself and the universe as a whole.\n",
    "\n",
    "Finally, Bogard makes subtle yet efficient use of rhetorical questioning to persuade his audience that natural darkness preservation is essential. He asks the readers to consider “what the vision of the night sky might inspire in each of us, in our children or grandchildren?” in a way that brutally plays to each of our emotions. By asking this question, Bogard draws out heartfelt ponderance from his readers about the affecting power of an untainted night sky. This rhetorical question tugs at the readers’ heartstrings; while the reader may have seen an unobscured night skyline before, the possibility that their child or grandchild will never get the chance sways them to see as Bogard sees. This strategy is definitively an appeal to pathos, forcing the audience to directly face an emotionally-charged inquiry that will surely spur some kind of response. By doing this, Bogard develops his argument, adding gutthral power to the idea that the issue of maintaining natural darkness is relevant and multifaceted.\n",
    "\n",
    "Writing as a reaction to his disappointment that artificial light has largely permeated the prescence of natural darkness, Paul Bogard argues that we must preserve true, unaffected darkness. He builds this claim by making use of a personal anecdote, allusions, and rhetorical questioning.\n",
    "\"\"\"\n",
    "\n",
    "def testContent(content):\n",
    "    if len(content) > 20:\n",
    "        num_features = 200\n",
    "        clean_test_essays = []\n",
    "        clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
    "        testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "        testDataVecs = np.array(testDataVecs)\n",
    "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "        preds = lstm_model.predict(testDataVecs)\n",
    "\n",
    "        if math.isnan(preds):\n",
    "            preds = 0\n",
    "        else:\n",
    "            preds = np.round(preds)\n",
    "\n",
    "        if preds < 0:\n",
    "            preds = 0\n",
    "    else:\n",
    "        preds = 0\n",
    "\n",
    "    return preds\n",
    "    \n",
    "print(\"the SAT 1 score essay scored\", testContent(contentBad))\n",
    "print(\"the SAT 4 score essay scored\", testContent(contentGood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Pickle glove embeddings\n",
    "with open('embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
