{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\n",
    "maximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  score  \n",
       "0              8   60.0  \n",
       "1              9   70.0  \n",
       "2              7   50.0  \n",
       "3             10   80.0  \n",
       "4              8   60.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_min = minimum_scores[X['essay_set']]\n",
    "old_max = maximum_scores[X['essay_set']]\n",
    "old_range = old_max - old_min\n",
    "new_min = 0\n",
    "new_max = 100\n",
    "new_range = (new_max - new_min)  \n",
    "X['score'] = (((X['domain1_score'] - old_min) * new_range) / old_range) + new_min\n",
    "\n",
    "# round score to nearest integer for cohen kappa calculation\n",
    "y = np.round(X['score'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tmax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tmax\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])       \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def makeFeatureVec2(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    if len(words) != 0:\n",
    "        featureVec = np.divide(featureVec,float(len(words)))\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs\n",
    "\n",
    "def getAvgFeatureVecs2(essay, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    essayFeatureVecs = np.zeros((len(essay),num_features),dtype=\"float32\")\n",
    "    for cnt, sentence in enumerate(essay):\n",
    "        essayFeatureVecs[cnt] = makeFeatureVec2(sentence, model, num_features)\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "      <td>58.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "      <td>53.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score      score  \n",
       "0                  8  60.000000  \n",
       "1                  9  70.000000  \n",
       "2                  7  50.000000  \n",
       "3                 10  80.000000  \n",
       "4                  8  60.000000  \n",
       "...              ...        ...  \n",
       "12971             35  58.333333  \n",
       "12972             32  53.333333  \n",
       "12973             40  66.666667  \n",
       "12974             40  66.666667  \n",
       "12975             40  66.666667  \n",
       "\n",
       "[12976 rows x 5 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Fold Started\n",
      "10380\n",
      "2596\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n",
      "##Fold Started\n",
      "10381\n",
      "2595\n"
     ]
    }
   ],
   "source": [
    "cv2 = KFold(n_splits=5, shuffle=True)\n",
    "trainData = []\n",
    "testData = []\n",
    "y_trainData = []\n",
    "y_testData = []\n",
    "for traincv, testcv in cv2.split(X):\n",
    "    print('##Fold Started')\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    y_trainData.append(y_train)\n",
    "    y_testData.append(y_test)\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "\n",
    "    trainDataVecs = []\n",
    "    testDataVecs = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "        trainDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "        trainDataVecs.append(np.array(trainDataVec))\n",
    "\n",
    "    for essay in test_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences = essay_to_sentences(essay, remove_stopwords = True)\n",
    "        testDataVec = getAvgFeatureVecs2(sentences, embedding_dict, num_features)\n",
    "        testDataVecs.append(np.array(testDataVec))\n",
    "    \n",
    "    trainDataVecs = pad_sequences(trainDataVecs, maxlen=128, padding='pre')\n",
    "    testDataVecs = pad_sequences(testDataVecs, maxlen=128, padding='pre')\n",
    "    \n",
    "    trainData.append(np.array(trainDataVecs))\n",
    "    testData.append(np.array(testDataVecs))\n",
    "    print(len(trainDataVecs))\n",
    "    print(len(testDataVecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10380, 128, 200)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 128, 200)          320800    \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 87s 516ms/step - loss: 2744.1730 - mae: 47.4264\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 90s 554ms/step - loss: 1766.1610 - mae: 36.5305\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 90s 551ms/step - loss: 1160.6017 - mae: 28.3124\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 92s 567ms/step - loss: 788.0730 - mae: 23.0155\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 93s 569ms/step - loss: 620.9207 - mae: 20.0373\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 93s 569ms/step - loss: 601.8450 - mae: 19.6028\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 94s 579ms/step - loss: 597.0498 - mae: 19.3679\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 93s 571ms/step - loss: 610.7814 - mae: 19.7046\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 94s 578ms/step - loss: 543.5765 - mae: 18.3394\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 92s 567ms/step - loss: 499.2356 - mae: 17.5925\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 92s 566ms/step - loss: 479.5268 - mae: 17.1701\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 92s 563ms/step - loss: 567.0622 - mae: 18.8094\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 91s 559ms/step - loss: 490.2878 - mae: 17.3716\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 92s 565ms/step - loss: 494.3501 - mae: 17.4894\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 91s 559ms/step - loss: 528.4256 - mae: 18.0717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ea091fb0d0>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "sentence_model = get_sentence_model()\n",
    "sentence_model.fit(trainData[0], y_trainData[0], batch_size=64, epochs=50, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: 0.4727047860436322\n"
     ]
    }
   ],
   "source": [
    "y_pred = sentence_model.predict(testData[0])\n",
    "\n",
    "# Save any one of the 8 models.\n",
    "if count == 5:\n",
    "     sentence_model.save('./final_lstm.h5')\n",
    "\n",
    "# Round y_pred to the nearest integer.\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "# Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "result = cohen_kappa_score(y_testData[0].values,y_pred,weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))\n",
    "results.append(result)\n",
    "\n",
    "count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 200], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_sentence_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, input_shape=[128, 200], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove embeddings\n",
    "embedding_dict={}\n",
    "with open(os.path.join(DATASET_DIR, 'glove/glove.6B.200d.txt'),'r', encoding='UTF8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word] = vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Trainig Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 3ms/step - loss: 3123.5160 - mae: 51.0037\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2157.0610 - mae: 41.2123\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1590.6437 - mae: 34.2053\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 1150.4660 - mae: 28.1779\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 858.4532 - mae: 24.0744\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 686.3947 - mae: 21.2124\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 600.0409 - mae: 19.6687\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 577.9081 - mae: 19.1833\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 522.6094 - mae: 18.1420\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 511.1010 - mae: 17.8376\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 508.6270 - mae: 17.7500\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 490.8485 - mae: 17.4496\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 485.9080 - mae: 17.2821\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 470.6692 - mae: 17.0855\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 462.5263 - mae: 16.8182\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 440.0886 - mae: 16.4554\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 442.3675 - mae: 16.5085\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 447.7542 - mae: 16.6566\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 431.1514 - mae: 16.2993\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 428.0002 - mae: 16.1420\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 425.9965 - mae: 16.1300\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 423.6664 - mae: 16.1448\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 406.3052 - mae: 15.7766\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 426.5104 - mae: 16.1616\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 411.2845 - mae: 15.8111\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 407.9010 - mae: 15.8241\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 398.0839 - mae: 15.6006\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 402.6129 - mae: 15.6657\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 391.0777 - mae: 15.5662\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 399.1314 - mae: 15.5408\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 392.0417 - mae: 15.4489\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 384.3425 - mae: 15.3806\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 387.3796 - mae: 15.3383\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 383.5808 - mae: 15.4112\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 384.9729 - mae: 15.2875\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 372.6984 - mae: 15.1426\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 375.1591 - mae: 15.1756\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 370.0771 - mae: 15.0149\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 374.3937 - mae: 15.0685\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 366.0028 - mae: 14.9395\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 359.8794 - mae: 14.7637\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 373.9305 - mae: 15.0572\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 361.4526 - mae: 14.9655\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 351.6436 - mae: 14.6694\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 353.4140 - mae: 14.7061\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 365.7703 - mae: 14.9860\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 352.4200 - mae: 14.6501\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 351.3224 - mae: 14.5758\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 346.6704 - mae: 14.4691\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 352.6951 - mae: 14.6511\n",
      "Kappa Score: 0.6573653537507302\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 4ms/step - loss: 3054.5680 - mae: 50.5460\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2172.1267 - mae: 41.4033\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1586.4658 - mae: 34.0518\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1139.0705 - mae: 28.0860\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 854.1183 - mae: 23.8731\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 664.3063 - mae: 20.8497\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 602.5221 - mae: 19.5846\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 584.0604 - mae: 19.2300\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 547.6932 - mae: 18.5570\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 504.5897 - mae: 17.6170\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 502.0396 - mae: 17.5683\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 486.7134 - mae: 17.2560\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 482.8360 - mae: 17.2007\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 461.5453 - mae: 16.8137\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 3ms/step - loss: 461.7545 - mae: 16.8436\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 450.8210 - mae: 16.6344\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 438.8937 - mae: 16.3423\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 447.5435 - mae: 16.5893\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 428.4290 - mae: 16.3041\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 418.1288 - mae: 16.0558\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 421.5110 - mae: 16.0796\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 418.0497 - mae: 15.9666\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 420.4483 - mae: 16.0518\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 413.1053 - mae: 15.9553\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 409.7277 - mae: 15.7444\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 395.2472 - mae: 15.5614\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 403.2541 - mae: 15.6754\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 395.8138 - mae: 15.6137\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 382.4204 - mae: 15.3187\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 397.8302 - mae: 15.5304\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 386.7457 - mae: 15.5325\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 378.9490 - mae: 15.2343\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 399.3044 - mae: 15.5802\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 385.8386 - mae: 15.3794\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 380.2340 - mae: 15.2144\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 373.2914 - mae: 14.9874\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 362.7997 - mae: 14.9583\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 376.1599 - mae: 15.1287\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 358.5407 - mae: 14.7867\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 360.8890 - mae: 14.8865\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 365.4380 - mae: 14.9915\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 366.1664 - mae: 14.9783\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 360.2879 - mae: 14.7548\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 359.3196 - mae: 14.8284\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 349.3814 - mae: 14.5520\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 353.6891 - mae: 14.6730: 0s - loss: 353.7902 - mae: 14.67\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 355.3621 - mae: 14.7697\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 347.7695 - mae: 14.5140\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 3ms/step - loss: 346.4423 - mae: 14.6363\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 351.7379 - mae: 14.6664\n",
      "Kappa Score: 0.6204407051087575\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 4ms/step - loss: 3151.1446 - mae: 51.2992\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2180.6084 - mae: 41.5404\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1590.9830 - mae: 34.3445\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1151.7779 - mae: 28.1999\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 864.3771 - mae: 24.1306\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 684.1746 - mae: 21.1823\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 612.1763 - mae: 19.7876\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 580.3700 - mae: 19.1052\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 527.8767 - mae: 18.1533\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 507.9580 - mae: 17.6906\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 495.8914 - mae: 17.4856\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 484.2078 - mae: 17.2286\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 454.9915 - mae: 16.7027\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 460.0968 - mae: 16.7589\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 446.5309 - mae: 16.5206\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 451.5122 - mae: 16.6431\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 440.4097 - mae: 16.3933\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 449.1263 - mae: 16.5310\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 437.6540 - mae: 16.3936\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 428.0709 - mae: 16.2790\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 417.3873 - mae: 16.0036\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 420.1429 - mae: 16.0716\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 414.9106 - mae: 15.9949\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 404.7020 - mae: 15.7333\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 406.1584 - mae: 15.6886\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 407.4774 - mae: 15.7281\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 390.1183 - mae: 15.4234\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 403.3618 - mae: 15.7291\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 397.7330 - mae: 15.4901\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 388.1579 - mae: 15.3968\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 393.4533 - mae: 15.4758\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 391.7914 - mae: 15.4528\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 381.9820 - mae: 15.3260\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 369.4627 - mae: 14.9969\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 374.4790 - mae: 15.0623\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 382.0269 - mae: 15.2789\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 365.2776 - mae: 14.9543\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 4ms/step - loss: 365.9379 - mae: 15.0772\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 371.8936 - mae: 15.2061\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 368.4802 - mae: 15.0462\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 368.4769 - mae: 14.9644\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 359.7673 - mae: 14.7682\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 359.1573 - mae: 14.8192\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 362.7346 - mae: 14.9111\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 352.1140 - mae: 14.7669\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 352.2024 - mae: 14.6752\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 347.5759 - mae: 14.5553\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 339.8416 - mae: 14.3718\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 345.0679 - mae: 14.5258\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 352.0526 - mae: 14.7966\n",
      "Kappa Score: 0.6349016901197491\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 4s 4ms/step - loss: 3186.2002 - mae: 51.5443\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 2204.1896 - mae: 41.8223\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1603.5777 - mae: 34.3227\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1167.5859 - mae: 28.4209\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 872.3891 - mae: 24.2799\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 672.2700 - mae: 20.9951\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 618.4557 - mae: 19.9816\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 595.4306 - mae: 19.4027\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 558.8342 - mae: 18.8344\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 513.8565 - mae: 17.9634\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 498.9615 - mae: 17.6585\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 488.3537 - mae: 17.3848\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 462.5410 - mae: 16.9274\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 453.2907 - mae: 16.7713\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 464.7968 - mae: 16.9802\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 440.0766 - mae: 16.4088\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 443.6279 - mae: 16.5244\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 440.0943 - mae: 16.3696\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 441.3607 - mae: 16.4032\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 417.8190 - mae: 15.8748\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 428.0191 - mae: 16.2058\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 421.0022 - mae: 15.9745\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 420.9503 - mae: 15.9876\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 405.1430 - mae: 15.6755\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 401.9715 - mae: 15.6037\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 399.5105 - mae: 15.6125\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 398.6996 - mae: 15.6037\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 404.6484 - mae: 15.7365\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 393.5978 - mae: 15.4748\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 386.4983 - mae: 15.3864\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 381.5167 - mae: 15.3626\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 376.8562 - mae: 15.2619\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 381.8478 - mae: 15.2557\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 377.1105 - mae: 15.1946\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 381.7483 - mae: 15.1956\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 368.2943 - mae: 14.9927\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 362.0223 - mae: 14.8435\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 364.8683 - mae: 14.9075\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 358.8497 - mae: 14.9038\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 360.3560 - mae: 14.8939\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 357.7637 - mae: 14.7163\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 362.9203 - mae: 14.7661\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 342.2541 - mae: 14.3977\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 365.1352 - mae: 14.9214\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 354.8307 - mae: 14.7472\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 348.7581 - mae: 14.6014\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 340.1946 - mae: 14.4552\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 352.1611 - mae: 14.6734\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 353.7934 - mae: 14.7449\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 347.4616 - mae: 14.5638\n",
      "Kappa Score: 0.6500500942477003\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 1, 200)            320800    \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 64)                67840     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 4s 5ms/step - loss: 3063.6701 - mae: 50.5177\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 2129.2920 - mae: 40.9544\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 1546.8970 - mae: 33.5919\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1126.8598 - mae: 27.8835\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 844.6284 - mae: 23.7072\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 669.3601 - mae: 20.9086\n",
      "Epoch 7/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 598.5200 - mae: 19.5900\n",
      "Epoch 8/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 591.1925 - mae: 19.4609\n",
      "Epoch 9/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 542.5459 - mae: 18.5098\n",
      "Epoch 10/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 499.3449 - mae: 17.6173\n",
      "Epoch 11/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 498.8963 - mae: 17.6935\n",
      "Epoch 12/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 484.1016 - mae: 17.3660\n",
      "Epoch 13/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 465.7256 - mae: 16.9013\n",
      "Epoch 14/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 473.4125 - mae: 17.0134\n",
      "Epoch 15/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 456.2590 - mae: 16.7346\n",
      "Epoch 16/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 453.4947 - mae: 16.7539\n",
      "Epoch 17/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 446.0949 - mae: 16.5963\n",
      "Epoch 18/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 444.7454 - mae: 16.5527\n",
      "Epoch 19/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 447.8423 - mae: 16.4653\n",
      "Epoch 20/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 424.6046 - mae: 16.1340\n",
      "Epoch 21/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 412.5822 - mae: 15.8560\n",
      "Epoch 22/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 412.8009 - mae: 15.8024\n",
      "Epoch 23/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 417.6592 - mae: 15.8900\n",
      "Epoch 24/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 418.5527 - mae: 16.0442\n",
      "Epoch 25/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 406.6859 - mae: 15.7354\n",
      "Epoch 26/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 401.1503 - mae: 15.6413\n",
      "Epoch 27/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 394.0312 - mae: 15.5173\n",
      "Epoch 28/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 399.8145 - mae: 15.6860\n",
      "Epoch 29/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 389.4949 - mae: 15.4309\n",
      "Epoch 30/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 384.8159 - mae: 15.4090\n",
      "Epoch 31/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 385.1781 - mae: 15.4046\n",
      "Epoch 32/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 367.4727 - mae: 14.8247\n",
      "Epoch 33/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 381.3225 - mae: 15.2658\n",
      "Epoch 34/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 380.7746 - mae: 15.2371\n",
      "Epoch 35/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 365.1176 - mae: 14.9515\n",
      "Epoch 36/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 372.4607 - mae: 15.1827\n",
      "Epoch 37/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 374.9843 - mae: 15.1759\n",
      "Epoch 38/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 370.1616 - mae: 14.9900\n",
      "Epoch 39/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 357.4415 - mae: 14.7405\n",
      "Epoch 40/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 366.8804 - mae: 15.0170\n",
      "Epoch 41/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 358.3045 - mae: 14.8742\n",
      "Epoch 42/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 362.8317 - mae: 14.9715\n",
      "Epoch 43/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 358.2259 - mae: 14.7170\n",
      "Epoch 44/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 360.7851 - mae: 14.8277\n",
      "Epoch 45/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 361.9131 - mae: 14.9373\n",
      "Epoch 46/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 344.3169 - mae: 14.5863\n",
      "Epoch 47/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 357.3367 - mae: 14.7969\n",
      "Epoch 48/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 353.0532 - mae: 14.6319\n",
      "Epoch 49/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 349.6731 - mae: 14.5978\n",
      "Epoch 50/50\n",
      "163/163 [==============================] - 1s 4ms/step - loss: 335.0487 - mae: 14.3203\n",
      "Kappa Score: 0.6509678981895286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    \n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "        # Obtaining all sentences from the training essays.\n",
    "        sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "    \n",
    "#     # Initializing variables for word2vec model.\n",
    "    num_features = 200 \n",
    "#     min_word_count = 40\n",
    "#     num_workers = 4\n",
    "#     context = 10\n",
    "#     downsampling = 1e-3\n",
    "\n",
    "#     print(\"Training Word2Vec Model...\")\n",
    "#     model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "#     model.init_sims(replace=True)\n",
    "#     model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "    \n",
    "    model = embedding_dict\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    \n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./final_lstm.h5')\n",
    "            \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.round(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.6427\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \", np.round(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the SAT 1 score essay scored [[63.]]\n",
      "the SAT 4 score essay scored [[78.]]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "contentBad = \"\"\"\n",
    "    In â€œLet there be dark,â€ Paul Bogard talks about the importance of darkness.\n",
    "\n",
    "Darkness is essential to humans. Bogard states, â€œOur bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of â€œshort sleepâ€ is â€œlong light.â€ Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isnâ€™t a place for this much artificial light in our lives.â€ (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n",
    "\n",
    "Animals also need darkness. Bogard states, â€œThe rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well knownâ€”the 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggsâ€”and some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the worldâ€™s flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earthâ€™s ecology would collapse...â€ (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n",
    "\"\"\" \n",
    "\n",
    "contentGood = \"\"\"\n",
    "    In response to our worldâ€™s growing reliance on artificial light, writer Paul Bogard argues that natural darkness should be preserved in his article â€œLet There be darkâ€. He effectively builds his argument by using a personal anecdote, allusions to art and history, and rhetorical questions.\n",
    "\n",
    "Bogard starts his article off by recounting a personal story â€“ a summer spent on a Minnesota lake where there was â€œwoods so dark that [his] hands disappeared before [his] eyes.â€ In telling this brief anecdote, Bogard challenges the audience to remember a time where they could fully amass themselves in natural darkness void of artificial light. By drawing in his readers with a personal encounter about night darkness, the author means to establish the potential for beauty, glamour, and awe-inspiring mystery that genuine darkness can possess. He builds his argument for the preservation of natural darkness by reminiscing for his readers a first-hand encounter that proves the â€œirreplaceable value of darkness.â€ This anecdote provides a baseline of sorts for readers to find credence with the authorâ€™s claims.\n",
    "\n",
    "Bogardâ€™s argument is also furthered by his use of allusion to art â€“ Van Goghâ€™s â€œStarry Nightâ€ â€“ and modern history â€“ Parisâ€™ reputation as â€œThe City of Lightâ€. By first referencing â€œStarry Nightâ€, a painting generally considered to be undoubtedly beautiful, Bogard establishes that the natural magnificence of stars in a dark sky is definite. A world absent of excess artificial light could potentially hold the key to a grand, glorious night sky like Van Goghâ€™s according to the writer. This urges the readers to weigh the disadvantages of our world consumed by unnatural, vapid lighting. Furthermore, Bogardâ€™s alludes to Paris as â€œthe famed â€˜city of lightâ€™â€. He then goes on to state how Paris has taken steps to exercise more sustainable lighting practices. By doing this, Bogard creates a dichotomy between Parisâ€™ traditionally alluded-to name and the reality of what Paris is becoming â€“ no longer â€œthe city of lightâ€, but moreso â€œthe city of lightâ€¦before 2 AMâ€. This furthers his line of argumentation because it shows how steps can be and are being taken to preserve natural darkness. It shows that even a city that is literally famous for being constantly lit can practically address light pollution in a manner that preserves the beauty of both the city itself and the universe as a whole.\n",
    "\n",
    "Finally, Bogard makes subtle yet efficient use of rhetorical questioning to persuade his audience that natural darkness preservation is essential. He asks the readers to consider â€œwhat the vision of the night sky might inspire in each of us, in our children or grandchildren?â€ in a way that brutally plays to each of our emotions. By asking this question, Bogard draws out heartfelt ponderance from his readers about the affecting power of an untainted night sky. This rhetorical question tugs at the readersâ€™ heartstrings; while the reader may have seen an unobscured night skyline before, the possibility that their child or grandchild will never get the chance sways them to see as Bogard sees. This strategy is definitively an appeal to pathos, forcing the audience to directly face an emotionally-charged inquiry that will surely spur some kind of response. By doing this, Bogard develops his argument, adding gutthral power to the idea that the issue of maintaining natural darkness is relevant and multifaceted.\n",
    "\n",
    "Writing as a reaction to his disappointment that artificial light has largely permeated the prescence of natural darkness, Paul Bogard argues that we must preserve true, unaffected darkness. He builds this claim by making use of a personal anecdote, allusions, and rhetorical questioning.\n",
    "\"\"\"\n",
    "\n",
    "def testContent(content):\n",
    "    if len(content) > 20:\n",
    "        num_features = 200\n",
    "        clean_test_essays = []\n",
    "        clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n",
    "        testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "        testDataVecs = np.array(testDataVecs)\n",
    "        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "        preds = lstm_model.predict(testDataVecs)\n",
    "\n",
    "        if math.isnan(preds):\n",
    "            preds = 0\n",
    "        else:\n",
    "            preds = np.round(preds)\n",
    "\n",
    "        if preds < 0:\n",
    "            preds = 0\n",
    "    else:\n",
    "        preds = 0\n",
    "\n",
    "    return preds\n",
    "    \n",
    "print(\"the SAT 1 score essay scored\", testContent(contentBad))\n",
    "print(\"the SAT 4 score essay scored\", testContent(contentGood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Pickle glove embeddings\n",
    "with open('embeddings.pickle', 'wb') as handle:\n",
    "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
